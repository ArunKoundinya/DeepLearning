{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunKoundinya/DeepLearning/blob/main/posts/deep-learning-project-msis/AmazonReviews_Part7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW9FLtPyVPQl"
      },
      "source": [
        "# Amazon Reviews Sentiment Analysis - Part 7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploration of Attention and Self Attention on the existing BiLSTM Model.\n",
        "\n",
        "Conclusion: Simpler customer created Attention Model function is slightly better because of computation time and slightly better test accuracy.\n",
        "\n",
        "Using attention certainly has improved the accuracy of the earlier BiLSTM Model.\n",
        "\n",
        "Selected simpler model will be used for further finetunning."
      ],
      "metadata": {
        "id": "6xk5-xpajP2z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "- [1 - Packages](#1)\n",
        "- [2 - Loading the Dataset](#2)\n",
        "- [3 - Pre-Processing the Data](#3)\n",
        "- [4 - Model-1 Basic Custom Attention Model](#4)\n",
        "- [5 - Model-2 Advance Customer Attention Model](#5)\n",
        "- [6 - Model-3 SelfAttenion model](#6)"
      ],
      "metadata": {
        "id": "LRq3_UOfjIC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Loading the Packages"
      ],
      "metadata": {
        "id": "vCI71nuTjRad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21Y3K1VOVJfF",
        "outputId": "f2113f8a-dbd8-4e89-8bd3-1dcfaa879b53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandarallel\n",
            "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill>=0.3.1 (from pandarallel)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (1.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from pandarallel) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1->pandarallel) (1.16.0)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16673 sha256=a780718bc84d983454f723a074355c10b8cea74b700d7a1ea4588353d1cc07cb\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/4f/1e/34e057bb868842209f1623f195b74fd7eda229308a7352d47f\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: dill, pandarallel\n",
            "Successfully installed dill-0.3.8 pandarallel-1.6.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pandarallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YffzxgR0VSZR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SimpleRNN, GRU, Bidirectional, Attention,Input,Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from pandarallel import pandarallel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Loading the Data"
      ],
      "metadata": {
        "id": "YkhBfvH3jSkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9WaDLXrVUem",
        "outputId": "08d6a981-aeb4-4a6a-adb5-b6413dd41eb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/')\n",
        "\n",
        "testdata = pd.read_csv('test_data_sample_complete.csv')\n",
        "traindata = pd.read_csv('train_data_sample_complete.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WudSZ3wTVXH_",
        "outputId": "930f8ae1-5713-47a4-aedd-6908ce3edd91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 20 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ]
        }
      ],
      "source": [
        "pandarallel.initialize(progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Pre-Processing the data"
      ],
      "metadata": {
        "id": "PyAEb3MEjWTO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHrcRhnQVukq",
        "outputId": "9ccaddb7-4e9b-4eea-a081-8e7478e84866"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 100)\n",
            "(5000, 100)\n",
            "(5000, 100)\n"
          ]
        }
      ],
      "source": [
        "train_data = traindata.sample(n=100000, random_state=42)\n",
        "test_data = testdata.sample(n=10000, random_state=42)\n",
        "\n",
        "train_data['class_index'] = train_data['class_index'].map({1:0, 2:1})\n",
        "test_data['class_index'] = test_data['class_index'].map({1:0, 2:1})\n",
        "\n",
        "train_data['review_combined_lemma'] = train_data['review_combined_lemma'].fillna('')\n",
        "test_data['review_combined_lemma'] = test_data['review_combined_lemma'].fillna('')\n",
        "\n",
        "X_train = train_data.review_combined_lemma\n",
        "y_train = np.array(train_data.class_index)\n",
        "\n",
        "X = test_data.review_combined_lemma\n",
        "y = np.array(test_data.class_index)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\",)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "X_sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "X_train = pad_sequences(X_sequences_train, maxlen=100)\n",
        "X = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_dev.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3XdSxi5VxoA"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5T07A-6OV1-L"
      },
      "outputs": [],
      "source": [
        "word2idx = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWegv1h6V3cO"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(glove_path):\n",
        "    embedding_index = {}\n",
        "    with open(glove_path, encoding=\"utf8\") as glove_file:\n",
        "        for line in glove_file:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQeLE0QvV5SI"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n",
        "    mat=np.zeros((vocab_size,embedding_dim))\n",
        "    for key,value in word2idx.items():\n",
        "      mat[value]=embedding_index.get(key)\n",
        "    mat[np.isnan(mat)] = 0\n",
        "    return mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oVhqcTAV6cz"
      },
      "outputs": [],
      "source": [
        "glove_path3 = f\"glove.6B/glove.twitter.27B.200d.txt\"\n",
        "embedding_index_Twitter_200d = load_embeddings(glove_path3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQo3tN8WBbu",
        "outputId": "338ec864-ca3a-4c37-c8cf-f4a526523d25"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.43551999,  0.16238999, -0.29269999, -0.29675001, -0.34759   ,\n",
              "       -0.47275001,  0.8125    ,  0.25753999,  0.063817  , -0.39695999,\n",
              "       -0.63590002,  0.27177   , -0.62805003, -0.56298   ,  0.18736   ,\n",
              "       -0.2068    , -0.24707   ,  0.16885   ,  0.50615001,  0.031079  ,\n",
              "        0.16841   , -0.87362999, -0.11618   ,  0.10592   , -0.35339999,\n",
              "        0.65625   ,  0.070923  ,  0.098416  ,  0.47573   , -0.12987   ,\n",
              "        0.22447   ,  0.69542003, -0.47979999, -0.16331001, -0.58661997,\n",
              "        0.039876  ,  0.51730001, -0.081318  ,  0.33581001, -0.28227001,\n",
              "        0.097423  ,  0.086391  , -0.012591  , -0.31064001,  0.049688  ,\n",
              "        0.51059002, -0.25094   , -0.014923  ,  0.12813   , -0.20479999,\n",
              "       -0.54636002, -0.055901  , -0.84912997, -0.23548   ,  0.17764001,\n",
              "       -0.31343001,  0.34996   , -0.82489997, -0.17274   , -0.15154   ,\n",
              "        0.33089   , -0.30372   ,  0.010554  , -0.078452  , -0.36133999,\n",
              "        0.41997001, -0.15302999, -0.32323   ,  0.63178003, -0.090845  ,\n",
              "       -0.38863   , -0.23664001,  0.091741  ,  0.070776  ,  0.71016002,\n",
              "       -0.26671001, -0.27506   , -0.26003999,  0.44433001, -0.20695999,\n",
              "        0.38530999,  0.36493999,  0.75682002,  0.17899001,  0.18409   ,\n",
              "        0.11041   ,  0.50098002,  0.15883   ,  0.037857  ,  0.28481999,\n",
              "        0.36295   ,  0.4413    , -0.16407   , -0.36001   ,  0.022854  ,\n",
              "       -0.62028003,  0.69516999, -0.45416   , -0.40709001,  0.16159   ,\n",
              "        0.37006   , -0.40299001, -0.41363001,  0.26462999,  0.2078    ,\n",
              "        0.13292   , -0.28707999, -0.20352   ,  0.086132  ,  0.22634999,\n",
              "       -0.26052999,  0.1594    , -0.40485999, -0.37944001,  0.15368   ,\n",
              "        0.48357001,  0.59877998, -0.078901  ,  0.52380002,  0.32205001,\n",
              "       -0.04105   ,  0.49676001,  0.034558  , -0.011625  , -0.48692   ,\n",
              "        0.18361001,  0.2181    ,  0.026098  , -0.63301003, -0.40235001,\n",
              "       -0.35194001,  0.27325001,  0.10173   ,  0.02382   , -0.040737  ,\n",
              "       -0.61325002, -0.39431   ,  0.062469  , -0.35971001, -0.027608  ,\n",
              "        0.26736   , -0.63182998, -0.13828   ,  0.15346999,  0.37463   ,\n",
              "        0.029464  , -0.18398   , -0.17923   , -0.12501   ,  0.44969001,\n",
              "        0.1224    , -0.51299   , -4.48629999,  0.13257   ,  0.18114001,\n",
              "       -0.020733  ,  0.13391   , -0.3346    ,  0.050939  , -0.14139999,\n",
              "        0.25650999,  0.30274999,  0.57305002,  0.0093119 ,  0.23918   ,\n",
              "        0.62794   ,  0.70270997,  0.17521   , -0.0149    ,  0.20555   ,\n",
              "        0.49822   ,  0.37237999,  0.10829   ,  0.26648   ,  0.12772   ,\n",
              "        0.12913001,  0.037382  , -0.68593001, -0.26249   , -0.11311   ,\n",
              "        0.10306   ,  0.12908   , -0.054654  ,  0.60295999, -0.29122999,\n",
              "        0.67360997, -0.52846998, -0.30971   , -0.11496   ,  0.74361002,\n",
              "        0.52497   ,  0.042953  ,  0.22132   ,  0.86691999,  0.18026   ,\n",
              "        0.019078  , -0.29036999, -0.19461   , -0.0211    , -0.39921001])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "embedding_matrix_twitter_200d = create_embedding_matrix(embedding_index_Twitter_200d, word2idx, vocab_size, 200)\n",
        "embedding_matrix_twitter_200d[word2idx[\"book\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgyyZqN6oUWs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbIt43sOoUqX",
        "outputId": "14fe75a2-d7b6-45ca-fe46-f83cd59b44bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100000,)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Model -1 Basic Custom Made Attention Model."
      ],
      "metadata": {
        "id": "LOv0QCxGjeWk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48YQGtTA9Z-j"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features):\n",
        "        # Compute attention scores\n",
        "        score = self.W1(features)\n",
        "\n",
        "        # Apply softmax activation to obtain attention weights\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # Compute context vector as the weighted sum of features\n",
        "        context_vector = attention_weights * features\n",
        "\n",
        "        return context_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmtScR1xMAVs"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(100,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(embedding_layer)\n",
        "context_vector = Attention(8)(bilstm)\n",
        "simplernn = SimpleRNN(4, activation=\"tanh\")(context_vector)\n",
        "output = Dense(1, activation=\"sigmoid\")(simplernn)\n",
        "\n",
        "model_lstm_bi_embed_attention = Model(inputs=inputs, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgHcjwsJAtEo",
        "outputId": "3b8d2dce-bd84-43ae-b4c4-da5237b3798f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 100, 200)          36072400  \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 100, 8)           6560      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " attention_1 (Attention)     (None, 100, 8)            81        \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 4)                 52        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,079,098\n",
            "Trainable params: 6,698\n",
            "Non-trainable params: 36,072,400\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQqNjc8lW7Ce",
        "outputId": "3f50010a-1b3c-4ecc-d214-e417d76a2354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 85s 51ms/step - loss: 0.4217 - accuracy: 0.8193 - val_loss: 0.3560 - val_accuracy: 0.8520\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.3139 - accuracy: 0.8773 - val_loss: 0.3239 - val_accuracy: 0.8690\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2816 - accuracy: 0.8896 - val_loss: 0.2925 - val_accuracy: 0.8830\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2635 - accuracy: 0.8969 - val_loss: 0.2791 - val_accuracy: 0.8888\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2530 - accuracy: 0.9009 - val_loss: 0.2901 - val_accuracy: 0.8830\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 77s 49ms/step - loss: 0.2457 - accuracy: 0.9044 - val_loss: 0.2807 - val_accuracy: 0.8856\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2392 - accuracy: 0.9075 - val_loss: 0.2964 - val_accuracy: 0.8786\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2340 - accuracy: 0.9097 - val_loss: 0.2641 - val_accuracy: 0.8924\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2282 - accuracy: 0.9119 - val_loss: 0.2636 - val_accuracy: 0.8944\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2239 - accuracy: 0.9138 - val_loss: 0.2618 - val_accuracy: 0.8948\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 77s 50ms/step - loss: 0.2228 - accuracy: 0.9144 - val_loss: 0.2826 - val_accuracy: 0.8852\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2183 - accuracy: 0.9164 - val_loss: 0.2750 - val_accuracy: 0.8862\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2158 - accuracy: 0.9176 - val_loss: 0.3087 - val_accuracy: 0.8770\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 77s 50ms/step - loss: 0.2139 - accuracy: 0.9185 - val_loss: 0.2633 - val_accuracy: 0.8942\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2104 - accuracy: 0.9205 - val_loss: 0.2632 - val_accuracy: 0.8942\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2079 - accuracy: 0.9213 - val_loss: 0.2727 - val_accuracy: 0.8906\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2085 - accuracy: 0.9213 - val_loss: 0.2729 - val_accuracy: 0.8922\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2030 - accuracy: 0.9239 - val_loss: 0.2677 - val_accuracy: 0.8910\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.2022 - accuracy: 0.9247 - val_loss: 0.2815 - val_accuracy: 0.8904\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1997 - accuracy: 0.9247 - val_loss: 0.2650 - val_accuracy: 0.8958\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1988 - accuracy: 0.9254 - val_loss: 0.2686 - val_accuracy: 0.8928\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1989 - accuracy: 0.9258 - val_loss: 0.2714 - val_accuracy: 0.8906\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 77s 50ms/step - loss: 0.1964 - accuracy: 0.9265 - val_loss: 0.2814 - val_accuracy: 0.8906\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1941 - accuracy: 0.9282 - val_loss: 0.2697 - val_accuracy: 0.8922\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1981 - accuracy: 0.9258 - val_loss: 0.2736 - val_accuracy: 0.8948\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1936 - accuracy: 0.9283 - val_loss: 0.2659 - val_accuracy: 0.8940\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1920 - accuracy: 0.9292 - val_loss: 0.2862 - val_accuracy: 0.8862\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1919 - accuracy: 0.9293 - val_loss: 0.2724 - val_accuracy: 0.8908\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 0.1892 - accuracy: 0.9303 - val_loss: 0.2720 - val_accuracy: 0.8926\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 79s 50ms/step - loss: 0.1887 - accuracy: 0.9308 - val_loss: 0.2727 - val_accuracy: 0.8920\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1864 - accuracy: 0.9315 - val_loss: 0.2760 - val_accuracy: 0.8926\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1845 - accuracy: 0.9324 - val_loss: 0.2878 - val_accuracy: 0.8876\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1849 - accuracy: 0.9320 - val_loss: 0.2858 - val_accuracy: 0.8904\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1829 - accuracy: 0.9329 - val_loss: 0.2699 - val_accuracy: 0.8942\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1828 - accuracy: 0.9331 - val_loss: 0.2740 - val_accuracy: 0.8952\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1810 - accuracy: 0.9342 - val_loss: 0.2743 - val_accuracy: 0.8942\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1800 - accuracy: 0.9342 - val_loss: 0.2771 - val_accuracy: 0.8924\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1791 - accuracy: 0.9343 - val_loss: 0.2873 - val_accuracy: 0.8950\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1789 - accuracy: 0.9356 - val_loss: 0.2710 - val_accuracy: 0.8934\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1774 - accuracy: 0.9350 - val_loss: 0.2824 - val_accuracy: 0.8930\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1769 - accuracy: 0.9357 - val_loss: 0.2986 - val_accuracy: 0.8908\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1772 - accuracy: 0.9355 - val_loss: 0.2746 - val_accuracy: 0.8954\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1758 - accuracy: 0.9367 - val_loss: 0.2991 - val_accuracy: 0.8928\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1748 - accuracy: 0.9366 - val_loss: 0.2923 - val_accuracy: 0.8914\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1735 - accuracy: 0.9373 - val_loss: 0.3112 - val_accuracy: 0.8848\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1733 - accuracy: 0.9373 - val_loss: 0.2980 - val_accuracy: 0.8944\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1723 - accuracy: 0.9376 - val_loss: 0.2789 - val_accuracy: 0.8930\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1735 - accuracy: 0.9374 - val_loss: 0.2777 - val_accuracy: 0.8950\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1709 - accuracy: 0.9389 - val_loss: 0.2985 - val_accuracy: 0.8906\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 78s 50ms/step - loss: 0.1711 - accuracy: 0.9382 - val_loss: 0.2835 - val_accuracy: 0.8974\n",
            "157/157 [==============================] - 2s 12ms/step - loss: 0.2658 - accuracy: 0.9026\n",
            "Test Loss: 0.26584601402282715, Test Accuracy: 0.9025999903678894\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_model_lstm_bi_embed_attention = model_lstm_bi_embed_attention.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_dev, y_dev),verbose=1)\n",
        "\n",
        "loss, accuracy = model_lstm_bi_embed_attention.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Model -2 Advanced Custom Made Attention Model."
      ],
      "metadata": {
        "id": "kHWakjg0jjgp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsg_aw-RHtYM"
      },
      "outputs": [],
      "source": [
        "class Attention_Update(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention_Update, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Initialize weights for attention mechanism\n",
        "        self.Wa = self.add_weight(name=\"att_weight_1\", shape=(input_shape[-1], 8),\n",
        "                                 initializer=\"normal\")\n",
        "        self.Wb = self.add_weight(name=\"att_weight_2\", shape=(input_shape[-1], 8),\n",
        "                                 initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias_2\", shape=(input_shape[1], 8),\n",
        "                                 initializer=\"zeros\")\n",
        "\n",
        "        super(Attention_Update, self).build(input_shape)\n",
        "\n",
        "    def call(self, features):\n",
        "        # Compute attention scores\n",
        "        score = self.W1(features)\n",
        "\n",
        "        # Apply softmax activation to obtain attention weights\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # Compute context vector as the weighted sum of features\n",
        "        context_vector = attention_weights * features\n",
        "\n",
        "        new_hidden_state = tf.tanh(tf.matmul(context_vector, self.Wa) + tf.matmul(features, self.Wb) + self.b)\n",
        "        return new_hidden_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YvHmleLBMx8d"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(100,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(embedding_layer)\n",
        "new_hidden_state = Attention_Update(8)(bilstm)\n",
        "simplernn = SimpleRNN(4, activation=\"tanh\")(new_hidden_state)\n",
        "output = Dense(1, activation=\"sigmoid\")(simplernn)\n",
        "\n",
        "model_lstm_bi_embed_attention2 = Model(inputs=inputs, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkFxwKw6Pg1x",
        "outputId": "94234577-0085-4dbf-a8a8-cf9565391856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 100, 200)          36072400  \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 100, 8)           6560      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " attention__update_1 (Attent  (None, 100, 8)           1009      \n",
            " ion_Update)                                                     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 4)                 52        \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,080,026\n",
            "Trainable params: 7,626\n",
            "Non-trainable params: 36,072,400\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQo7LvRIU9r8",
        "outputId": "49e4017f-b5c3-4112-d894-b6b4ae528057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 78s 47ms/step - loss: 0.4312 - accuracy: 0.8162 - val_loss: 0.3668 - val_accuracy: 0.8414\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.3161 - accuracy: 0.8689 - val_loss: 0.3328 - val_accuracy: 0.8572\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 72s 46ms/step - loss: 0.2790 - accuracy: 0.8851 - val_loss: 0.2849 - val_accuracy: 0.8804\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.2587 - accuracy: 0.8947 - val_loss: 0.2866 - val_accuracy: 0.8816\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2452 - accuracy: 0.9007 - val_loss: 0.2666 - val_accuracy: 0.8922\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2365 - accuracy: 0.9044 - val_loss: 0.2593 - val_accuracy: 0.8946\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.2294 - accuracy: 0.9075 - val_loss: 0.2539 - val_accuracy: 0.8972\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 71s 46ms/step - loss: 0.2238 - accuracy: 0.9108 - val_loss: 0.2588 - val_accuracy: 0.8990\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2188 - accuracy: 0.9125 - val_loss: 0.2699 - val_accuracy: 0.8858\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2148 - accuracy: 0.9142 - val_loss: 0.2537 - val_accuracy: 0.8930\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2115 - accuracy: 0.9161 - val_loss: 0.2486 - val_accuracy: 0.9004\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.2072 - accuracy: 0.9180 - val_loss: 0.2484 - val_accuracy: 0.8994\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.2043 - accuracy: 0.9194 - val_loss: 0.2478 - val_accuracy: 0.8970\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.2020 - accuracy: 0.9204 - val_loss: 0.2444 - val_accuracy: 0.8986\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1994 - accuracy: 0.9215 - val_loss: 0.2491 - val_accuracy: 0.8986\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1973 - accuracy: 0.9225 - val_loss: 0.2520 - val_accuracy: 0.8960\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1943 - accuracy: 0.9237 - val_loss: 0.2610 - val_accuracy: 0.8930\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1929 - accuracy: 0.9247 - val_loss: 0.2539 - val_accuracy: 0.8946\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.1914 - accuracy: 0.9249 - val_loss: 0.2465 - val_accuracy: 0.8962\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.1891 - accuracy: 0.9261 - val_loss: 0.2524 - val_accuracy: 0.8990\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 71s 45ms/step - loss: 0.1878 - accuracy: 0.9265 - val_loss: 0.2534 - val_accuracy: 0.8960\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1859 - accuracy: 0.9281 - val_loss: 0.2672 - val_accuracy: 0.8916\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1849 - accuracy: 0.9279 - val_loss: 0.2524 - val_accuracy: 0.8942\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1830 - accuracy: 0.9292 - val_loss: 0.2531 - val_accuracy: 0.8972\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1828 - accuracy: 0.9288 - val_loss: 0.2551 - val_accuracy: 0.8970\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1804 - accuracy: 0.9306 - val_loss: 0.2648 - val_accuracy: 0.8892\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1789 - accuracy: 0.9309 - val_loss: 0.2649 - val_accuracy: 0.8946\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1783 - accuracy: 0.9314 - val_loss: 0.2639 - val_accuracy: 0.8906\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1779 - accuracy: 0.9319 - val_loss: 0.2628 - val_accuracy: 0.8968\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 70s 44ms/step - loss: 0.1758 - accuracy: 0.9324 - val_loss: 0.2573 - val_accuracy: 0.8950\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1751 - accuracy: 0.9323 - val_loss: 0.2949 - val_accuracy: 0.8854\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1749 - accuracy: 0.9322 - val_loss: 0.2758 - val_accuracy: 0.8884\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1725 - accuracy: 0.9341 - val_loss: 0.2582 - val_accuracy: 0.8942\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1727 - accuracy: 0.9338 - val_loss: 0.2797 - val_accuracy: 0.8926\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1715 - accuracy: 0.9348 - val_loss: 0.2771 - val_accuracy: 0.8918\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1704 - accuracy: 0.9346 - val_loss: 0.2740 - val_accuracy: 0.8970\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1699 - accuracy: 0.9350 - val_loss: 0.2662 - val_accuracy: 0.8934\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1691 - accuracy: 0.9357 - val_loss: 0.2903 - val_accuracy: 0.8868\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1688 - accuracy: 0.9358 - val_loss: 0.2764 - val_accuracy: 0.8906\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1675 - accuracy: 0.9364 - val_loss: 0.2923 - val_accuracy: 0.8874\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1684 - accuracy: 0.9360 - val_loss: 0.2697 - val_accuracy: 0.8902\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1676 - accuracy: 0.9361 - val_loss: 0.2881 - val_accuracy: 0.8914\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1652 - accuracy: 0.9372 - val_loss: 0.2809 - val_accuracy: 0.8928\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 70s 44ms/step - loss: 0.1659 - accuracy: 0.9371 - val_loss: 0.2677 - val_accuracy: 0.8904\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1639 - accuracy: 0.9380 - val_loss: 0.2857 - val_accuracy: 0.8906\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1632 - accuracy: 0.9384 - val_loss: 0.2821 - val_accuracy: 0.8906\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 70s 45ms/step - loss: 0.1641 - accuracy: 0.9379 - val_loss: 0.2922 - val_accuracy: 0.8902\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1638 - accuracy: 0.9382 - val_loss: 0.2910 - val_accuracy: 0.8918\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1616 - accuracy: 0.9394 - val_loss: 0.2864 - val_accuracy: 0.8918\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 69s 44ms/step - loss: 0.1620 - accuracy: 0.9387 - val_loss: 0.3097 - val_accuracy: 0.8928\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.2995 - accuracy: 0.9008\n",
            "Test Loss: 0.2995133697986603, Test Accuracy: 0.9007999897003174\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_model_lstm_bi_embed_attention2 = model_lstm_bi_embed_attention2.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_dev, y_dev),verbose=1)\n",
        "\n",
        "loss, accuracy = model_lstm_bi_embed_attention2.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='6'></a>\n",
        "## 6 - Model -3 Self Attention Model"
      ],
      "metadata": {
        "id": "uZHF0PThjpkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-self-attention\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ereHAl051GTX",
        "outputId": "19f1b2a2-74ff-48df-a01b-20d87a154224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-self-attention\n",
            "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-self-attention) (1.23.5)\n",
            "Building wheels for collected packages: keras-self-attention\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18894 sha256=282baf05241afc51f4e831e3dae5a0b695f8ef681c37f2ac35ecea90e9b5e66f\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/f7/24/607b483144fb9c47b4ba2c5fba6b68e54aeee2d5bf6c05302e\n",
            "Successfully built keras-self-attention\n",
            "Installing collected packages: keras-self-attention\n",
            "Successfully installed keras-self-attention-0.51.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras_self_attention import SeqSelfAttention"
      ],
      "metadata": {
        "id": "LRauW4r_10Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.initializers import GlorotNormal\n",
        "\n",
        "initializer = GlorotNormal(seed=42)\n",
        "\n",
        "inputs = Input(shape=(100,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(embedding_layer)\n",
        "context_vector = SeqSelfAttention(attention_activation='sigmoid')(bilstm)\n",
        "simplernn = SimpleRNN(4, activation=\"tanh\")(context_vector)\n",
        "output = Dense(1, activation=\"sigmoid\")(simplernn)\n",
        "\n",
        "model_lstm_bi_embed_selfattention = Model(inputs=inputs, outputs=output)"
      ],
      "metadata": {
        "id": "HKICX6dv1Jqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm_bi_embed_selfattention.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wROhyB_B2ITO",
        "outputId": "e8408d2e-b5aa-43aa-d7f7-4099fc885928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_8 (Embedding)     (None, 100, 200)          36072400  \n",
            "                                                                 \n",
            " bidirectional_9 (Bidirectio  (None, 100, 8)           6560      \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " seq_self_attention_5 (SeqSe  (None, 100, 8)           577       \n",
            " lfAttention)                                                    \n",
            "                                                                 \n",
            " simple_rnn_7 (SimpleRNN)    (None, 4)                 52        \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 36,079,594\n",
            "Trainable params: 7,194\n",
            "Non-trainable params: 36,072,400\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm_bi_embed_selfattention.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_model_lstm_bi_embed_selfattention = model_lstm_bi_embed_selfattention.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_dev, y_dev),verbose=1)\n",
        "\n",
        "loss, accuracy = model_lstm_bi_embed_selfattention.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUdPmP9U2et2",
        "outputId": "4e396c49-bed8-404f-a6b9-849c3f135011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 213s 134ms/step - loss: 0.3881 - accuracy: 0.8373 - val_loss: 0.3168 - val_accuracy: 0.8672\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 210s 134ms/step - loss: 0.2854 - accuracy: 0.8846 - val_loss: 0.2807 - val_accuracy: 0.8850\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2571 - accuracy: 0.8972 - val_loss: 0.2657 - val_accuracy: 0.8894\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 209s 133ms/step - loss: 0.2443 - accuracy: 0.9032 - val_loss: 0.2639 - val_accuracy: 0.8908\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.2332 - accuracy: 0.9074 - val_loss: 0.2495 - val_accuracy: 0.8950\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.2259 - accuracy: 0.9105 - val_loss: 0.2502 - val_accuracy: 0.8970\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.2200 - accuracy: 0.9130 - val_loss: 0.2535 - val_accuracy: 0.8958\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.2139 - accuracy: 0.9163 - val_loss: 0.2563 - val_accuracy: 0.8940\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 207s 133ms/step - loss: 0.2108 - accuracy: 0.9182 - val_loss: 0.2436 - val_accuracy: 0.8994\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 207s 133ms/step - loss: 0.2085 - accuracy: 0.9190 - val_loss: 0.2436 - val_accuracy: 0.9008\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.2037 - accuracy: 0.9208 - val_loss: 0.2433 - val_accuracy: 0.9006\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 216s 138ms/step - loss: 0.2017 - accuracy: 0.9218 - val_loss: 0.2636 - val_accuracy: 0.8954\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 215s 137ms/step - loss: 0.1987 - accuracy: 0.9235 - val_loss: 0.2583 - val_accuracy: 0.8918\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1967 - accuracy: 0.9238 - val_loss: 0.2513 - val_accuracy: 0.8962\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 215s 137ms/step - loss: 0.1936 - accuracy: 0.9254 - val_loss: 0.2459 - val_accuracy: 0.8986\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1924 - accuracy: 0.9259 - val_loss: 0.2612 - val_accuracy: 0.8938\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1901 - accuracy: 0.9271 - val_loss: 0.2455 - val_accuracy: 0.8978\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1890 - accuracy: 0.9276 - val_loss: 0.2479 - val_accuracy: 0.8978\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1869 - accuracy: 0.9288 - val_loss: 0.2564 - val_accuracy: 0.8966\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 215s 137ms/step - loss: 0.1872 - accuracy: 0.9282 - val_loss: 0.2530 - val_accuracy: 0.8978\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1836 - accuracy: 0.9298 - val_loss: 0.2756 - val_accuracy: 0.8922\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 213s 137ms/step - loss: 0.1826 - accuracy: 0.9308 - val_loss: 0.2670 - val_accuracy: 0.8938\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 214s 137ms/step - loss: 0.1821 - accuracy: 0.9302 - val_loss: 0.2514 - val_accuracy: 0.8974\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 212s 136ms/step - loss: 0.1800 - accuracy: 0.9319 - val_loss: 0.2509 - val_accuracy: 0.8978\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 212s 136ms/step - loss: 0.1774 - accuracy: 0.9333 - val_loss: 0.2885 - val_accuracy: 0.8896\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1779 - accuracy: 0.9332 - val_loss: 0.2635 - val_accuracy: 0.8946\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1763 - accuracy: 0.9333 - val_loss: 0.2582 - val_accuracy: 0.8976\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1743 - accuracy: 0.9351 - val_loss: 0.2718 - val_accuracy: 0.8940\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1733 - accuracy: 0.9351 - val_loss: 0.2652 - val_accuracy: 0.8966\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1737 - accuracy: 0.9348 - val_loss: 0.2657 - val_accuracy: 0.8950\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 0.1720 - accuracy: 0.9362 - val_loss: 0.2730 - val_accuracy: 0.8914\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1707 - accuracy: 0.9359 - val_loss: 0.2620 - val_accuracy: 0.8972\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1704 - accuracy: 0.9366 - val_loss: 0.2559 - val_accuracy: 0.8950\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 210s 134ms/step - loss: 0.1705 - accuracy: 0.9363 - val_loss: 0.2613 - val_accuracy: 0.8974\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 209s 133ms/step - loss: 0.1677 - accuracy: 0.9377 - val_loss: 0.2807 - val_accuracy: 0.8946\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1687 - accuracy: 0.9373 - val_loss: 0.2633 - val_accuracy: 0.8954\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1666 - accuracy: 0.9381 - val_loss: 0.2677 - val_accuracy: 0.8924\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1662 - accuracy: 0.9384 - val_loss: 0.2704 - val_accuracy: 0.8948\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.1650 - accuracy: 0.9388 - val_loss: 0.2703 - val_accuracy: 0.8934\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 209s 133ms/step - loss: 0.1648 - accuracy: 0.9394 - val_loss: 0.2850 - val_accuracy: 0.8926\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1644 - accuracy: 0.9394 - val_loss: 0.2693 - val_accuracy: 0.8962\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1631 - accuracy: 0.9396 - val_loss: 0.2801 - val_accuracy: 0.8914\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.1645 - accuracy: 0.9388 - val_loss: 0.2687 - val_accuracy: 0.8954\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1622 - accuracy: 0.9406 - val_loss: 0.2722 - val_accuracy: 0.8948\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 0.1630 - accuracy: 0.9402 - val_loss: 0.2739 - val_accuracy: 0.8968\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.1609 - accuracy: 0.9408 - val_loss: 0.2718 - val_accuracy: 0.8972\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 0.1607 - accuracy: 0.9410 - val_loss: 0.2679 - val_accuracy: 0.8936\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 207s 133ms/step - loss: 0.1611 - accuracy: 0.9411 - val_loss: 0.2655 - val_accuracy: 0.8978\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 206s 132ms/step - loss: 0.1586 - accuracy: 0.9422 - val_loss: 0.2765 - val_accuracy: 0.8950\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 206s 132ms/step - loss: 0.1593 - accuracy: 0.9419 - val_loss: 0.2682 - val_accuracy: 0.8972\n",
            "157/157 [==============================] - 4s 25ms/step - loss: 0.2575 - accuracy: 0.9010\n",
            "Test Loss: 0.2574847638607025, Test Accuracy: 0.9010000228881836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VjdHbtsr3ikr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOmLmr6JsKkD7gjjOkAGseD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
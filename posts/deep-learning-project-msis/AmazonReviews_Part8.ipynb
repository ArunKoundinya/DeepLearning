{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunKoundinya/DeepLearning/blob/main/posts/deep-learning-project-msis/AmazonReviews_Part8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will taking the previous model of basic Attention and then following the below steps\n",
        "\n",
        "*   Increasing Model Complexity\n",
        "*   Regularizing the Model\n",
        "*   Hypertuning the Model\n",
        "*   Testing the Model on 10K dataset\n",
        "*   Finally testing the model on 1 Lakh (0.1 Million) dataset\n"
      ],
      "metadata": {
        "id": "ww8uIrPYg91j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "- [1 - Packages](#1)\n",
        "- [2 - Loading the Dataset](#2)\n",
        "- [3 - Pre-Processing the Data](#3)\n",
        "- [4 - Model-1 Attention Model](#4)\n",
        "- [5 - Model-2 Increasing Layers and Units](#5)\n",
        "- [6 - Model-3 Adding Regularization](#6)\n",
        "- [7 - Model-4 Hypertuning the model](#7)\n",
        "- [8 - Model-5 Testing Model on 10K dataset](#8)\n",
        "- [9 - Model-6 Testing Model on 1 Lac dataset](#9)"
      ],
      "metadata": {
        "id": "sxnRC89nhcY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='1'></a>\n",
        "## 1 - Loading the Packages"
      ],
      "metadata": {
        "id": "l6YRBw_-hxzS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ufs_Qt4v6EH",
        "outputId": "3903734a-bfb0-4132-bf8d-255f4b5d185c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pandarallel\n",
            "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dill>=0.3.1 (from pandarallel)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1 in /usr/local/lib/python3.10/dist-packages (from pandarallel) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from pandarallel) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1->pandarallel) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1->pandarallel) (1.16.0)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16673 sha256=be7381230ffce7eef1c746056f9539342fe9d214e2273eb451de3ba03a6a86ff\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/4f/1e/34e057bb868842209f1623f195b74fd7eda229308a7352d47f\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: dill, pandarallel\n",
            "Successfully installed dill-0.3.8 pandarallel-1.6.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pandarallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NthM8X7xwCie"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SimpleRNN, GRU, Bidirectional,Input,Dropout,Flatten,Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import GlorotUniform\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "from pandarallel import pandarallel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Loading the Data"
      ],
      "metadata": {
        "id": "Fsgyazlkhza_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qDnTi4HwJyx",
        "outputId": "745cdd7b-e546-4310-a8b2-46c57463217d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/')\n",
        "\n",
        "testdata = pd.read_csv('test_data_sample_complete.csv')\n",
        "traindata = pd.read_csv('train_data_sample_complete.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSAclUPcwVwA",
        "outputId": "bfaf6e9c-f6a5-4356-bdcd-34ed2fda15ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO: Pandarallel will run on 4 workers.\n",
            "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
          ]
        }
      ],
      "source": [
        "pandarallel.initialize(progress_bar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Pre-Processing the Data"
      ],
      "metadata": {
        "id": "um9eP_zah2ir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Hw0I96wXPp",
        "outputId": "4ba255bd-2027-416c-ae85-41d9a14c1285"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 100)\n",
            "(500, 100)\n",
            "(500, 100)\n"
          ]
        }
      ],
      "source": [
        "train_data = traindata.sample(n=10000, random_state=42)\n",
        "test_data = testdata.sample(n=1000, random_state=42)\n",
        "\n",
        "train_data['class_index'] = train_data['class_index'].map({1:0, 2:1})\n",
        "test_data['class_index'] = test_data['class_index'].map({1:0, 2:1})\n",
        "\n",
        "train_data['review_combined_lemma'] = train_data['review_combined_lemma'].fillna('')\n",
        "test_data['review_combined_lemma'] = test_data['review_combined_lemma'].fillna('')\n",
        "\n",
        "X_train = train_data.review_combined_lemma\n",
        "y_train = np.array(train_data.class_index)\n",
        "\n",
        "X = test_data.review_combined_lemma\n",
        "y = np.array(test_data.class_index)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\",)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "X_sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "X_train = pad_sequences(X_sequences_train, maxlen=100)\n",
        "X = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_dev.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxqHVv4hwcUZ"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFsV0y50whRq"
      },
      "outputs": [],
      "source": [
        "word2idx = tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w33LR0Srwi0l"
      },
      "outputs": [],
      "source": [
        "def load_embeddings(glove_path):\n",
        "    embedding_index = {}\n",
        "    with open(glove_path, encoding=\"utf8\") as glove_file:\n",
        "        for line in glove_file:\n",
        "            word, coefs = line.split(maxsplit=1)\n",
        "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "            embedding_index[word] = coefs\n",
        "    return embedding_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRDVM39Uwk_I"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(embedding_index, word2idx, vocab_size, embedding_dim):\n",
        "    mat=np.zeros((vocab_size,embedding_dim))\n",
        "    for key,value in word2idx.items():\n",
        "      mat[value]=embedding_index.get(key)\n",
        "    mat[np.isnan(mat)] = 0\n",
        "    return mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObBL5gGkwrND"
      },
      "outputs": [],
      "source": [
        "glove_path3 = f\"glove.6B/glove.twitter.27B.200d.txt\"\n",
        "embedding_index_Twitter_200d = load_embeddings(glove_path3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObwUcWuIw6cV"
      },
      "outputs": [],
      "source": [
        "embedding_matrix_twitter_200d = create_embedding_matrix(embedding_index_Twitter_200d, word2idx, vocab_size, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Model 1 - Attention Model from Previous Notebook."
      ],
      "metadata": {
        "id": "B4VxPTi_h7E2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuEubp_5w_jE"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, features):\n",
        "        # Compute attention scores\n",
        "        score = self.W1(features)\n",
        "\n",
        "        # Apply softmax activation to obtain attention weights\n",
        "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "        # Compute context vector as the weighted sum of features\n",
        "        context_vector = attention_weights * features\n",
        "\n",
        "        return context_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afdazjiz2oTz"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(100,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(embedding_layer)\n",
        "context_vector = Attention(8)(bilstm)\n",
        "simplernn = SimpleRNN(4, activation=\"tanh\")(context_vector)\n",
        "output = Dense(1, activation=\"sigmoid\")(simplernn)\n",
        "\n",
        "model_lstm_bi_embed_attention = Model(inputs=inputs, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzDtxkmU2qY5",
        "outputId": "e821050c-4f75-40a8-d79f-923db64595f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_21 (InputLayer)       [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_20 (Embedding)    (None, 100, 200)          7825600   \n",
            "                                                                 \n",
            " bidirectional_37 (Bidirect  (None, 100, 8)            6560      \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " attention_20 (Attention)    (None, 100, 8)            81        \n",
            "                                                                 \n",
            " simple_rnn_19 (SimpleRNN)   (None, 4)                 52        \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7832298 (29.88 MB)\n",
            "Trainable params: 6698 (26.16 KB)\n",
            "Non-trainable params: 7825600 (29.85 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp8CWBuA2snW",
        "outputId": "cba0b881-fd3e-4792-ede1-7d6c501f9dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 17s 77ms/step - loss: 0.6418 - accuracy: 0.6779 - val_loss: 0.5381 - val_accuracy: 0.8280\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.5023 - accuracy: 0.8053 - val_loss: 0.4550 - val_accuracy: 0.8300\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.4297 - accuracy: 0.8401 - val_loss: 0.4057 - val_accuracy: 0.8500\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 11s 71ms/step - loss: 0.3941 - accuracy: 0.8524 - val_loss: 0.4023 - val_accuracy: 0.8520\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 11s 69ms/step - loss: 0.3690 - accuracy: 0.8617 - val_loss: 0.3772 - val_accuracy: 0.8540\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.3457 - accuracy: 0.8738 - val_loss: 0.3716 - val_accuracy: 0.8580\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.3306 - accuracy: 0.8776 - val_loss: 0.3568 - val_accuracy: 0.8620\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 11s 68ms/step - loss: 0.3113 - accuracy: 0.8877 - val_loss: 0.3391 - val_accuracy: 0.8740\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.2991 - accuracy: 0.8959 - val_loss: 0.3549 - val_accuracy: 0.8580\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 11s 70ms/step - loss: 0.2912 - accuracy: 0.8987 - val_loss: 0.3703 - val_accuracy: 0.8520\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.4065 - accuracy: 0.8400\n",
            "Test Loss: 0.4064599871635437, Test Accuracy: 0.8399999737739563\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_model_lstm_bi_embed_attention = model_lstm_bi_embed_attention.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_dev, y_dev),verbose=1)\n",
        "\n",
        "loss, accuracy = model_lstm_bi_embed_attention.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='5'></a>\n",
        "## 5 - Model 2 - Attention Model with increased model complexity"
      ],
      "metadata": {
        "id": "XYa3jYORiBjw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iThqEoYxxJ-q"
      },
      "outputs": [],
      "source": [
        "inputs = Input(shape=(100,))\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=True))(embedding_layer)\n",
        "bilstm = Bidirectional(LSTM(128, activation='tanh',return_sequences=True))(bilstm)\n",
        "context_vector = Attention(64)(bilstm)\n",
        "simplernn = SimpleRNN(64, activation=\"tanh\",return_sequences=True)(context_vector)\n",
        "\n",
        "flatten = Flatten()(simplernn)\n",
        "\n",
        "ffn = Dense(64, activation='relu')(flatten)\n",
        "ffn = Dense(32, activation='relu')(ffn)\n",
        "\n",
        "output = Dense(1, activation=\"sigmoid\")(ffn)\n",
        "\n",
        "model_lstm_bi_embed_attention_complex = Model(inputs=inputs, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wixn6Jwqxyi-",
        "outputId": "a5a1fcfc-58c4-4b58-9926-4637adf35535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_25 (InputLayer)       [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_24 (Embedding)    (None, 100, 200)          7825600   \n",
            "                                                                 \n",
            " bidirectional_44 (Bidirect  (None, 100, 128)          135680    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_45 (Bidirect  (None, 100, 256)          263168    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " attention_24 (Attention)    (None, 100, 256)          16513     \n",
            "                                                                 \n",
            " simple_rnn_23 (SimpleRNN)   (None, 100, 64)           20544     \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 64)                409664    \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8673282 (33.09 MB)\n",
            "Trainable params: 847682 (3.23 MB)\n",
            "Non-trainable params: 7825600 (29.85 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention_complex.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rInkcRG-x0oV",
        "outputId": "e207ed49-2de0-490d-da72-f0de3b664267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 21s 87ms/step - loss: 0.4781 - accuracy: 0.7560 - val_loss: 0.4265 - val_accuracy: 0.8160\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 12s 77ms/step - loss: 0.3445 - accuracy: 0.8542 - val_loss: 0.4771 - val_accuracy: 0.8200\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 12s 76ms/step - loss: 0.2949 - accuracy: 0.8815 - val_loss: 0.3254 - val_accuracy: 0.8760\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 12s 76ms/step - loss: 0.2613 - accuracy: 0.8958 - val_loss: 0.3103 - val_accuracy: 0.8720\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 12s 76ms/step - loss: 0.2280 - accuracy: 0.9111 - val_loss: 0.3327 - val_accuracy: 0.8640\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 12s 76ms/step - loss: 0.1998 - accuracy: 0.9210 - val_loss: 0.3239 - val_accuracy: 0.8900\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 12s 73ms/step - loss: 0.1650 - accuracy: 0.9387 - val_loss: 0.3169 - val_accuracy: 0.8860\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 12s 75ms/step - loss: 0.3658 - accuracy: 0.8686 - val_loss: 0.3114 - val_accuracy: 0.8860\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 12s 74ms/step - loss: 0.1936 - accuracy: 0.9285 - val_loss: 0.3392 - val_accuracy: 0.8640\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 12s 74ms/step - loss: 0.1383 - accuracy: 0.9494 - val_loss: 0.3418 - val_accuracy: 0.8880\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3891 - accuracy: 0.8700\n",
            "Test Loss: 0.38913553953170776, Test Accuracy: 0.8700000047683716\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention_complex.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_model_lstm_bi_embed_attention_complex = model_lstm_bi_embed_attention_complex.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_dev, y_dev),verbose=1)\n",
        "\n",
        "loss, accuracy = model_lstm_bi_embed_attention_complex.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='6'></a>\n",
        "## 6 - Model 3 - Attention Model with increased model complexity and L2 regularization"
      ],
      "metadata": {
        "id": "S-BbOmyPiHtQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-c55_Cxx8gW"
      },
      "outputs": [],
      "source": [
        "from keras.regularizers import l2\n",
        "\n",
        "inputs = Input(shape=(100,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(64, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(embedding_layer)  # Applying L2 regularization\n",
        "bilstm = Bidirectional(LSTM(128, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(bilstm)  # Applying L2 regularization\n",
        "context_vector = Attention(64)(bilstm)\n",
        "simplernn = SimpleRNN(64, activation=\"tanh\", return_sequences=True, kernel_regularizer=l2(0.001))(context_vector)  # Applying L2 regularization\n",
        "\n",
        "flatten = Flatten()(simplernn)\n",
        "\n",
        "ffn = Dense(64, activation='relu', kernel_regularizer=l2(0.01))(flatten)  # Applying L2 regularization\n",
        "ffn = Dense(32, activation='relu', kernel_regularizer=l2(0.01))(ffn)  # Applying L2 regularization\n",
        "\n",
        "output = Dense(1, activation=\"sigmoid\")(ffn)\n",
        "\n",
        "model_lstm_bi_embed_attention_complex_regularized = Model(inputs=inputs, outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7smZLPXv37Je",
        "outputId": "e2f76920-794e-4c92-e265-8fd0ee17b47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_25 (InputLayer)       [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_24 (Embedding)    (None, 100, 200)          7825600   \n",
            "                                                                 \n",
            " bidirectional_44 (Bidirect  (None, 100, 128)          135680    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_45 (Bidirect  (None, 100, 256)          263168    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " attention_24 (Attention)    (None, 100, 256)          16513     \n",
            "                                                                 \n",
            " simple_rnn_23 (SimpleRNN)   (None, 100, 64)           20544     \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 64)                409664    \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8673282 (33.09 MB)\n",
            "Trainable params: 847682 (3.23 MB)\n",
            "Non-trainable params: 7825600 (29.85 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention_complex.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWoxTsZX48rz",
        "outputId": "9fe0ac7c-cef3-4829-948f-50ad5fd2006a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "313/313 [==============================] - 34s 81ms/step - loss: 0.8522 - accuracy: 0.6793 - val_loss: 0.5799 - val_accuracy: 0.8080\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 24s 75ms/step - loss: 0.5054 - accuracy: 0.8136 - val_loss: 0.4421 - val_accuracy: 0.8480\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 24s 76ms/step - loss: 0.4426 - accuracy: 0.8395 - val_loss: 0.4436 - val_accuracy: 0.8440\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 24s 75ms/step - loss: 0.3979 - accuracy: 0.8549 - val_loss: 0.3793 - val_accuracy: 0.8880\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 24s 75ms/step - loss: 0.3613 - accuracy: 0.8734 - val_loss: 0.3799 - val_accuracy: 0.8740\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 23s 75ms/step - loss: 0.3447 - accuracy: 0.8823 - val_loss: 0.4183 - val_accuracy: 0.8400\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 24s 75ms/step - loss: 0.3248 - accuracy: 0.8929 - val_loss: 0.3635 - val_accuracy: 0.8700\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.3111 - accuracy: 0.8934 - val_loss: 0.3575 - val_accuracy: 0.8820\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 23s 73ms/step - loss: 0.2940 - accuracy: 0.9022 - val_loss: 0.3337 - val_accuracy: 0.8820\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.2696 - accuracy: 0.9163 - val_loss: 0.3640 - val_accuracy: 0.8900\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4259 - accuracy: 0.8740\n",
            "Test Loss: 0.4259127974510193, Test Accuracy: 0.8740000128746033\n"
          ]
        }
      ],
      "source": [
        "model_lstm_bi_embed_attention_complex_regularized.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "history_model_lstm_bi_embed_attention_complex_regularized = model_lstm_bi_embed_attention_complex_regularized.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_dev, y_dev),verbose=1)\n",
        "\n",
        "loss, accuracy = model_lstm_bi_embed_attention_complex_regularized.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='7'></a>\n",
        "## 7 - Model 4 - Hypertuning the model parameters"
      ],
      "metadata": {
        "id": "JwgnJM-4iNsI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jr-OW_fH5VeT",
        "outputId": "d8e995e6-4f39-4742-eef2-04736546d00b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (24.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "pip install keras-tuner\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyorrlnmAHq4",
        "outputId": "0c9d376e-9314-4795-c47c-f994568def6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 03m 58s]\n",
            "val_accuracy: 0.515999972820282\n",
            "\n",
            "Best val_accuracy So Far: 0.9039999842643738\n",
            "Total elapsed time: 00h 52m 31s\n",
            "Epoch 1/10\n",
            "313/313 [==============================] - 31s 77ms/step - loss: 0.8653 - accuracy: 0.7216 - val_loss: 0.5203 - val_accuracy: 0.7840\n",
            "Epoch 2/10\n",
            "313/313 [==============================] - 22s 72ms/step - loss: 0.4649 - accuracy: 0.8291 - val_loss: 0.3962 - val_accuracy: 0.8560\n",
            "Epoch 3/10\n",
            "313/313 [==============================] - 22s 71ms/step - loss: 0.4399 - accuracy: 0.8362 - val_loss: 0.4352 - val_accuracy: 0.8360\n",
            "Epoch 4/10\n",
            "313/313 [==============================] - 22s 71ms/step - loss: 0.3949 - accuracy: 0.8627 - val_loss: 0.3611 - val_accuracy: 0.8660\n",
            "Epoch 5/10\n",
            "313/313 [==============================] - 23s 72ms/step - loss: 0.3590 - accuracy: 0.8775 - val_loss: 0.3384 - val_accuracy: 0.8960\n",
            "Epoch 6/10\n",
            "313/313 [==============================] - 22s 71ms/step - loss: 0.3348 - accuracy: 0.8880 - val_loss: 0.3715 - val_accuracy: 0.8660\n",
            "Epoch 7/10\n",
            "313/313 [==============================] - 22s 72ms/step - loss: 0.3165 - accuracy: 0.8968 - val_loss: 0.3211 - val_accuracy: 0.8900\n",
            "Epoch 8/10\n",
            "313/313 [==============================] - 22s 72ms/step - loss: 0.2982 - accuracy: 0.9021 - val_loss: 0.3448 - val_accuracy: 0.8860\n",
            "Epoch 9/10\n",
            "313/313 [==============================] - 23s 72ms/step - loss: 0.2825 - accuracy: 0.9120 - val_loss: 0.3464 - val_accuracy: 0.8960\n",
            "Epoch 10/10\n",
            "313/313 [==============================] - 23s 72ms/step - loss: 0.2634 - accuracy: 0.9200 - val_loss: 0.3144 - val_accuracy: 0.8900\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "\n",
        "import kerastuner as kt\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    inputs = Input(shape=(100,))\n",
        "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "    bilstm = Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(embedding_layer)  # Applying L2 regularization\n",
        "    bilstm = Bidirectional(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(bilstm)  # Applying L2 regularization\n",
        "    context_vector = Attention(hp.Int('attention_units', min_value=32, max_value=128, step=32))(bilstm)\n",
        "    simplernn = SimpleRNN(hp.Int('rnn_units', min_value=32, max_value=128, step=32), activation=\"tanh\", return_sequences=True, kernel_regularizer=l2(0.001))(context_vector)  # Applying L2 regularization\n",
        "\n",
        "    flatten = Flatten()(simplernn)\n",
        "\n",
        "    ffn = Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu', kernel_regularizer=l2(0.01))(flatten)  # Applying L2 regularization\n",
        "    ffn = Dense(hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu', kernel_regularizer=l2(0.01))(ffn)  # Applying L2 regularization\n",
        "\n",
        "    output = Dense(1, activation=\"sigmoid\")(ffn)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_epochs=10,\n",
        "                     factor=3,\n",
        "                     directory='./my_dir',  # Saving results to the current directory\n",
        "                     project_name='hyperparameter_search')\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=10, validation_data=(X_dev, y_dev))\n",
        "\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_dev, y_dev))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAbP6e-rAqtk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e8987d-2100-4a0b-f98b-987c6551cd99"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'lstm_units': 64,\n",
              " 'attention_units': 96,\n",
              " 'rnn_units': 64,\n",
              " 'dense_units': 128,\n",
              " 'learning_rate': 0.001,\n",
              " 'tuner/epochs': 10,\n",
              " 'tuner/initial_epoch': 0,\n",
              " 'tuner/bracket': 0,\n",
              " 'tuner/round': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "best_hps.get_config()['values']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='8'></a>\n",
        "## 8 - Model 4 - Attention Model Tuned with HyperParamerters on 10K dataset"
      ],
      "metadata": {
        "id": "_IvAY-S0iWso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "lstm_units = 64\n",
        "attention_units = 96\n",
        "rnn_units = 64\n",
        "dense_units = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "inputs = Input(shape=(100,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(embedding_layer)\n",
        "bilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(bilstm)\n",
        "context_vector = Attention(attention_units)(bilstm)\n",
        "simplernn = SimpleRNN(rnn_units, activation=\"tanh\", return_sequences=True, kernel_regularizer=l2(0.001))(context_vector)\n",
        "flatten = Flatten()(simplernn)\n",
        "ffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.01))(flatten)\n",
        "ffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.01))(ffn)\n",
        "output = Dense(1, activation=\"sigmoid\")(ffn)\n",
        "\n",
        "model_lstm_bi_embed_attention_complex_regularized_tuned = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model_lstm_bi_embed_attention_complex_regularized_tuned.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_lstm_bi_embed_attention_complex_regularized_tuned.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugk7Xt_gQpUl",
        "outputId": "c5933dc7-5a8b-4e09-e9ce-2eee73babc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 100, 200)          7825600   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirecti  (None, 100, 128)          135680    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, 100, 128)          98816     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " attention_2 (Attention)     (None, 100, 128)          12481     \n",
            "                                                                 \n",
            " simple_rnn_2 (SimpleRNN)    (None, 100, 64)           12352     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               819328    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8920898 (34.03 MB)\n",
            "Trainable params: 1095298 (4.18 MB)\n",
            "Non-trainable params: 7825600 (29.85 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history_model_lstm_bi_embed_attention_complex_regularized_tuned = model_lstm_bi_embed_attention_complex_regularized_tuned.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_dev, y_dev), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_lstm_bi_embed_attention_complex_regularized_tuned.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBgX-ByfSZxf",
        "outputId": "f391490d-ef80-4ebe-c168-a679e61b1c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.2935 - accuracy: 0.9049 - val_loss: 0.3507 - val_accuracy: 0.8920\n",
            "Epoch 2/50\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.2742 - accuracy: 0.9147 - val_loss: 0.3456 - val_accuracy: 0.8960\n",
            "Epoch 3/50\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.2555 - accuracy: 0.9252 - val_loss: 0.3871 - val_accuracy: 0.8720\n",
            "Epoch 4/50\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.2467 - accuracy: 0.9279 - val_loss: 0.3462 - val_accuracy: 0.8900\n",
            "Epoch 5/50\n",
            "313/313 [==============================] - 23s 75ms/step - loss: 0.2249 - accuracy: 0.9405 - val_loss: 0.3699 - val_accuracy: 0.8860\n",
            "Epoch 6/50\n",
            "313/313 [==============================] - 23s 74ms/step - loss: 0.2010 - accuracy: 0.9500 - val_loss: 0.4522 - val_accuracy: 0.8820\n",
            "Epoch 7/50\n",
            "313/313 [==============================] - 23s 75ms/step - loss: 0.1829 - accuracy: 0.9592 - val_loss: 0.4411 - val_accuracy: 0.8880\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3897 - accuracy: 0.8660\n",
            "Test Loss: 0.38966041803359985, Test Accuracy: 0.8659999966621399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model_lstm_bi_embed_attention_complex_regularized_tuned.evaluate(X_dev, y_dev)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Op2kyNCS7zN",
        "outputId": "5028fd23-bae9-4f1b-a2c2-912acbac8d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3456 - accuracy: 0.8960\n",
            "Test Loss: 0.345572829246521, Test Accuracy: 0.8960000276565552\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='9'></a>\n",
        "## 9 - Model 5 - Attention Model Tuned with HyperParamerters on 1 Lakh dataset i.e.; 0.1 Million"
      ],
      "metadata": {
        "id": "Kl2qHsrDifcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = traindata.sample(n=100000, random_state=42)\n",
        "test_data = testdata.sample(n=10000, random_state=42)\n",
        "\n",
        "train_data['class_index'] = train_data['class_index'].map({1:0, 2:1})\n",
        "test_data['class_index'] = test_data['class_index'].map({1:0, 2:1})\n",
        "\n",
        "train_data['review_combined_lemma'] = train_data['review_combined_lemma'].fillna('')\n",
        "test_data['review_combined_lemma'] = test_data['review_combined_lemma'].fillna('')\n",
        "\n",
        "X_train = train_data.review_combined_lemma\n",
        "y_train = np.array(train_data.class_index)\n",
        "\n",
        "X = test_data.review_combined_lemma\n",
        "y = np.array(test_data.class_index)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"<UNK>\",)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "tokenizer.word_index['<PAD>'] = 0\n",
        "\n",
        "X_sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "X_sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "X_train = pad_sequences(X_sequences_train, maxlen=100)\n",
        "X = pad_sequences(X_sequences, maxlen=100)\n",
        "\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_dev.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb2WEmJgU23K",
        "outputId": "1a1d98b4-ad97-41f0-8cdc-20c5ec8d616f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 100)\n",
            "(5000, 100)\n",
            "(5000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index)\n",
        "word2idx = tokenizer.word_index\n",
        "\n",
        "embedding_matrix_twitter_200d = create_embedding_matrix(embedding_index_Twitter_200d, word2idx, vocab_size, 200)"
      ],
      "metadata": {
        "id": "O5vsKB11U-TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "lstm_units = 64\n",
        "attention_units = 96\n",
        "rnn_units = 64\n",
        "dense_units = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "inputs = Input(shape=(100,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n",
        "bilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(embedding_layer)\n",
        "bilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(bilstm)\n",
        "context_vector = Attention(attention_units)(bilstm)\n",
        "simplernn = SimpleRNN(rnn_units, activation=\"tanh\", return_sequences=True, kernel_regularizer=l2(0.0001))(context_vector)\n",
        "flatten = Flatten()(simplernn)\n",
        "ffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001))(flatten)\n",
        "ffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001))(ffn)\n",
        "output = Dense(1, activation=\"sigmoid\")(ffn)\n",
        "\n",
        "model_lstm_bi_embed_attention_complex_regularized_tuned = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Adam(learning_rate)\n",
        "model_lstm_bi_embed_attention_complex_regularized_tuned.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model_lstm_bi_embed_attention_complex_regularized_tuned.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koPUOnd0VIre",
        "outputId": "9b256c28-6805-4479-b1ad-de9217a5ea50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, 100, 200)          36072400  \n",
            "                                                                 \n",
            " bidirectional_10 (Bidirect  (None, 100, 128)          135680    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " bidirectional_11 (Bidirect  (None, 100, 128)          98816     \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " attention_4 (Attention)     (None, 100, 128)          12481     \n",
            "                                                                 \n",
            " simple_rnn_4 (SimpleRNN)    (None, 100, 64)           12352     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 6400)              0         \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 128)               819328    \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37167698 (141.78 MB)\n",
            "Trainable params: 1095298 (4.18 MB)\n",
            "Non-trainable params: 36072400 (137.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "history_model_lstm_bi_embed_attention_complex_regularized_tuned = model_lstm_bi_embed_attention_complex_regularized_tuned.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_dev, y_dev), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_lstm_bi_embed_attention_complex_regularized_tuned.evaluate(X_test, y_test)\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtlE01BZVOu_",
        "outputId": "546f747f-05b1-4efd-c210-6a8ad0f0b99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "3125/3125 [==============================] - 220s 68ms/step - loss: 0.3667 - accuracy: 0.8648 - val_loss: 0.3111 - val_accuracy: 0.8830\n",
            "Epoch 2/50\n",
            "3125/3125 [==============================] - 213s 68ms/step - loss: 0.2764 - accuracy: 0.9005 - val_loss: 0.2824 - val_accuracy: 0.8960\n",
            "Epoch 3/50\n",
            "3125/3125 [==============================] - 211s 68ms/step - loss: 0.2553 - accuracy: 0.9099 - val_loss: 0.2676 - val_accuracy: 0.9018\n",
            "Epoch 4/50\n",
            "3125/3125 [==============================] - 209s 67ms/step - loss: 0.2438 - accuracy: 0.9151 - val_loss: 0.2733 - val_accuracy: 0.8984\n",
            "Epoch 5/50\n",
            "3125/3125 [==============================] - 209s 67ms/step - loss: 0.2390 - accuracy: 0.9198 - val_loss: 0.7372 - val_accuracy: 0.4946\n",
            "Epoch 6/50\n",
            "3125/3125 [==============================] - 207s 66ms/step - loss: 0.2892 - accuracy: 0.9109 - val_loss: 0.2930 - val_accuracy: 0.8944\n",
            "Epoch 7/50\n",
            "3125/3125 [==============================] - 208s 67ms/step - loss: 0.2623 - accuracy: 0.9174 - val_loss: 0.3470 - val_accuracy: 0.9028\n",
            "Epoch 8/50\n",
            "3125/3125 [==============================] - 209s 67ms/step - loss: 0.2982 - accuracy: 0.8979 - val_loss: 0.3047 - val_accuracy: 0.9012\n",
            "157/157 [==============================] - 2s 14ms/step - loss: 0.2499 - accuracy: 0.9126\n",
            "Test Loss: 0.24991442263126373, Test Accuracy: 0.9125999808311462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open('model_lstm_bi_embed_attention_complex_regularized_tuned.pkl', 'wb') as f:\n",
        "    pickle.dump(model_lstm_bi_embed_attention_complex_regularized_tuned, f)"
      ],
      "metadata": {
        "id": "3bJ3V1rHVVCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A4DDDbCmfmT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM8bIbogFwv0ozPQOxXzxeI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
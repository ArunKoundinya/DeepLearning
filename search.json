[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is dedicated for learnings on deep learning using tensor flow.\nMy other sites."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html",
    "href": "posts/tensor-basics-part2/index.html",
    "title": "Tensor Flow Basics - 2",
    "section": "",
    "text": "This article primarily discussed on importance of broadcasting and its easy of implementation using tensors."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "href": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "title": "Tensor Flow Basics - 2",
    "section": "Tensor data types",
    "text": "Tensor data types\n\n\n\n\n\n\n\n\nData Type\nPython Type\nDescription\n\n\n\n\nDT_FLOAT\ntf.float32\n32 bits floating point.\n\n\nDT_DOUBLE\ntf.float64\n64 bits floating point.\n\n\nDT_INT8\ntf.int8\n8 bits signed integer.\n\n\nDT_INT16\ntf.int16\n16 bits signed integer.\n\n\nDT_INT32\ntf.int32\n32 bits signed integer.\n\n\nDT_INT64\ntf.int64\n64 bits signed integer.\n\n\nDT_UINT8\ntf.uint8\n8 bits unsigned integer.\n\n\nDT_STRING\ntf.string\nVariable length byte arrays. Each element of a tensor is a byte array.\n\n\nDT_BOOL\ntf.bool\nBoolean.\n\n\n\n\nimport tensorflow as tf\nprint(tf.__version__)\n\n2.15.0\n\n\n\nx_new = tf.constant(1,shape=(2,3),dtype=tf.int8)\n\n\ny_new = tf.constant(2,shape=(2,3),dtype=tf.int8)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "href": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "title": "Tensor Flow Basics - 2",
    "section": "Re-cap Operations",
    "text": "Re-cap Operations\n\nElement Wise Operations\nMatrix Multiplicaitons\n\n\nprint(\"Addition:\\n\", tf.add(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nprint(\"Subtraction:\\n\", tf.subtract(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nAddition:\n tf.Tensor(\n[[3 3 3]\n [3 3 3]], shape=(2, 3), dtype=int8)\n \n \nSubtraction:\n tf.Tensor(\n[[-1 -1 -1]\n [-1 -1 -1]], shape=(2, 3), dtype=int8)\n \n \n\n\n\nprint(\"Multiplication:\\n\", tf.multiply(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(2,3))))\n\nMultiplication:\n tf.Tensor(\n[[-0.5718114  -0.18780218 -2.0768495 ]\n [ 0.29304612  0.08317164 -1.3320862 ]], shape=(2, 3), dtype=float32)\n\n\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,2))))\nprint(\" \")\nprint(\" \")\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,1))))\n\nMatrix Multiplication:\n tf.Tensor(\n[[-1.6848389  -0.44041786]\n [-0.9548799  -0.22109865]], shape=(2, 2), dtype=float32)\n \n \nMatrix Multiplication:\n tf.Tensor(\n[[ 0.37789747]\n [-0.12570143]], shape=(2, 1), dtype=float32)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#broadcasting",
    "href": "posts/tensor-basics-part2/index.html#broadcasting",
    "title": "Tensor Flow Basics - 2",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is useful when the dimensions of matrices are different from each other. This is highly useful to add and subtract the matrices of different lengths.\nHere’s a brief summary of how broadcasting works for addition and subtraction:\nAddition: If the shapes of the two tensors are different, TensorFlow compares their shapes element-wise, starting from the trailing dimensions. If the dimensions are compatible or one of the dimensions is 1, broadcasting can occur. For example, you can add a scalar to a matrix, and the scalar value will be added to every element of the matrix.\nSubtraction: Similar to addition, broadcasting allows you to subtract a scalar or a vector from a matrix, or subtract matrices of different shapes.\n\nx1 = tf.random.normal(shape=(2,3))\n\nx2 = tf.random.normal(shape=(2,1))\n\n\nprint(\"x1:\\n\",x1)\nprint(\" \")\nprint(\" \")\nprint(\"x2:\\n\",x2)\n\nx1:\n tf.Tensor(\n[[ 0.39297998 -0.12811422 -0.60474324]\n [ 0.30773026  1.4076523  -0.57274765]], shape=(2, 3), dtype=float32)\n \n \nx2:\n tf.Tensor(\n[[0.10113019]\n [1.0277312 ]], shape=(2, 1), dtype=float32)\n\n\n\ntf.broadcast_to(x2, [2, 3])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.10113019, 0.10113019, 0.10113019],\n       [1.0277312 , 1.0277312 , 1.0277312 ]], dtype=float32)&gt;\n\n\n\ntf.add(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.49411017, -0.02698404, -0.50361305],\n       [ 1.3354614 ,  2.4353833 ,  0.45498353]], dtype=float32)&gt;\n\n\n\ntf.subtract(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.2918498 , -0.22924441, -0.7058734 ],\n       [-0.7200009 ,  0.37992108, -1.6004789 ]], dtype=float32)&gt;\n\n\nIn the above examples we have seen that broadcasting of same tensor dimension i.e.; of two dimensional.\nBelow, we will see the beauty of broadcasting of different dimensions.\n\nx3 = tf.random.normal(shape=(2, 2, 2)) #3d tensor\nx4 = tf.random.normal(shape=(2, 1)) #2d tensor\n\n\ntf.add(x3,x4)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-1.0228443 ,  0.34588122],\n        [ 2.188323  , -0.7523911 ]],\n\n       [[-0.56017506, -1.5636952 ],\n        [ 1.572252  ,  1.6189265 ]]], dtype=float32)&gt;\n\n\n\ntf.broadcast_to(x4, [2,2,2])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]],\n\n       [[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]]], dtype=float32)&gt;"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html",
    "href": "posts/shallow-linear-model/index.html",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "",
    "text": "This article primarly discusses on implementation of simple linear regression in both sklearn and tensorflow.\nimport tensorflow as tf\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nWe will consider a toy dataset of diabetes, described below, to perform both linear regression using OLS and shallow neural networks, and learn from both approaches.\ndata = load_diabetes()\nprint(data['DESCR'])\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nprint(\"Number of Independent features: \", data['data'].shape[1])\nprint(\"Number of Training Instances: \", data['target'].shape[0])\n\nNumber of Independent features:  10\nNumber of Training Instances:  442\nLet us divide the entire dataset into train and test set\nX= data['data']\ny = data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#linear-regression-sci-kit-learn",
    "href": "posts/shallow-linear-model/index.html#linear-regression-sci-kit-learn",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Linear Regression Sci-kit Learn",
    "text": "Linear Regression Sci-kit Learn\n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\nMean Squared Error: 2900.193628493482\n\n\n\nprint(\"Model Coefficients: \\n\", model.coef_)\n\nModel Coefficients: \n [  37.90402135 -241.96436231  542.42875852  347.70384391 -931.48884588\n  518.06227698  163.41998299  275.31790158  736.1988589    48.67065743]\n\n\n\nprint(\"Model Intercept: \\n\", model.intercept_)\n\nModel Intercept: \n 151.34560453985995"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#shallow-neural-network---linear-regression",
    "href": "posts/shallow-linear-model/index.html#shallow-neural-network---linear-regression",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Shallow Neural Network - Linear Regression",
    "text": "Shallow Neural Network - Linear Regression\nWe can view the shallow neural network for linear regression in below way\n\n\ny_reshaped = y.reshape(-1, 1)\n\n# Concatenate y as an additional column to X\nX_with_y = np.concatenate((X, y_reshaped), axis=1)\n\n# Compute the correlation matrix\ncorrelation_matrix = np.corrcoef(X_with_y, rowvar=False)\n\n\nprint(np.round(correlation_matrix,1))\n\n[[ 1.   0.2  0.2  0.3  0.3  0.2 -0.1  0.2  0.3  0.3  0.2]\n [ 0.2  1.   0.1  0.2  0.   0.1 -0.4  0.3  0.1  0.2  0. ]\n [ 0.2  0.1  1.   0.4  0.2  0.3 -0.4  0.4  0.4  0.4  0.6]\n [ 0.3  0.2  0.4  1.   0.2  0.2 -0.2  0.3  0.4  0.4  0.4]\n [ 0.3  0.   0.2  0.2  1.   0.9  0.1  0.5  0.5  0.3  0.2]\n [ 0.2  0.1  0.3  0.2  0.9  1.  -0.2  0.7  0.3  0.3  0.2]\n [-0.1 -0.4 -0.4 -0.2  0.1 -0.2  1.  -0.7 -0.4 -0.3 -0.4]\n [ 0.2  0.3  0.4  0.3  0.5  0.7 -0.7  1.   0.6  0.4  0.4]\n [ 0.3  0.1  0.4  0.4  0.5  0.3 -0.4  0.6  1.   0.5  0.6]\n [ 0.3  0.2  0.4  0.4  0.3  0.3 -0.3  0.4  0.5  1.   0.4]\n [ 0.2  0.   0.6  0.4  0.2  0.2 -0.4  0.4  0.6  0.4  1. ]]\n\n\nI’m using the above correlation matrix as weight initializer to check whether the weights will be in similar way to that of the regular OLS.\n\nno_of_features = X.shape[1]\n\ninitializer_weights = tf.keras.initializers.Constant(value= [0.2,0.2,0.3,0.3,0.2,-0.1,0.2,0.3,0.3,0.2])\n\ninitializer_bias = tf.keras.initializers.Constant(value = [1.0])\n\n\nmodel_tf = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1,\n                          activation=\"linear\",\n                          input_shape=(no_of_features,),\n                          kernel_initializer = initializer_weights,\n                          bias_initializer = initializer_bias)\n])\n\nmodel_tf.compile(optimizer='sgd', loss='mean_squared_error',metrics=['mse'])\n\nmodel_tf.fit(X_train, y_train, epochs=1500, verbose=0)\n\ny_pred_test = model_tf.predict(X_test)\n\n3/3 [==============================] - 0s 4ms/step\n\n\n\nmodel_tf.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_7 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 11 (44.00 Byte)\nTrainable params: 11 (44.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nAs we can see above there is just one layer connecting the paramters;\n11 Paramters are 10 weight paramters and 1 bias parameter that needs to be learned by the neural network\n\nmse_tf = mean_squared_error(y_test, y_pred_test.flatten())\nprint(\"Mean Squared Error:\", mse_tf)\n\nMean Squared Error: 2925.001255544474\n\n\n\nweights, bias = model_tf.layers[0].get_weights()\nprint(\"Weights:\")\nprint(weights)\nprint(\"Bias:\")\nprint(bias)\n\nWeights:\n[[ 5.9075676e+01]\n [-9.0772835e+01]\n [ 3.6696533e+02]\n [ 2.5461090e+02]\n [ 9.4142288e-02]\n [-3.7435982e+01]\n [-1.8325632e+02]\n [ 1.4889064e+02]\n [ 2.8909210e+02]\n [ 1.5162097e+02]]\nBias:\n[153.55777]"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#comparision-of-parameters",
    "href": "posts/shallow-linear-model/index.html#comparision-of-parameters",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Comparision of Parameters",
    "text": "Comparision of Parameters\n\nimport pandas as pd\n\n\npd.concat(\n    [pd.DataFrame(model.coef_, columns=[\"Model Coefficients of OLS\"]),\n    pd.DataFrame(weights,columns=[\"Model Coefficients of NN\"])],\n    axis = 1\n)\n\n\n\n  \n    \n\n\n\n\n\n\nModel Coefficients of OLS\nModel Coefficients of NN\n\n\n\n\n0\n37.904021\n59.075676\n\n\n1\n-241.964362\n-90.772835\n\n\n2\n542.428759\n366.965332\n\n\n3\n347.703844\n254.610901\n\n\n4\n-931.488846\n0.094142\n\n\n5\n518.062277\n-37.435982\n\n\n6\n163.419983\n-183.256317\n\n\n7\n275.317902\n148.890640\n\n\n8\n736.198859\n289.092102\n\n\n9\n48.670657\n151.620972\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAt this juncture, it’s evident that the weights in neural networks are derived through iterative optimization methods, notably gradient descent, optimizing model performance. Conversely, OLS regression employs statistical techniques to determine model coefficients. Consequently, the fundamental nature of these weights differs significantly.\nAt present, it’s feasible to manually interpret the coefficients derived from OLS regression, aiding in understanding the relationships between variables.\nHowever, this interpretability is not readily achievable with neural networks, underscoring their characterization as black box models primarily designed for prediction rather than inference.\nIt seems there is ongoing research seeks to enhance interpretability in neural networks which i’m not yet aware at this point of time."
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html",
    "href": "posts/tensor-basics-part1/index.html",
    "title": "Tensor Flow Basics -1",
    "section": "",
    "text": "These are initial learnings on tensors hope it is helpful for everyone\nimport tensorflow as tf"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "href": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "title": "Tensor Flow Basics -1",
    "section": "What is tensor Flow?",
    "text": "What is tensor Flow?\nTensor flow is deep learning framework developed by Google in 2011 and made it publicly available in the year 2015.\nIt is flexible, scalable solutio that enable us to build models with the existing frameworks.\nIt is like sci-kit learn library but more advanced and flexible as we can custom build our own neural network.\n\nprint(tf.__version__)\n\n2.15.0"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "href": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "title": "Tensor Flow Basics -1",
    "section": "Basics Unit in TensorFlow Framework - Tensor",
    "text": "Basics Unit in TensorFlow Framework - Tensor\nTensors are multi-dimensional arrays designed for numerical data representation; although they share some similarities with NumPy arrays, they possess certain unique features that give them an advantage in deep learning tasks. One of these key advantages is their ability to utilize hardware acceleration from GPUs and TPUs to significantly speed up computational operations, which is especially useful when working with input data such as images, text, and videos.\nIn simple words “ML” needs numbers; In case of large dimensions we need matrices. These matrices are called tensors. As these are specially designed to use hardware capabilities to accelerate learnings."
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#creating-tensors",
    "href": "posts/tensor-basics-part1/index.html#creating-tensors",
    "title": "Tensor Flow Basics -1",
    "section": "Creating Tensors",
    "text": "Creating Tensors\n\ntf.constant(1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 1],\n       [1, 1]], dtype=int32)&gt;\n\n\nHere we have created a basic tensor with constant number 1 with a shape 2,2 i.e.; two rows and two columns.\nAnd its datatype is integer.\nIt is a numpy array.\n\n## Manually providing the shape\ny = tf.constant([[1, 2, 3], [4, 5, 6]])\nprint(y)\n\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nInstead of giving the shape here we have manually given the values\n\ntf.rank(y)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\nHere we are checking what is the rank of the tensor.\n\n\nprint(\"The rank of scalar is \" , tf.rank(tf.constant(1)))\nprint(\"The rank of vector is \" , tf.rank(tf.constant(1,shape=(5))))\nprint(\"The rank of matrix is \" , tf.rank(tf.constant(1,shape=(5,4))))\nprint(\"The rank of rank3tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3))))\n\nThe rank of scalar is  tf.Tensor(0, shape=(), dtype=int32)\nThe rank of vector is  tf.Tensor(1, shape=(), dtype=int32)\nThe rank of matrix is  tf.Tensor(2, shape=(), dtype=int32)\nThe rank of rank3tensor is  tf.Tensor(3, shape=(), dtype=int32)\n\n\nCan there me more than 3 dimensional; of course, but we cannot represent them pictographically.\n\nprint(\"The rank of rank5tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3))))\nprint(\"The rank of rank9tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3,1,1,3,3))))\n\nThe rank of rank5tensor is  tf.Tensor(5, shape=(), dtype=int32)\nThe rank of rank9tensor is  tf.Tensor(9, shape=(), dtype=int32)"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "href": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "title": "Tensor Flow Basics -1",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\ntf.constant(1.1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1.1, 1.1],\n       [1.1, 1.1]], dtype=float32)&gt;\n\n\n\nTypeCasting\nThe moment we kept 1.1 the data-type has changed to float; Lets check how to typecast integer to float and vice versa\n\nx_int = tf.constant(1, shape=(2,2))\nprint(x_int)\nx_float = tf.cast(x_int, dtype = tf.float32)\nprint(x_float)\nx_float_int = tf.cast(x_float, tf.int32)\nprint(x_float_int)\n\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\n\n\n\n\nIndexing\nSimilar to numpy array we can do indexing for the tensors\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\ny[0]\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\ny[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n\nExpanding a matrix\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(tf.expand_dims(y,axis=0)) ## Expanding at the beginning of the tensor\nprint(tf.expand_dims(y,axis=1)) ## Expanding at the Middle of the tensor { for this example}\nprint(tf.expand_dims(y,axis=-1)) ## Expanding at the End of the tensor\n\ntf.Tensor(\n[[[1 2 3]\n  [4 5 6]]], shape=(1, 2, 3), dtype=int32)\ntf.Tensor(\n[[[1 2 3]]\n\n [[4 5 6]]], shape=(2, 1, 3), dtype=int32)\ntf.Tensor(\n[[[1]\n  [2]\n  [3]]\n\n [[4]\n  [5]\n  [6]]], shape=(2, 3, 1), dtype=int32)\n\n\n\n\nTensor Aggregation\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(\"Smallest of the number is \",tf.reduce_min(y).numpy())\nprint(\"Largest of the number is \",tf.reduce_max(y).numpy())\n\nSmallest of the number is  1\nLargest of the number is  6\n\n\n\nprint(\"Sum of the numbers are \",tf.reduce_sum(y).numpy())\nprint(\"Average of the numbers are \",tf.reduce_mean(y).numpy())\n\nSum of the numbers are  21\nAverage of the numbers are  3\n\n\n\n\nMatrix with all ones,zeroes and identity\n\nz =  tf.ones([2,3])\nprint(z)\nprint(\" \")\n\nx  =  tf.constant(1,shape=(2,3),dtype=tf.float32)\nprint(x)\nprint(\" \")\n\nz =  tf.zeros([2,3])\nprint(z)\nprint(\" \")\n\nz = tf.eye(3)\nprint(z)\n\ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[0. 0. 0.]\n [0. 0. 0.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float32)\n\n\n\n\nReshaping and Transposing Tensors\n\nx_initial = tf.constant(1, shape=(4,3))\nprint(x_initial)\n\ntf.Tensor(\n[[1 1 1]\n [1 1 1]\n [1 1 1]\n [1 1 1]], shape=(4, 3), dtype=int32)\n\n\n\ntf.reshape(x_initial,shape=(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[1, 1, 1],\n        [1, 1, 1]],\n\n       [[1, 1, 1],\n        [1, 1, 1]]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(2,6))\n\n&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(12,1))\n\n&lt;tf.Tensor: shape=(12, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1]], dtype=int32)&gt;\n\n\nHere we can reshape to any other shape; However, the multiplication of shapes should remain the same.\n\ntf.reshape(x_initial,shape=-1) #Flatten the array\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)&gt;\n\n\n\ntf.transpose(x_initial)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[1, 1, 1, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 1]], dtype=int32)&gt;\n\n\nThe initial shape of (4,3) got changed to (3,4)\n\n\nDistributions\n\nx1  = tf.random.normal((3,3))\nprint(x1)\nprint(\" \")\n\nx1  = tf.random.normal((3,3),mean = 0, stddev =1 )\nprint(x1)\nprint(\" \")\n\nx2 =  tf.random.uniform((3,3))\nprint(x2)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.591363    0.12791212  0.38762185]\n [ 0.26025018  1.7209182  -0.7802837 ]\n [ 0.89150804 -0.9648455   0.64507854]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-1.0034778   0.05435322 -1.3141975 ]\n [-0.17819698 -1.9136705  -0.9396771 ]\n [ 2.2143493   0.33600262  0.8174351 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[0.04812372 0.79855204 0.8067709 ]\n [0.67069924 0.06617999 0.14941025]\n [0.31500185 0.17441607 0.7476181 ]], shape=(3, 3), dtype=float32)\n \n\n\n\n\nMathematical Operations\n\nAddition\nSubtraction\nMultiplication\nDivision\n\n\nx4 = tf.random.normal((3,3))\nprint(x4)\nprint(\" \")\ny4 = tf.random.normal((3,3))\nprint(y4)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.5908952   3.5452905  -0.34438497]\n [-0.5237503   1.2899861  -0.50684774]\n [ 1.2187229   0.50014    -0.6212071 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-0.7346279   2.9705956  -0.45318994]\n [-0.947753    0.6556651   3.018978  ]\n [-1.5799906  -1.2278746   0.57451475]], shape=(3, 3), dtype=float32)\n \n\n\n\nprint(x4+y4)\nprint(\" \")\ntf.add(x4,y4)\n\ntf.Tensor(\n[[-0.14373273  6.5158863  -0.7975749 ]\n [-1.4715033   1.9456513   2.5121303 ]\n [-0.3612677  -0.7277346  -0.04669237]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.14373273,  6.5158863 , -0.7975749 ],\n       [-1.4715033 ,  1.9456513 ,  2.5121303 ],\n       [-0.3612677 , -0.7277346 , -0.04669237]], dtype=float32)&gt;\n\n\n\nprint(x4-y4)\nprint(\" \")\ntf.subtract(x4,y4)\n\ntf.Tensor(\n[[ 1.3255231   0.5746949   0.10880497]\n [ 0.4240027   0.63432103 -3.525826  ]\n [ 2.7987137   1.7280147  -1.1957219 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 1.3255231 ,  0.5746949 ,  0.10880497],\n       [ 0.4240027 ,  0.63432103, -3.525826  ],\n       [ 2.7987137 ,  1.7280147 , -1.1957219 ]], dtype=float32)&gt;\n\n\n\nprint(x4*y4)\nprint(\" \")\ntf.multiply(x4,y4)\n\ntf.Tensor(\n[[-0.43408808 10.531624    0.1560718 ]\n [ 0.49638593  0.8457989  -1.5301622 ]\n [-1.9255708  -0.6141092  -0.35689265]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.43408808, 10.531624  ,  0.1560718 ],\n       [ 0.49638593,  0.8457989 , -1.5301622 ],\n       [-1.9255708 , -0.6141092 , -0.35689265]], dtype=float32)&gt;\n\n\n\nprint(x4/y4)\nprint(\" \")\ntf.divide(x4,y4)\n\ntf.Tensor(\n[[-0.8043462   1.1934612   0.7599131 ]\n [ 0.5526232   1.9674467  -0.16788718]\n [-0.77134824 -0.40732172 -1.0812727 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.8043462 ,  1.1934612 ,  0.7599131 ],\n       [ 0.5526232 ,  1.9674467 , -0.16788718],\n       [-0.77134824, -0.40732172, -1.0812727 ]], dtype=float32)&gt;\n\n\n\n\nMatrix Multiplications\n\nx4_new = tf.random.normal((3,2))\ny4_new = tf.random.normal((2,3))\n\nprint(tf.matmul(x4_new,y4_new))\n\ntf.Tensor(\n[[ 0.5675167   0.29923582 -2.018334  ]\n [-0.5145724   1.9392778  -1.5560541 ]\n [ 1.0566927  -2.3777525   0.73752195]], shape=(3, 3), dtype=float32)\n\n\n\nx4_new @ y4_new\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.5675167 ,  0.29923582, -2.018334  ],\n       [-0.5145724 ,  1.9392778 , -1.5560541 ],\n       [ 1.0566927 , -2.3777525 ,  0.73752195]], dtype=float32)&gt;\n\n\nTwo ways of matrix of multiplication in tensors; There is also a way using dot product; which we will discuss later :)"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html",
    "href": "posts/gradient_tape_learning/index.html",
    "title": "Basic Tensor Gradient Tape",
    "section": "",
    "text": "This blog has been created out of curiosity to develop gradient descent from scratch rather than using the gradient descent algorithm directly.\nThis has been a good learning experience for me, and I have created it as a blog post for both my future reference and for sharing what I’ve learned"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#loading-libraries",
    "href": "posts/gradient_tape_learning/index.html#loading-libraries",
    "title": "Basic Tensor Gradient Tape",
    "section": "0. Loading Libraries",
    "text": "0. Loading Libraries\n\nimport tensorflow as tf\nimport numpy as np"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#first-derivative-at-one-point",
    "href": "posts/gradient_tape_learning/index.html#first-derivative-at-one-point",
    "title": "Basic Tensor Gradient Tape",
    "section": "1. First Derivative (At one point)",
    "text": "1. First Derivative (At one point)\n\n1.1. First Derivate for single variable\n\nx = tf.constant(100.0)\nb = tf.constant(10.0)\n\nwith tf.GradientTape() as tape:\n  tape.watch(x)\n  y = x ** 2 + b\n  dy_dx = tape.gradient(y, x)\n\ndel tape\n\n\nprint(dy_dx)\n\ntf.Tensor(200.0, shape=(), dtype=float32)\n\n\nFor equation x**2 +b the first derivate at point where x=100 is 200\n\n\n1.2. First Derivate with two variables\nWhen calculating two derivative it is mandatory to define as persistent=True\n\nx = tf.constant(20.0)\nb = tf.constant(10.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n  tape.watch(x)\n  tape.watch(b)\n  y = x ** 2 + b ** 2\n  dy_dx = tape.gradient(y, x)\n  dy_db = tape.gradient(y, b)\n\ndel tape\n\nWARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\nWARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n\n\n\nprint(dy_dx)\nprint(dy_db)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nFor equation x**2 + b**2 the first derivate at point where x=20 is 40 and where b=10 is 20\n\n\n1.3. First Derivate with two variables - Simpler Code\n\n1.3.1. Using tf.constant - No output\nWe when remove tape.watch(x) it is important for us to define as tf.Variable as gradient needs to be calculated iteratively at that point\n\nx = tf.constant(20.0)\nb = tf.constant(10.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    y = x ** 2 + b ** 2\ndy_dx, dy_db = tape.gradient(y, [x, b])\n\n\nprint(dy_dx)\nprint(dy_db)\n\nNone\nNone\n\n\n\n\n1.3.2. Using tf.Variable - Output\nAlso, using simpler code we can see we can pass variables in a list\n\nx = tf.Variable(20.0)\nb = tf.Variable(10.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    y = x ** 2 + b ** 2\ndy_dx, dy_db = tape.gradient(y, [x, b])\n\n\nprint(dy_dx)\nprint(dy_db)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\ntf.Tensor(20.0, shape=(), dtype=float32)"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#second-derivate-using-one-variable",
    "href": "posts/gradient_tape_learning/index.html#second-derivate-using-one-variable",
    "title": "Basic Tensor Gradient Tape",
    "section": "2. Second Derivate using one variable",
    "text": "2. Second Derivate using one variable\n\n2.1. Wrong indentation of code\nThe issue with the below is code is about code indentation when we need to calculate second derivative.\n\nx = tf.Variable(20.0)\nb = tf.Variable(10.0)\n\nwith tf.GradientTape(persistent=True) as tape2:\n  with tf.GradientTape(persistent=True) as tape1:\n    y = x ** 2 + b ** 2\ndy_dx = tape1.gradient(y, x)\ndy_dx_1 = tape2.gradient(dy_dx, x)\n\n\nprint(dy_dx)\nprint(dy_dx_1)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\nNone\n\n\n\n\n2.2. With right indentation of code\n\nx = tf.Variable(20.0)\nb = tf.Variable(10.0)\n\nwith tf.GradientTape(persistent=True) as tape2:\n  with tf.GradientTape(persistent=True) as tape1:\n    y = x ** 2 + b ** 2\n  dy_dx = tape1.gradient(y, x)\ndy_dx_1 = tape2.gradient(dy_dx, x)\n\n\nprint(dy_dx)\nprint(dy_dx_1)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\n\n\nFor equation x**2 + b**2 the first derivate at point where x=20 is 40 and where b=10 is 20\n\n\n2.3. Second Order Derivate for array of numbers\n\nx = tf.Variable([-3,-2,-1,0,1,2,3],dtype=tf.float32)\n\nwith tf.GradientTape(persistent=True) as tape2:\n  with tf.GradientTape(persistent=True) as tape1:\n    y = tf.math.square(x)\n  dy_dx = tape1.gradient(y, x)\ndy_dx_1 = tape2.gradient(dy_dx, x)\n\n\nprint(dy_dx)\nprint(dy_dx_1)\n\ntf.Tensor([-6. -4. -2.  0.  2.  4.  6.], shape=(7,), dtype=float32)\ntf.Tensor([2. 2. 2. 2. 2. 2. 2.], shape=(7,), dtype=float32)"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#gradient-descent-function",
    "href": "posts/gradient_tape_learning/index.html#gradient-descent-function",
    "title": "Basic Tensor Gradient Tape",
    "section": "3.0 Gradient Descent Function",
    "text": "3.0 Gradient Descent Function\n\n\n\n\n\nHere we will try to create a gradient descent function which will iterative to calculate the derivate and update the weights as per the learning rate.\n\ndef gradientdescent(learning_rate, w0):\n  with tf.GradientTape() as tape:\n    y = tf.math.square(w0)\n\n  dy_dw0 = tape.gradient(y, w0)\n  w0 = w0 - learning_rate * dy_dw0\n  return w0\n\n\nw0 = tf.Variable(1.0,dtype=tf.float32)\n\nBelow we are running for 10k epochs to arrive at the minimal value given the function y = x^2 which is nothing but a parabola.\n\nfor i in range(10000):\n  w0 = tf.Variable(gradientdescent(0.01,w0).numpy(),dtype=tf.float32)\n\n\nw0.numpy()\n\n5.803526e-37\n\n\nAfter running for 10K epochs we can clearly observe how we have arrived at almost 0 value.\n\nw0 = tf.Variable(1.0,dtype=tf.float32)\n\nweights = []\nfor i in range(10000):\n  weights.append(w0.numpy())\n  w0 = tf.Variable(gradientdescent(0.01,w0).numpy(),dtype=tf.float32)\n  \nimport pandas as pd\nfrom plotnine import *\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'epoch': range(10000), 'w0': weights})\n\n# Plot the data using ggplot\n(ggplot(df, aes(x='epoch', y='w0'))\n + geom_line()\n + labs(title='w0 over epochs', x='Epoch', y='w0')\n + theme_minimal())\n\n\nAs we can see clearly how we have successfully performed gradient descent for a toy example"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html",
    "href": "posts/deep-learning-project-msis/index.html",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "",
    "text": "This article explores the use of deep learning models, such as feed forward neural networks (FFNN) and recurrent neural networks (RNN), such Bidirectional LSTM (BiLSTM) for sentiment analysis. This is one of our first project of deep learning where-in we have took this opportunity to build our basics strongly.\nFor the purpose of analyzing sentiment trends over an extended period, we utilize a substantial dataset consisting of 3 Million Amazon product reviews. This data, sourced from the Stanford Network Analysis Project (SNAP), spans 18 years, providing a rich longitudinal view of consumer sentiments. However, for our modeling we could use only 0.1 Million of the data considering our system constraints.\n\n\n\n\n\nEach review includes a numeric score representing the sentiment polarity. Negative review is represented as class 1 while positive review is represented with class 2. This serves as a foundational metric for sentiment analysis"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#introduction",
    "href": "posts/deep-learning-project-msis/index.html#introduction",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "",
    "text": "This article explores the use of deep learning models, such as feed forward neural networks (FFNN) and recurrent neural networks (RNN), such Bidirectional LSTM (BiLSTM) for sentiment analysis. This is one of our first project of deep learning where-in we have took this opportunity to build our basics strongly.\nFor the purpose of analyzing sentiment trends over an extended period, we utilize a substantial dataset consisting of 3 Million Amazon product reviews. This data, sourced from the Stanford Network Analysis Project (SNAP), spans 18 years, providing a rich longitudinal view of consumer sentiments. However, for our modeling we could use only 0.1 Million of the data considering our system constraints.\n\n\n\n\n\nEach review includes a numeric score representing the sentiment polarity. Negative review is represented as class 1 while positive review is represented with class 2. This serves as a foundational metric for sentiment analysis"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#data-preprocessing",
    "href": "posts/deep-learning-project-msis/index.html#data-preprocessing",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nIn the preprocessing pipeline for sentiment analysis, the following steps were performed using nltk library : Punctuation Removal , Tokenization, Stopword Elimination and Lemmatization. Notably, the stopword list has been specifically curated to retain negations such as “not,” “no,” and other negatory contractions.\n\nstop_words = set(stopwords.words('english')) - { 'not', 'no', 'couldn', \"couldn't\", \"wouldn't\", \"shouldn't\", \"isn't\",\n                                                \"aren't\", \"wasn't\", \"weren't\", \"don't\", \"doesn't\", \"hadn't\", \"hasn't\",\n                                                 \"won't\", \"can't\", \"mightn't\",\"needn't\",\"nor\",\"shouldn\",\"should've\",\"should\",\n                                                 \"weren\",\"wouldn\",\"mustn't\",\"mustn\",\"didn't\",\"didn\",\"doesn\",\"did\",\"does\",\"hadn\",\n                                                 \"hasn\",\"haven't\",\"haven\",\"needn\",\"shan't\"}\n\ndef preprocess(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Tokenize text into words\n    words = word_tokenize(text)\n    # Remove stopwords\n    words = [word for word in words if word not in stop_words]\n    # Lemmatize words\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n    # Join the words back into a single string\n    text = ' '.join(words)\n    return text"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#data-visualization",
    "href": "posts/deep-learning-project-msis/index.html#data-visualization",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Data Visualization",
    "text": "Data Visualization\n \nThis word cloud is characterized by a significant presence of highly positive terms such as “love,” “great,” “best,” “perfect,” and “excellent.” These words indicate strong satisfaction and enjoyment, commonly found in reviews that endorse a product. The words “highly recommend,” “amazing,” and “favorite” suggest that positive reviews often include recommendations and personal favoritism towards the products. The presence of words like “beautiful” and “enjoy” also emphasizes an emotional connection with the product.\nThe negative word cloud features words such as “disappoint,” “waste,” “poor,” “bad,” and “problem.” These strongly negative terms are indicative of dissatisfaction and issues with the products. Terms like “return” and “refund” suggest actions taken by dissatisfied customers. Words like “boring,” “dull,” and “worst” reflect critical opinions about the product’s quality or entertainment value."
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#traditional-machine-learning",
    "href": "posts/deep-learning-project-msis/index.html#traditional-machine-learning",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Traditional Machine Learning",
    "text": "Traditional Machine Learning\nAfter conducting a thorough evaluation, we concluded that the Random Forest model outperformed SVM in multiple metrics, making it the preferred baseline model. This initial selection lays the foundation for further exploration and refinement of sentiment analysis techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nHyper Parameters\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\nRandom Forest with Count Vectorizer\nEstimators : 200, Max Depth:20, Min Samples Split : 2\n82%\n79%\n79%\n\n\nRandom Forest with TF-IDF\nEstimators : 200, Max Depth:20, Min Samples Split : 5\n86%\n83%\n84%"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#neural-networks",
    "href": "posts/deep-learning-project-msis/index.html#neural-networks",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Neural Networks",
    "text": "Neural Networks\nIn our pursuit of advancing practical expertise in deep learning applications, we executed the following steps in a phased manner within the Neural Networks framework:\n\nPreprocessing in neural networks\n\n# Initialize a tokenizer with an out-of-vocabulary token\ntokenizer = Tokenizer(oov_token=\"&lt;UNK&gt;\")\n\n# Fit the tokenizer on the training data to build the vocabulary\ntokenizer.fit_on_texts(X_train)\n\n# Add a special padding token to the word index with index 0\ntokenizer.word_index['&lt;PAD&gt;'] = 0\n\n# Convert the text data into sequences of token indices using the trained tokenizer\nX_sequences_train = tokenizer.texts_to_sequences(X_train)\nX_sequences = tokenizer.texts_to_sequences(X)\n\n# Pad the sequences to ensure uniform length\n# maxlen is set to 100, meaning sequences longer than 100 tokens will be truncated, and shorter sequences will be padded\nX_train = pad_sequences(X_sequences_train, maxlen=100)\nX = pad_sequences(X_sequences, maxlen=100)\n\n\n\nFeed Forward Network\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFeed Forward Network\n10,000 to 3.6 million\n51%\n51%\n51%\nPoor model\n\n\n\nThe reason for its poor performance is likely due to the fact that the input words are represented as numerical numbers. In traditional machine learning, the representation of TF-IDF has shown better results. Therefore, the lesson learned is that we need to convert the input words into a better representation, such as one-hot encoding.\n\n\nOne Hot Encoding\nNow, we have successfully converted the input data into one-hot vectors and we see that the number of parameters to be learned is also huge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFeed Forward Network with one hot encoding\n10,000\nSystem Crashed\nSystem Crashed\nSystem Crashed\nSytem Crashed\n\n\n\nImmediately after this the system got crashed because of the size of vector and its computation even the computer size of 50 GB RAM could. not sustain.\n\n\n\n\n\ntherefore, the lesson learned is that we need to convert the input words into a better representation, like an embedding layer and then we performed on different architectures to explore the better fit for the model.\n\n\nNeural Networks with Embedding Layer\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFeed Forward Network with Embedding Layer\n10,000\n100%\n85%\n85%\nOverfitting\n\n\nGRU with Embedding Layer\n10,000\n100%\n80%\n80%\nOverfitting\n\n\nLSTM with Embedding Layer\n10,000\n100%\n80%\n80%\nOverfitting\n\n\nBi-LSTM with Embedding Layer\n10,000\n100%\n81%\n81%\nOverfitting\n\n\nBi-LSTM with Embedding Layer\n100,000\n100%\n86%\n86%\nOverfitting\n\n\n\nThe reason for overfitting is likely because, given the size of the data, the embedding layer is attempting to learn model parameters within the vocabulary of the input data. However, during validation and testing, there may be many out-of-vocabulary words, leading to underperformance. However, when we increased the dataset to 100K, the accuracy improved. The lesson learned is that if we can input the data with pre-trained embeddings learned on a larger corpus, we can achieve a better and more balanced model.\n\n\nNeural Networks with Pre-Trained Embedding Layer\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nBi-LSTM with Pretrained twitter Embeddings of 50D\n10,000\n90%\n85%\n84%\nDecent Model\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D\n100,000\n94%\n87%\n85%\nDecent Model\n\n\n\nGiven the success of pre-trained embeddings with larger dimensions, we aim to retain this learning and proceed to incorporate a more advanced architecture. The lesson we learned is that larger-dimensional embeddings capture richer attributes of words, which is beneficial. As part of our efforts to enhance the model’s learning, we decided to add an attention layer. This layer allows the model to focus on specific words, further improving its performance.\n\n\nBi-LSTMs with Attention Layer\nWe explored with three variants of attention. Couple of them are custom created and other is with Self Attention layer.\n\n## Custom Made Simple Attention Layer\n\nclass Attention(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        # Initialize the attention mechanism's parameters\n        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")  # Dense layer to compute attention scores\n        self.V = tf.keras.layers.Dense(1)  # Dense layer for the attention mechanism's weight calculation\n\n    def call(self, features):\n        # Compute attention scores\n        score = self.W1(features)\n\n        # Apply softmax activation to obtain attention weights\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n        # Compute context vector as the weighted sum of features\n        context_vector = attention_weights * features\n\n        return context_vector\n\n\n## Custom Made Slightly Complicated Attention Layer\n\nclass Attention_Update(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention_Update, self).__init__()\n        # Initialize parameters for the attention mechanism\n        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")  # Dense layer to compute attention scores\n        self.V = tf.keras.layers.Dense(1)  # Dense layer for attention weight calculation\n\n    def build(self, input_shape):\n        # Initialize trainable weights for attention mechanism\n        self.Wa = self.add_weight(name=\"att_weight_1\", shape=(input_shape[-1], 8),\n                                  initializer=\"normal\")  # Weight matrix for context vector computation\n        self.Wb = self.add_weight(name=\"att_weight_2\", shape=(input_shape[-1], 8),\n                                  initializer=\"normal\")  # Weight matrix for input features\n        self.b = self.add_weight(name=\"att_bias_2\", shape=(input_shape[1], 8),\n                                 initializer=\"zeros\")  # Bias term for context vector computation\n\n        super(Attention_Update, self).build(input_shape)\n\n    def call(self, features):\n        # Compute attention scores\n        score = self.W1(features)\n\n        # Apply softmax activation to obtain attention weights\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n        # Compute context vector as the weighted sum of features\n        context_vector = attention_weights * features\n\n        # Update the hidden state using attention mechanism\n        new_hidden_state = tf.tanh(tf.matmul(context_vector, self.Wa) + tf.matmul(features, self.Wb) + self.b)\n\n        return new_hidden_state\n\n\n# Define input layer with shape (100,)\ninputs = Input(shape=(100,))\n\n# Create an embedding layer with pre-trained weights\n# vocab_size: size of the vocabulary\n# output_dim: dimension of the embedding space\n# input_length: length of input sequences\n# weights: pre-trained embedding matrix\n# trainable: set to False to keep the pre-trained weights fixed during training\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n\n# Apply bidirectional LSTM to capture contextual information\nbilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(embedding_layer)\n\n# Apply self-attention mechanism to focus on important features\ncontext_vector = SeqSelfAttention(attention_activation='sigmoid')(bilstm)\n\n# Apply SimpleRNN to capture sequential patterns\nsimplernn = SimpleRNN(4, activation=\"tanh\")(context_vector)\n\n# Output layer with sigmoid activation for binary classification\noutput = Dense(1, activation=\"sigmoid\")(simplernn)\n\n# Define the model\nmodel_lstm_bi_embed_selfattention = Model(inputs=inputs, outputs=output)\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D - Simple Attention\n100,000\n94%\n90%\n90%\nGood Model\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D - Slightly Complicated Attention\n100,000\n94%\n89%\n90%\nGood Model\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D - Keras Self Attention Layer\n100,000\n94%\n90%\n90%\nGood Model\n\n\n\nAll these models performed equally well; however, our intention is to create an even better model. Therefore, we proceeded to develop a custom model consisting of two bi-LSTMs with a simple attention layer, followed by an RNN, two feedforward networks, and finally, a softmax layer.\n\n\nCustom Made Neural Network Block\n\n\n\n\n\nThis Block we feel it can enhance sentence comprehension by learning relevant words and their dependencies within the sentence. It consists of several components that work together to achieve this goal:\n\nCombining Two Bi-LSTMs: This increases its complexity and enables it to learn sentence context in both forward and backward directions. This helps to capture a more comprehensive understanding of the text.\nAttention Layer: It focuses on relevant words within the sentence, allowing the model to concentrate on key information while disregarding irrelevant details. This mechanism helps to improve the overall accuracy of the model.\nSimple RNN: It helps to learn and capture relevant parameters based on context. This facilitates the understanding of word dependencies within the sentence and enables the model to achieve more accurate sentiment analysis.\n\nNotably, this block operates without a dense layer. Instead, it focuses on leveraging Bi-LSTMs, attention mechanisms, and simple RNNs to achieve effective sentence comprehension and sentiment analysis. However, dense layers can be introduced to the model to introduce more complexity and enable interactions between words and their attributes, thus improving overall comprehension and analysis.\n\n# Define hyperparameters\nlstm_units = 64\nattention_units = 96\nrnn_units = 64\ndense_units = 128\nlearning_rate = 0.001\n\n# Define input layer with shape (100,)\ninputs = Input(shape=(100,))\n\n# Create an embedding layer with pre-trained weights\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n\n# Apply bidirectional LSTM layers with regularization\nbilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(embedding_layer)\nbilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(bilstm)\n\n# Apply attention mechanism\ncontext_vector = Attention(attention_units)(bilstm)\n\n# Apply SimpleRNN layer with regularization\nsimplernn = SimpleRNN(rnn_units, activation=\"tanh\", return_sequences=True, kernel_regularizer=l2(0.0001))(context_vector)\n\n# Flatten the output for feedforward layers\nflatten = Flatten()(simplernn)\n\n# Apply two feedforward layers with regularization\nffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001))(flatten)\nffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001))(ffn)\n\n# Output layer with sigmoid activation for binary classification\noutput = Dense(1, activation=\"sigmoid\")(ffn)\n\n# Define the model\nmodel_lstm_bi_embed_attention_complex_regularized_tuned = Model(inputs=inputs, outputs=output)\n\n# Compile the model\noptimizer = keras.optimizers.Adam(learning_rate)\nmodel_lstm_bi_embed_attention_complex_regularized_tuned.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Print model summary\nmodel_lstm_bi_embed_attention_complex_regularized_tuned.summary()\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFinal Custom block model with hyper tuned parameters\n100K\n91%\n91%\n91%\nBalanced Model"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#conclusion",
    "href": "posts/deep-learning-project-msis/index.html#conclusion",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Conclusion",
    "text": "Conclusion\nNeural networks, when fine-tuned, regularized, and expanded, possess a great capacity to arrive at a better model. While traditional machine learning approach has given us 85% accuracy, the inherent flexibility of neural networks enables us to create of more sophisticated/complicated models. These models we feel are better to capture intricate patterns within the data, ultimately leading to superior performance.\nWe found great fulfillment in undertaking this project, prioritizing our learning journey beyond the confines of grading rubrics. It provided us with invaluable insights and a deeper understanding of the intricacies involved.\nEntire code can be downloaded from this link."
  },
  {
    "objectID": "posts/tensor_math_operations/index.html",
    "href": "posts/tensor_math_operations/index.html",
    "title": "Basic Tensor Math Operations",
    "section": "",
    "text": "This blog, I just a basic commparison of math operations between numpy and tensor variables.\nThis is a good learning for me as almost all commands are similar to that of numpy."
  },
  {
    "objectID": "posts/tensor_math_operations/index.html#loading-libraries",
    "href": "posts/tensor_math_operations/index.html#loading-libraries",
    "title": "Basic Tensor Math Operations",
    "section": "0. Loading Libraries",
    "text": "0. Loading Libraries\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.__version__\n\n'2.15.0'\n\n\n\ntf.config.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "posts/tensor_math_operations/index.html#defining-variables",
    "href": "posts/tensor_math_operations/index.html#defining-variables",
    "title": "Basic Tensor Math Operations",
    "section": "1. Defining variables",
    "text": "1. Defining variables\n\nvar = tf.constant([[30,40,50],[45,65,70]])\n\nvar1 = np.array([[30,40,50],[45,65,70]])\n\nprint(var)\nprint(\" \")\nprint(var1)\n\ntf.Tensor(\n[[30 40 50]\n [45 65 70]], shape=(2, 3), dtype=int32)\n \n[[30 40 50]\n [45 65 70]]"
  },
  {
    "objectID": "posts/tensor_math_operations/index.html#math-operations",
    "href": "posts/tensor_math_operations/index.html#math-operations",
    "title": "Basic Tensor Math Operations",
    "section": "2. Math Operations",
    "text": "2. Math Operations\n\n2.1. Absolute\n\nprint(var * -1 )\nprint(\" \")\nprint(var1 * -1 )\n\ntf.Tensor(\n[[-30 -40 -50]\n [-45 -65 -70]], shape=(2, 3), dtype=int32)\n \n[[-30 -40 -50]\n [-45 -65 -70]]\n\n\n\nprint(tf.math.abs(var * -1))\nprint(\"\")\nprint(np.abs(var1 * -1))\n\ntf.Tensor(\n[[30 40 50]\n [45 65 70]], shape=(2, 3), dtype=int32)\n\n[[30 40 50]\n [45 65 70]]\n\n\nHere we can see both the outputs and commands are very similar to that of numpy\n\n\n2.2. Argmax/Argmin\n\nprint(tf.math.argmax(var))\nprint(\"\")\nprint(np.argmax(var1))\n\ntf.Tensor([1 1 1], shape=(3,), dtype=int64)\n\n5\n\n\nThis is the only difference we can see where tensorflow by default considers as axis=0 while numpy takes all numbers\n\nprint(tf.math.argmax(var,axis=0))\nprint(\"\")\nprint(np.argmax(var1,axis=0))\n\ntf.Tensor([1 1 1], shape=(3,), dtype=int64)\n\n[1 1 1]\n\n\n\nprint(tf.math.argmax(var,axis=1))\nprint(\"\")\nprint(np.argmax(var1,axis=1))\n\ntf.Tensor([2 2], shape=(2,), dtype=int64)\n\n[2 2]\n\n\n\n\n2.3. Maximum or Minumum\n\nprint(tf.math.reduce_max(var))\nprint(\"\")\nprint(np.max(var1))\n\ntf.Tensor(70, shape=(), dtype=int32)\n\n70\n\n\n\nprint(tf.math.reduce_max(var,axis = 0))\nprint(\"\")\nprint(np.max(var1,axis = 0))\n\ntf.Tensor([45 65 70], shape=(3,), dtype=int32)\n\n[45 65 70]\n\n\n\nprint(tf.math.reduce_max(var,axis = 1))\nprint(\"\")\nprint(np.max(var1,axis = 1))\n\ntf.Tensor([50 70], shape=(2,), dtype=int32)\n\n[50 70]\n\n\n\n\n2.4. Sum\n\nprint(tf.math.reduce_sum(var))\nprint(\"\")\nprint(np.sum(var1))\n\ntf.Tensor(300, shape=(), dtype=int32)\n\n300\n\n\n\nprint(tf.math.reduce_sum(var,axis=0))\nprint(\"\")\nprint(np.sum(var1,axis=0))\n\ntf.Tensor([ 75 105 120], shape=(3,), dtype=int32)\n\n[ 75 105 120]\n\n\n\n\n2.5. Power\n\nprint(tf.math.pow(var,2))\nprint(\"\")\nprint(np.power(var1,2))\nprint(\"\")\nprint(var ** 2)\nprint(\"\")\nprint(var1 ** 2)\n\ntf.Tensor(\n[[ 900 1600 2500]\n [2025 4225 4900]], shape=(2, 3), dtype=int32)\n\n[[ 900 1600 2500]\n [2025 4225 4900]]\n\ntf.Tensor(\n[[ 900 1600 2500]\n [2025 4225 4900]], shape=(2, 3), dtype=int32)\n\n[[ 900 1600 2500]\n [2025 4225 4900]]\n\n\n\n\n2.6. Log\n\nvar = tf.cast(var, tf.float32)\n\nprint(tf.math.log(var))\nprint(\"\")\nprint(np.log(var1))\n\ntf.Tensor(\n[[3.4011974 3.6888795 3.912023 ]\n [3.8066626 4.1743875 4.248495 ]], shape=(2, 3), dtype=float32)\n\n[[3.40119738 3.68887945 3.91202301]\n [3.80666249 4.17438727 4.24849524]]\n\n\n\n\n2.7. Exponential\n\nvar = tf.cast(var, tf.float32)\n\nprint(tf.math.exp(var))\nprint(\"\")\nprint(np.exp(var1))\n\ntf.Tensor(\n[[1.0686474e+13 2.3538525e+17 5.1847055e+21]\n [3.4934271e+19 1.6948892e+28 2.5154387e+30]], shape=(2, 3), dtype=float32)\n\n[[1.06864746e+13 2.35385267e+17 5.18470553e+21]\n [3.49342711e+19 1.69488924e+28 2.51543867e+30]]\n\n\n\n\n2.8. Squareroot\n\nprint(tf.math.sqrt(var))\nprint(\"\")\nprint(np.sqrt(var1))\n\ntf.Tensor(\n[[5.4772253 6.3245554 7.071068 ]\n [6.708204  8.062258  8.3666   ]], shape=(2, 3), dtype=float32)\n\n[[5.47722558 6.32455532 7.07106781]\n [6.70820393 8.06225775 8.36660027]]"
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html",
    "href": "posts/shallow_to_multi_linear/index.html",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "",
    "text": "In this blog, I will share my insights gained from analyzing a simulated dataset comprising 10,000 rows. Specifically, I’ll delve into a comparative analysis between simple models and complex models.\nAs a foundational understanding, it is widely acknowledged in the field that for simple handcrafted datasets of smaller to modest sizes simpler models outperform neural network models.\nLet us validate the same through practical experimentation and exploration."
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#loading-libraries",
    "href": "posts/shallow_to_multi_linear/index.html#loading-libraries",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "0. Loading Libraries",
    "text": "0. Loading Libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#generating-simulated-dataset",
    "href": "posts/shallow_to_multi_linear/index.html#generating-simulated-dataset",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "1. Generating Simulated dataset",
    "text": "1. Generating Simulated dataset\n\nx1 = np.random.normal(size=10000)\nx2 = np.random.normal(size=10000) * 2\nx3 = np.power(x1,2)\n\ny = x1 + x2 + x3\n\ny1 = (x1 * np.exp(x2) + np.exp(-x3) * x2 ) * x1 + y\n\nfinaldf = pd.DataFrame({\n    \"X1\" : x1,\n    \"X2\" : x2,\n    \"X3\" : x3,\n    \"Y\"  : y\n})\n\nfinaldf1 = pd.DataFrame({\n    \"X1\" : x1,\n    \"X2\" : x2,\n    \"X3\" : x3,\n    \"Y\"  : y1\n})\n\nprint(finaldf.head())\n\nprint(finaldf1.head())\n\n         X1        X2        X3         Y\n0  1.524132 -0.934670  2.322977  2.912439\n1 -0.813685  3.410324  0.662084  3.258723\n2  0.183997 -3.398534  0.033855 -3.180682\n3  0.161710 -4.845459  0.026150 -4.657599\n4 -0.567619  3.020272  0.322191  2.774844\n         X1        X2        X3          Y\n0  1.524132 -0.934670  2.322977   3.685127\n1 -0.813685  3.410324  0.662084  21.872114\n2  0.183997 -3.398534  0.033855  -3.784055\n3  0.161710 -4.845459  0.026150  -5.420728\n4 -0.567619  3.020272  0.322191   8.136591\n\n\nHere we have created two dataframes;\n\nOne with simple linear relationship\nOther with complex relationship with exponential functions"
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#linear-regression-sci-kit-learn",
    "href": "posts/shallow_to_multi_linear/index.html#linear-regression-sci-kit-learn",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "2. Linear Regression Sci-Kit Learn",
    "text": "2. Linear Regression Sci-Kit Learn\n\n2.1 Linear Relationship Model\n\nX = finaldf[[\"X1\",\"X2\",\"X3\"]]\nY = finaldf[[\"Y\"]]\n\n\nlmmodel = LinearRegression()\n\nlmmodel.fit(X,Y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlmmodel.coef_\n\narray([[1., 1., 1.]])\n\n\n\ny_pred = lmmodel.predict(X)\n\nmean_squared_error(Y, y_pred)\n\n2.3287394852055727e-30\n\n\nBecause of the linear relationship between the dependent and indepdent variables; the mse is very low.\n\n\n2.2 Complex Relationship Linear Model\n\nX1 = finaldf1[[\"X1\",\"X2\",\"X3\"]]\nY1 = finaldf1[[\"Y\"]]\n\n\nlmmodel1 = LinearRegression()\n\nlmmodel1.fit(X1,Y1)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlmmodel1.coef_\n\narray([[2.69112711, 8.39388391, 9.43535975]])\n\n\n\ny_pred = lmmodel.predict(X1)\n\nmean_squared_error(Y1, y_pred)\n\n4385.825525517447\n\n\nBecause of the complex relationship between the dependent and independent variables; the mse is very high compared to earlier model.\n\n\n\nModel Type\nDatset\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\n\nNow let us explore Neural Network Models."
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#tensor-flow-nn-linear-models.",
    "href": "posts/shallow_to_multi_linear/index.html#tensor-flow-nn-linear-models.",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "3. Tensor Flow NN Linear Models.",
    "text": "3. Tensor Flow NN Linear Models.\n\n3.1 Shallow NN - Linear Relationship\n\nmodetf = tf.keras.Sequential([\n    tfl.Dense(1,\n              input_shape = (3,),\n              activation = \"linear\"\n              )\n])\n\n\nmodetf.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 4 (16.00 Byte)\nTrainable params: 4 (16.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodetf.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf.fit(X,Y,epochs=100, verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec5a55b0f40&gt;\n\n\n\nmodetf.layers[0].get_weights()\n\n[array([[1.0000002 ],\n        [0.99999994],\n        [1.        ]], dtype=float32),\n array([1.8922645e-08], dtype=float32)]\n\n\nAs we have seen in the previous blog the weights are not same and accurate compared to the OLS model.\n\nmean_squared_error(Y, modetf.predict(X).flatten())\n\n313/313 [==============================] - 1s 1ms/step\n\n\n8.450062478432384e-14\n\n\nmse is very low.\n\n\n3.2 Shallow NN - Complex Relationship\n\nmodetf.fit(X1,Y1,epochs=100,verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec59cd10760&gt;\n\n\n\nmodetf.layers[0].get_weights()\n\n[array([[ 7.9459767],\n        [10.005791 ],\n        [19.434795 ]], dtype=float32),\n array([-2.91453], dtype=float32)]\n\n\n\nmean_squared_error(Y1, modetf.predict(X1).flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n4279.216985484737\n\n\nmse is high compared to earlier one.\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\n\nNow let us explore Multi-Layer Neural Network Models.\n\n\n3.3 Multi-Layer NN - Linear Relationship\n\nmodetf1 = tf.keras.Sequential([\n    tfl.Dense(100,\n              input_shape = (3,),\n              activation = \"linear\"\n              ),\n    tfl.Dense(1,\n              activation = \"linear\"\n              )\n])\n\n\nmodetf1.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_1 (Dense)             (None, 100)               400       \n                                                                 \n dense_2 (Dense)             (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 501 (1.96 KB)\nTrainable params: 501 (1.96 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nTheory it is still a shallow linear model. As activations used are of identity function. So, we will not see any change in mse\n\nmodetf1.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf1.fit(X,Y,epochs=100, verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec5902b9360&gt;\n\n\n\nmean_squared_error(Y, modetf1.predict(X).flatten())\n\n313/313 [==============================] - 1s 2ms/step\n\n\n3.834542424936751e-14\n\n\nmse almost remained the same for this model as to that of shallow NN model.\n\n\n3.4. Multi-Layer NN - Complex Relationship\n\nmodetf1.fit(X1,Y1,epochs=100, verbose=0)\n#modetf2_pred =\n#mean_squared_error(Y, modetf2_pred.flatten())\nmean_squared_error(Y1, np.nan_to_num(modetf1.predict(X1)).flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n4506.472209255081\n\n\nmse is also same for this model.\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\nMulti-Layer NN\nLinear Relationship\n3.834542424936751e-14\n\n\nMulti-Layer NN\nComplex Relationship\n4506.472209255081\n\n\n\n\n\n3.5 Mulit-Layer NN with Sigmoid - Simpler Relationship\n\nmodetf2 = tf.keras.Sequential([\n    tfl.Dense(4,\n              input_shape = (3,),\n              activation = \"linear\"\n              ),\n    tfl.Dense(3,\n              input_shape = (3,),\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(1,\n              activation = \"linear\"\n              )\n])\n\n\nmodetf2.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf2.fit(X,Y,epochs=100, verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec5901c7b80&gt;\n\n\n\nmodetf2_pred = np.nan_to_num(modetf2.predict(X))\nmean_squared_error(Y, modetf2_pred.flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n0.013774647608438807\n\n\nUpon introducing a model with a complex activation function to a simpler dataset, the mean squared error (MSE) surged significantly. This sharp increase suggests that such a model may not be suitable for this dataset.\n\n\n3.6 Mulit-Layer NN with Sigmoid - Complex Relationship\n\nmodetf2.fit(X1,Y1,epochs=100, verbose=0)\nmean_squared_error(Y1, np.nan_to_num(modetf2.predict(X1)).flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n4385.579539532295\n\n\nSince the dataset has intricate complex nature, the mse for this model remained the same.\n\n\n\n\n\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\nMulti-Layer NN\nLinear Relationship\n3.834542424936751e-14\n\n\nMulti-Layer NN\nComplex Relationship\n4506.472209255081\n\n\nMulti-Layer with Sigmoid NN\nLinear Relationship\n0.013774647608438807.\n\n\nMulti-Layer with Sigmoid NN\nComplex Relationship\n4385.579539532295\n\n\n\n\n\n3.7. Multi-Layer NN with Sigmoid & Relu - Complex Relationship\n\nmodetf3 = tf.keras.Sequential([\n    tfl.Dense(3,\n              input_shape = (3,),\n              activation = \"relu\"\n              ),\n    tfl.Dense(6,\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(12,\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(6,\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(1,\n              activation = \"linear\"\n              )\n])\n\nmodetf3.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf3.fit(X1,Y1,epochs=1000, verbose=0)\n\nmean_squared_error(Y1, modetf3.predict(X1).flatten())\n\n313/313 [==============================] - 1s 2ms/step\n\n\n4433.14019525845\n\n\nSince the dataset has intricate complex nature, the mse for this model remained the same.\n\n\n\n\n\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\nMulti-Layer NN\nLinear Relationship\n3.834542424936751e-14\n\n\nMulti-Layer NN\nComplex Relationship\n4506.472209255081\n\n\nMulti-Layer with Sigmoid NN\nLinear Relationship\n0.013774647608438807.\n\n\nMulti-Layer with Sigmoid NN\nComplex Relationship\n4385.579539532295.\n\n\nMulti-Layer with Sigmoid & Relu NN\nComplex Relationship\n4433.14019525845\n\n\n\nThis blog serves as a straightforward demonstration illustrating that when a dataset lacks intricate complexities and comprises handcrafted features where each row remains relatively consistent within a random normal distribution, simpler models outperform their more complex counterparts.\nIt’s important to note that while simpler models excel in such scenarios, neural networks incur significantly longer computing times. Thus, every level of complexity comes with its associated costs. Therefore, it’s crucial for us to clearly define the ultimate objective of our modeling endeavors before venturing into additional complexities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This website is designed to record my learnings on Deep Learning using Tensor Flow Framework. Since, website will be very handy to review even on a mobile compared to Jupyter Notebook.\nThe coding is done in python majorly using the following packages as of now:\n\nTensor Flow\nnumpy\nPandas\nplotnine { Similar to that of ggplot of R }\ndfply { Similar to that of tidyverse of R }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon Reviews - Sentiment Classification\n\n\n\n\n\n\ntensorflow\n\n\nclassification\n\n\nsequencemodels\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Tensor Gradient Tape\n\n\n\n\n\n\nTensor Basics\n\n\nGradient Tape\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Tensor Math Operations\n\n\n\n\n\n\nTensor Basics\n\n\nTensor Math\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShallow to Multi-Layer Neural Network - Linear Regression\n\n\n\n\n\n\nLinear Regression\n\n\nShallow Neural Network\n\n\nMulti-Layer Neural Network\n\n\n\nSecond Neural network model\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShallow Neural Network - Linear Regression\n\n\n\n\n\n\nLinear Regression\n\n\nShallow Neural Network\n\n\n\nFirst Neural network model\n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics - 2\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics -1\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\nNo matching items"
  }
]
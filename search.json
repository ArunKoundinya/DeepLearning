[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is dedicated for learnings on deep learning using tensor flow.\nMy other sites."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html",
    "href": "posts/tensor-basics-part2/index.html",
    "title": "Tensor Flow Basics - 2",
    "section": "",
    "text": "This article primarily discussed on importance of broadcasting and its easy of implementation using tensors."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "href": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "title": "Tensor Flow Basics - 2",
    "section": "Tensor data types",
    "text": "Tensor data types\n\n\n\n\n\n\n\n\nData Type\nPython Type\nDescription\n\n\n\n\nDT_FLOAT\ntf.float32\n32 bits floating point.\n\n\nDT_DOUBLE\ntf.float64\n64 bits floating point.\n\n\nDT_INT8\ntf.int8\n8 bits signed integer.\n\n\nDT_INT16\ntf.int16\n16 bits signed integer.\n\n\nDT_INT32\ntf.int32\n32 bits signed integer.\n\n\nDT_INT64\ntf.int64\n64 bits signed integer.\n\n\nDT_UINT8\ntf.uint8\n8 bits unsigned integer.\n\n\nDT_STRING\ntf.string\nVariable length byte arrays. Each element of a tensor is a byte array.\n\n\nDT_BOOL\ntf.bool\nBoolean.\n\n\n\n\nimport tensorflow as tf\nprint(tf.__version__)\n\n2.15.0\n\n\n\nx_new = tf.constant(1,shape=(2,3),dtype=tf.int8)\n\n\ny_new = tf.constant(2,shape=(2,3),dtype=tf.int8)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "href": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "title": "Tensor Flow Basics - 2",
    "section": "Re-cap Operations",
    "text": "Re-cap Operations\n\nElement Wise Operations\nMatrix Multiplicaitons\n\n\nprint(\"Addition:\\n\", tf.add(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nprint(\"Subtraction:\\n\", tf.subtract(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nAddition:\n tf.Tensor(\n[[3 3 3]\n [3 3 3]], shape=(2, 3), dtype=int8)\n \n \nSubtraction:\n tf.Tensor(\n[[-1 -1 -1]\n [-1 -1 -1]], shape=(2, 3), dtype=int8)\n \n \n\n\n\nprint(\"Multiplication:\\n\", tf.multiply(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(2,3))))\n\nMultiplication:\n tf.Tensor(\n[[-0.5718114  -0.18780218 -2.0768495 ]\n [ 0.29304612  0.08317164 -1.3320862 ]], shape=(2, 3), dtype=float32)\n\n\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,2))))\nprint(\" \")\nprint(\" \")\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,1))))\n\nMatrix Multiplication:\n tf.Tensor(\n[[-1.6848389  -0.44041786]\n [-0.9548799  -0.22109865]], shape=(2, 2), dtype=float32)\n \n \nMatrix Multiplication:\n tf.Tensor(\n[[ 0.37789747]\n [-0.12570143]], shape=(2, 1), dtype=float32)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#broadcasting",
    "href": "posts/tensor-basics-part2/index.html#broadcasting",
    "title": "Tensor Flow Basics - 2",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is useful when the dimensions of matrices are different from each other. This is highly useful to add and subtract the matrices of different lengths.\nHere’s a brief summary of how broadcasting works for addition and subtraction:\nAddition: If the shapes of the two tensors are different, TensorFlow compares their shapes element-wise, starting from the trailing dimensions. If the dimensions are compatible or one of the dimensions is 1, broadcasting can occur. For example, you can add a scalar to a matrix, and the scalar value will be added to every element of the matrix.\nSubtraction: Similar to addition, broadcasting allows you to subtract a scalar or a vector from a matrix, or subtract matrices of different shapes.\n\nx1 = tf.random.normal(shape=(2,3))\n\nx2 = tf.random.normal(shape=(2,1))\n\n\nprint(\"x1:\\n\",x1)\nprint(\" \")\nprint(\" \")\nprint(\"x2:\\n\",x2)\n\nx1:\n tf.Tensor(\n[[ 0.39297998 -0.12811422 -0.60474324]\n [ 0.30773026  1.4076523  -0.57274765]], shape=(2, 3), dtype=float32)\n \n \nx2:\n tf.Tensor(\n[[0.10113019]\n [1.0277312 ]], shape=(2, 1), dtype=float32)\n\n\n\ntf.broadcast_to(x2, [2, 3])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.10113019, 0.10113019, 0.10113019],\n       [1.0277312 , 1.0277312 , 1.0277312 ]], dtype=float32)&gt;\n\n\n\ntf.add(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.49411017, -0.02698404, -0.50361305],\n       [ 1.3354614 ,  2.4353833 ,  0.45498353]], dtype=float32)&gt;\n\n\n\ntf.subtract(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.2918498 , -0.22924441, -0.7058734 ],\n       [-0.7200009 ,  0.37992108, -1.6004789 ]], dtype=float32)&gt;\n\n\nIn the above examples we have seen that broadcasting of same tensor dimension i.e.; of two dimensional.\nBelow, we will see the beauty of broadcasting of different dimensions.\n\nx3 = tf.random.normal(shape=(2, 2, 2)) #3d tensor\nx4 = tf.random.normal(shape=(2, 1)) #2d tensor\n\n\ntf.add(x3,x4)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-1.0228443 ,  0.34588122],\n        [ 2.188323  , -0.7523911 ]],\n\n       [[-0.56017506, -1.5636952 ],\n        [ 1.572252  ,  1.6189265 ]]], dtype=float32)&gt;\n\n\n\ntf.broadcast_to(x4, [2,2,2])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]],\n\n       [[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]]], dtype=float32)&gt;"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html",
    "href": "posts/tensor-basics-part1/index.html",
    "title": "Tensor Flow Basics -1",
    "section": "",
    "text": "These are initial learnings on tensors hope it is helpful for everyone\nimport tensorflow as tf"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "href": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "title": "Tensor Flow Basics -1",
    "section": "What is tensor Flow?",
    "text": "What is tensor Flow?\nTensor flow is deep learning framework developed by Google in 2011 and made it publicly available in the year 2015.\nIt is flexible, scalable solutio that enable us to build models with the existing frameworks.\nIt is like sci-kit learn library but more advanced and flexible as we can custom build our own neural network.\n\nprint(tf.__version__)\n\n2.15.0"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "href": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "title": "Tensor Flow Basics -1",
    "section": "Basics Unit in TensorFlow Framework - Tensor",
    "text": "Basics Unit in TensorFlow Framework - Tensor\nTensors are multi-dimensional arrays designed for numerical data representation; although they share some similarities with NumPy arrays, they possess certain unique features that give them an advantage in deep learning tasks. One of these key advantages is their ability to utilize hardware acceleration from GPUs and TPUs to significantly speed up computational operations, which is especially useful when working with input data such as images, text, and videos.\nIn simple words “ML” needs numbers; In case of large dimensions we need matrices. These matrices are called tensors. As these are specially designed to use hardware capabilities to accelerate learnings."
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#creating-tensors",
    "href": "posts/tensor-basics-part1/index.html#creating-tensors",
    "title": "Tensor Flow Basics -1",
    "section": "Creating Tensors",
    "text": "Creating Tensors\n\ntf.constant(1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 1],\n       [1, 1]], dtype=int32)&gt;\n\n\nHere we have created a basic tensor with constant number 1 with a shape 2,2 i.e.; two rows and two columns.\nAnd its datatype is integer.\nIt is a numpy array.\n\n## Manually providing the shape\ny = tf.constant([[1, 2, 3], [4, 5, 6]])\nprint(y)\n\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nInstead of giving the shape here we have manually given the values\n\ntf.rank(y)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\nHere we are checking what is the rank of the tensor.\n\n\nprint(\"The rank of scalar is \" , tf.rank(tf.constant(1)))\nprint(\"The rank of vector is \" , tf.rank(tf.constant(1,shape=(5))))\nprint(\"The rank of matrix is \" , tf.rank(tf.constant(1,shape=(5,4))))\nprint(\"The rank of rank3tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3))))\n\nThe rank of scalar is  tf.Tensor(0, shape=(), dtype=int32)\nThe rank of vector is  tf.Tensor(1, shape=(), dtype=int32)\nThe rank of matrix is  tf.Tensor(2, shape=(), dtype=int32)\nThe rank of rank3tensor is  tf.Tensor(3, shape=(), dtype=int32)\n\n\nCan there me more than 3 dimensional; of course, but we cannot represent them pictographically.\n\nprint(\"The rank of rank5tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3))))\nprint(\"The rank of rank9tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3,1,1,3,3))))\n\nThe rank of rank5tensor is  tf.Tensor(5, shape=(), dtype=int32)\nThe rank of rank9tensor is  tf.Tensor(9, shape=(), dtype=int32)"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "href": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "title": "Tensor Flow Basics -1",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\ntf.constant(1.1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1.1, 1.1],\n       [1.1, 1.1]], dtype=float32)&gt;\n\n\n\nTypeCasting\nThe moment we kept 1.1 the data-type has changed to float; Lets check how to typecast integer to float and vice versa\n\nx_int = tf.constant(1, shape=(2,2))\nprint(x_int)\nx_float = tf.cast(x_int, dtype = tf.float32)\nprint(x_float)\nx_float_int = tf.cast(x_float, tf.int32)\nprint(x_float_int)\n\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\n\n\n\n\nIndexing\nSimilar to numpy array we can do indexing for the tensors\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\ny[0]\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\ny[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n\nExpanding a matrix\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(tf.expand_dims(y,axis=0)) ## Expanding at the beginning of the tensor\nprint(tf.expand_dims(y,axis=1)) ## Expanding at the Middle of the tensor { for this example}\nprint(tf.expand_dims(y,axis=-1)) ## Expanding at the End of the tensor\n\ntf.Tensor(\n[[[1 2 3]\n  [4 5 6]]], shape=(1, 2, 3), dtype=int32)\ntf.Tensor(\n[[[1 2 3]]\n\n [[4 5 6]]], shape=(2, 1, 3), dtype=int32)\ntf.Tensor(\n[[[1]\n  [2]\n  [3]]\n\n [[4]\n  [5]\n  [6]]], shape=(2, 3, 1), dtype=int32)\n\n\n\n\nTensor Aggregation\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(\"Smallest of the number is \",tf.reduce_min(y).numpy())\nprint(\"Largest of the number is \",tf.reduce_max(y).numpy())\n\nSmallest of the number is  1\nLargest of the number is  6\n\n\n\nprint(\"Sum of the numbers are \",tf.reduce_sum(y).numpy())\nprint(\"Average of the numbers are \",tf.reduce_mean(y).numpy())\n\nSum of the numbers are  21\nAverage of the numbers are  3\n\n\n\n\nMatrix with all ones,zeroes and identity\n\nz =  tf.ones([2,3])\nprint(z)\nprint(\" \")\n\nx  =  tf.constant(1,shape=(2,3),dtype=tf.float32)\nprint(x)\nprint(\" \")\n\nz =  tf.zeros([2,3])\nprint(z)\nprint(\" \")\n\nz = tf.eye(3)\nprint(z)\n\ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[0. 0. 0.]\n [0. 0. 0.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float32)\n\n\n\n\nReshaping and Transposing Tensors\n\nx_initial = tf.constant(1, shape=(4,3))\nprint(x_initial)\n\ntf.Tensor(\n[[1 1 1]\n [1 1 1]\n [1 1 1]\n [1 1 1]], shape=(4, 3), dtype=int32)\n\n\n\ntf.reshape(x_initial,shape=(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[1, 1, 1],\n        [1, 1, 1]],\n\n       [[1, 1, 1],\n        [1, 1, 1]]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(2,6))\n\n&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(12,1))\n\n&lt;tf.Tensor: shape=(12, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1]], dtype=int32)&gt;\n\n\nHere we can reshape to any other shape; However, the multiplication of shapes should remain the same.\n\ntf.reshape(x_initial,shape=-1) #Flatten the array\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)&gt;\n\n\n\ntf.transpose(x_initial)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[1, 1, 1, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 1]], dtype=int32)&gt;\n\n\nThe initial shape of (4,3) got changed to (3,4)\n\n\nDistributions\n\nx1  = tf.random.normal((3,3))\nprint(x1)\nprint(\" \")\n\nx1  = tf.random.normal((3,3),mean = 0, stddev =1 )\nprint(x1)\nprint(\" \")\n\nx2 =  tf.random.uniform((3,3))\nprint(x2)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.591363    0.12791212  0.38762185]\n [ 0.26025018  1.7209182  -0.7802837 ]\n [ 0.89150804 -0.9648455   0.64507854]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-1.0034778   0.05435322 -1.3141975 ]\n [-0.17819698 -1.9136705  -0.9396771 ]\n [ 2.2143493   0.33600262  0.8174351 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[0.04812372 0.79855204 0.8067709 ]\n [0.67069924 0.06617999 0.14941025]\n [0.31500185 0.17441607 0.7476181 ]], shape=(3, 3), dtype=float32)\n \n\n\n\n\nMathematical Operations\n\nAddition\nSubtraction\nMultiplication\nDivision\n\n\nx4 = tf.random.normal((3,3))\nprint(x4)\nprint(\" \")\ny4 = tf.random.normal((3,3))\nprint(y4)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.5908952   3.5452905  -0.34438497]\n [-0.5237503   1.2899861  -0.50684774]\n [ 1.2187229   0.50014    -0.6212071 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-0.7346279   2.9705956  -0.45318994]\n [-0.947753    0.6556651   3.018978  ]\n [-1.5799906  -1.2278746   0.57451475]], shape=(3, 3), dtype=float32)\n \n\n\n\nprint(x4+y4)\nprint(\" \")\ntf.add(x4,y4)\n\ntf.Tensor(\n[[-0.14373273  6.5158863  -0.7975749 ]\n [-1.4715033   1.9456513   2.5121303 ]\n [-0.3612677  -0.7277346  -0.04669237]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.14373273,  6.5158863 , -0.7975749 ],\n       [-1.4715033 ,  1.9456513 ,  2.5121303 ],\n       [-0.3612677 , -0.7277346 , -0.04669237]], dtype=float32)&gt;\n\n\n\nprint(x4-y4)\nprint(\" \")\ntf.subtract(x4,y4)\n\ntf.Tensor(\n[[ 1.3255231   0.5746949   0.10880497]\n [ 0.4240027   0.63432103 -3.525826  ]\n [ 2.7987137   1.7280147  -1.1957219 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 1.3255231 ,  0.5746949 ,  0.10880497],\n       [ 0.4240027 ,  0.63432103, -3.525826  ],\n       [ 2.7987137 ,  1.7280147 , -1.1957219 ]], dtype=float32)&gt;\n\n\n\nprint(x4*y4)\nprint(\" \")\ntf.multiply(x4,y4)\n\ntf.Tensor(\n[[-0.43408808 10.531624    0.1560718 ]\n [ 0.49638593  0.8457989  -1.5301622 ]\n [-1.9255708  -0.6141092  -0.35689265]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.43408808, 10.531624  ,  0.1560718 ],\n       [ 0.49638593,  0.8457989 , -1.5301622 ],\n       [-1.9255708 , -0.6141092 , -0.35689265]], dtype=float32)&gt;\n\n\n\nprint(x4/y4)\nprint(\" \")\ntf.divide(x4,y4)\n\ntf.Tensor(\n[[-0.8043462   1.1934612   0.7599131 ]\n [ 0.5526232   1.9674467  -0.16788718]\n [-0.77134824 -0.40732172 -1.0812727 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.8043462 ,  1.1934612 ,  0.7599131 ],\n       [ 0.5526232 ,  1.9674467 , -0.16788718],\n       [-0.77134824, -0.40732172, -1.0812727 ]], dtype=float32)&gt;\n\n\n\n\nMatrix Multiplications\n\nx4_new = tf.random.normal((3,2))\ny4_new = tf.random.normal((2,3))\n\nprint(tf.matmul(x4_new,y4_new))\n\ntf.Tensor(\n[[ 0.5675167   0.29923582 -2.018334  ]\n [-0.5145724   1.9392778  -1.5560541 ]\n [ 1.0566927  -2.3777525   0.73752195]], shape=(3, 3), dtype=float32)\n\n\n\nx4_new @ y4_new\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.5675167 ,  0.29923582, -2.018334  ],\n       [-0.5145724 ,  1.9392778 , -1.5560541 ],\n       [ 1.0566927 , -2.3777525 ,  0.73752195]], dtype=float32)&gt;\n\n\nTwo ways of matrix of multiplication in tensors; There is also a way using dot product; which we will discuss later :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This website is designed to record my learnings on Deep Learning using Tensor Flow Framework. Since, website will be very handy to review even on a mobile compared to Jupyter Notebook.\nThe coding is done in python majorly using the following packages as of now:\n\nTensor Flow\nnumpy\nPandas\nplotnine { Similar to that of ggplot of R }\ndfply { Similar to that of tidyverse of R }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics - 2\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics -1\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\nNo matching items"
  }
]
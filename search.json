[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is dedicated for learnings on deep learning using tensor flow.\nMy other sites."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html",
    "href": "posts/tensor-basics-part2/index.html",
    "title": "Tensor Flow Basics - 2",
    "section": "",
    "text": "This article primarily discussed on importance of broadcasting and its easy of implementation using tensors."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "href": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "title": "Tensor Flow Basics - 2",
    "section": "Tensor data types",
    "text": "Tensor data types\n\n\n\n\n\n\n\n\nData Type\nPython Type\nDescription\n\n\n\n\nDT_FLOAT\ntf.float32\n32 bits floating point.\n\n\nDT_DOUBLE\ntf.float64\n64 bits floating point.\n\n\nDT_INT8\ntf.int8\n8 bits signed integer.\n\n\nDT_INT16\ntf.int16\n16 bits signed integer.\n\n\nDT_INT32\ntf.int32\n32 bits signed integer.\n\n\nDT_INT64\ntf.int64\n64 bits signed integer.\n\n\nDT_UINT8\ntf.uint8\n8 bits unsigned integer.\n\n\nDT_STRING\ntf.string\nVariable length byte arrays. Each element of a tensor is a byte array.\n\n\nDT_BOOL\ntf.bool\nBoolean.\n\n\n\n\nimport tensorflow as tf\nprint(tf.__version__)\n\n2.15.0\n\n\n\nx_new = tf.constant(1,shape=(2,3),dtype=tf.int8)\n\n\ny_new = tf.constant(2,shape=(2,3),dtype=tf.int8)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "href": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "title": "Tensor Flow Basics - 2",
    "section": "Re-cap Operations",
    "text": "Re-cap Operations\n\nElement Wise Operations\nMatrix Multiplicaitons\n\n\nprint(\"Addition:\\n\", tf.add(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nprint(\"Subtraction:\\n\", tf.subtract(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nAddition:\n tf.Tensor(\n[[3 3 3]\n [3 3 3]], shape=(2, 3), dtype=int8)\n \n \nSubtraction:\n tf.Tensor(\n[[-1 -1 -1]\n [-1 -1 -1]], shape=(2, 3), dtype=int8)\n \n \n\n\n\nprint(\"Multiplication:\\n\", tf.multiply(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(2,3))))\n\nMultiplication:\n tf.Tensor(\n[[-0.5718114  -0.18780218 -2.0768495 ]\n [ 0.29304612  0.08317164 -1.3320862 ]], shape=(2, 3), dtype=float32)\n\n\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,2))))\nprint(\" \")\nprint(\" \")\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,1))))\n\nMatrix Multiplication:\n tf.Tensor(\n[[-1.6848389  -0.44041786]\n [-0.9548799  -0.22109865]], shape=(2, 2), dtype=float32)\n \n \nMatrix Multiplication:\n tf.Tensor(\n[[ 0.37789747]\n [-0.12570143]], shape=(2, 1), dtype=float32)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#broadcasting",
    "href": "posts/tensor-basics-part2/index.html#broadcasting",
    "title": "Tensor Flow Basics - 2",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is useful when the dimensions of matrices are different from each other. This is highly useful to add and subtract the matrices of different lengths.\nHere’s a brief summary of how broadcasting works for addition and subtraction:\nAddition: If the shapes of the two tensors are different, TensorFlow compares their shapes element-wise, starting from the trailing dimensions. If the dimensions are compatible or one of the dimensions is 1, broadcasting can occur. For example, you can add a scalar to a matrix, and the scalar value will be added to every element of the matrix.\nSubtraction: Similar to addition, broadcasting allows you to subtract a scalar or a vector from a matrix, or subtract matrices of different shapes.\n\nx1 = tf.random.normal(shape=(2,3))\n\nx2 = tf.random.normal(shape=(2,1))\n\n\nprint(\"x1:\\n\",x1)\nprint(\" \")\nprint(\" \")\nprint(\"x2:\\n\",x2)\n\nx1:\n tf.Tensor(\n[[ 0.39297998 -0.12811422 -0.60474324]\n [ 0.30773026  1.4076523  -0.57274765]], shape=(2, 3), dtype=float32)\n \n \nx2:\n tf.Tensor(\n[[0.10113019]\n [1.0277312 ]], shape=(2, 1), dtype=float32)\n\n\n\ntf.broadcast_to(x2, [2, 3])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.10113019, 0.10113019, 0.10113019],\n       [1.0277312 , 1.0277312 , 1.0277312 ]], dtype=float32)&gt;\n\n\n\ntf.add(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.49411017, -0.02698404, -0.50361305],\n       [ 1.3354614 ,  2.4353833 ,  0.45498353]], dtype=float32)&gt;\n\n\n\ntf.subtract(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.2918498 , -0.22924441, -0.7058734 ],\n       [-0.7200009 ,  0.37992108, -1.6004789 ]], dtype=float32)&gt;\n\n\nIn the above examples we have seen that broadcasting of same tensor dimension i.e.; of two dimensional.\nBelow, we will see the beauty of broadcasting of different dimensions.\n\nx3 = tf.random.normal(shape=(2, 2, 2)) #3d tensor\nx4 = tf.random.normal(shape=(2, 1)) #2d tensor\n\n\ntf.add(x3,x4)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-1.0228443 ,  0.34588122],\n        [ 2.188323  , -0.7523911 ]],\n\n       [[-0.56017506, -1.5636952 ],\n        [ 1.572252  ,  1.6189265 ]]], dtype=float32)&gt;\n\n\n\ntf.broadcast_to(x4, [2,2,2])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]],\n\n       [[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]]], dtype=float32)&gt;"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html",
    "href": "posts/tensor-basics-part1/index.html",
    "title": "Tensor Flow Basics -1",
    "section": "",
    "text": "These are initial learnings on tensors hope it is helpful for everyone\nimport tensorflow as tf"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "href": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "title": "Tensor Flow Basics -1",
    "section": "What is tensor Flow?",
    "text": "What is tensor Flow?\nTensor flow is deep learning framework developed by Google in 2011 and made it publicly available in the year 2015.\nIt is flexible, scalable solutio that enable us to build models with the existing frameworks.\nIt is like sci-kit learn library but more advanced and flexible as we can custom build our own neural network.\n\nprint(tf.__version__)\n\n2.15.0"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "href": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "title": "Tensor Flow Basics -1",
    "section": "Basics Unit in TensorFlow Framework - Tensor",
    "text": "Basics Unit in TensorFlow Framework - Tensor\nTensors are multi-dimensional arrays designed for numerical data representation; although they share some similarities with NumPy arrays, they possess certain unique features that give them an advantage in deep learning tasks. One of these key advantages is their ability to utilize hardware acceleration from GPUs and TPUs to significantly speed up computational operations, which is especially useful when working with input data such as images, text, and videos.\nIn simple words “ML” needs numbers; In case of large dimensions we need matrices. These matrices are called tensors. As these are specially designed to use hardware capabilities to accelerate learnings."
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#creating-tensors",
    "href": "posts/tensor-basics-part1/index.html#creating-tensors",
    "title": "Tensor Flow Basics -1",
    "section": "Creating Tensors",
    "text": "Creating Tensors\n\ntf.constant(1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 1],\n       [1, 1]], dtype=int32)&gt;\n\n\nHere we have created a basic tensor with constant number 1 with a shape 2,2 i.e.; two rows and two columns.\nAnd its datatype is integer.\nIt is a numpy array.\n\n## Manually providing the shape\ny = tf.constant([[1, 2, 3], [4, 5, 6]])\nprint(y)\n\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nInstead of giving the shape here we have manually given the values\n\ntf.rank(y)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\nHere we are checking what is the rank of the tensor.\n\n\nprint(\"The rank of scalar is \" , tf.rank(tf.constant(1)))\nprint(\"The rank of vector is \" , tf.rank(tf.constant(1,shape=(5))))\nprint(\"The rank of matrix is \" , tf.rank(tf.constant(1,shape=(5,4))))\nprint(\"The rank of rank3tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3))))\n\nThe rank of scalar is  tf.Tensor(0, shape=(), dtype=int32)\nThe rank of vector is  tf.Tensor(1, shape=(), dtype=int32)\nThe rank of matrix is  tf.Tensor(2, shape=(), dtype=int32)\nThe rank of rank3tensor is  tf.Tensor(3, shape=(), dtype=int32)\n\n\nCan there me more than 3 dimensional; of course, but we cannot represent them pictographically.\n\nprint(\"The rank of rank5tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3))))\nprint(\"The rank of rank9tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3,1,1,3,3))))\n\nThe rank of rank5tensor is  tf.Tensor(5, shape=(), dtype=int32)\nThe rank of rank9tensor is  tf.Tensor(9, shape=(), dtype=int32)"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "href": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "title": "Tensor Flow Basics -1",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\ntf.constant(1.1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1.1, 1.1],\n       [1.1, 1.1]], dtype=float32)&gt;\n\n\n\nTypeCasting\nThe moment we kept 1.1 the data-type has changed to float; Lets check how to typecast integer to float and vice versa\n\nx_int = tf.constant(1, shape=(2,2))\nprint(x_int)\nx_float = tf.cast(x_int, dtype = tf.float32)\nprint(x_float)\nx_float_int = tf.cast(x_float, tf.int32)\nprint(x_float_int)\n\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\n\n\n\n\nIndexing\nSimilar to numpy array we can do indexing for the tensors\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\ny[0]\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\ny[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n\nExpanding a matrix\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(tf.expand_dims(y,axis=0)) ## Expanding at the beginning of the tensor\nprint(tf.expand_dims(y,axis=1)) ## Expanding at the Middle of the tensor { for this example}\nprint(tf.expand_dims(y,axis=-1)) ## Expanding at the End of the tensor\n\ntf.Tensor(\n[[[1 2 3]\n  [4 5 6]]], shape=(1, 2, 3), dtype=int32)\ntf.Tensor(\n[[[1 2 3]]\n\n [[4 5 6]]], shape=(2, 1, 3), dtype=int32)\ntf.Tensor(\n[[[1]\n  [2]\n  [3]]\n\n [[4]\n  [5]\n  [6]]], shape=(2, 3, 1), dtype=int32)\n\n\n\n\nTensor Aggregation\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(\"Smallest of the number is \",tf.reduce_min(y).numpy())\nprint(\"Largest of the number is \",tf.reduce_max(y).numpy())\n\nSmallest of the number is  1\nLargest of the number is  6\n\n\n\nprint(\"Sum of the numbers are \",tf.reduce_sum(y).numpy())\nprint(\"Average of the numbers are \",tf.reduce_mean(y).numpy())\n\nSum of the numbers are  21\nAverage of the numbers are  3\n\n\n\n\nMatrix with all ones,zeroes and identity\n\nz =  tf.ones([2,3])\nprint(z)\nprint(\" \")\n\nx  =  tf.constant(1,shape=(2,3),dtype=tf.float32)\nprint(x)\nprint(\" \")\n\nz =  tf.zeros([2,3])\nprint(z)\nprint(\" \")\n\nz = tf.eye(3)\nprint(z)\n\ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[0. 0. 0.]\n [0. 0. 0.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float32)\n\n\n\n\nReshaping and Transposing Tensors\n\nx_initial = tf.constant(1, shape=(4,3))\nprint(x_initial)\n\ntf.Tensor(\n[[1 1 1]\n [1 1 1]\n [1 1 1]\n [1 1 1]], shape=(4, 3), dtype=int32)\n\n\n\ntf.reshape(x_initial,shape=(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[1, 1, 1],\n        [1, 1, 1]],\n\n       [[1, 1, 1],\n        [1, 1, 1]]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(2,6))\n\n&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(12,1))\n\n&lt;tf.Tensor: shape=(12, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1]], dtype=int32)&gt;\n\n\nHere we can reshape to any other shape; However, the multiplication of shapes should remain the same.\n\ntf.reshape(x_initial,shape=-1) #Flatten the array\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)&gt;\n\n\n\ntf.transpose(x_initial)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[1, 1, 1, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 1]], dtype=int32)&gt;\n\n\nThe initial shape of (4,3) got changed to (3,4)\n\n\nDistributions\n\nx1  = tf.random.normal((3,3))\nprint(x1)\nprint(\" \")\n\nx1  = tf.random.normal((3,3),mean = 0, stddev =1 )\nprint(x1)\nprint(\" \")\n\nx2 =  tf.random.uniform((3,3))\nprint(x2)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.591363    0.12791212  0.38762185]\n [ 0.26025018  1.7209182  -0.7802837 ]\n [ 0.89150804 -0.9648455   0.64507854]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-1.0034778   0.05435322 -1.3141975 ]\n [-0.17819698 -1.9136705  -0.9396771 ]\n [ 2.2143493   0.33600262  0.8174351 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[0.04812372 0.79855204 0.8067709 ]\n [0.67069924 0.06617999 0.14941025]\n [0.31500185 0.17441607 0.7476181 ]], shape=(3, 3), dtype=float32)\n \n\n\n\n\nMathematical Operations\n\nAddition\nSubtraction\nMultiplication\nDivision\n\n\nx4 = tf.random.normal((3,3))\nprint(x4)\nprint(\" \")\ny4 = tf.random.normal((3,3))\nprint(y4)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.5908952   3.5452905  -0.34438497]\n [-0.5237503   1.2899861  -0.50684774]\n [ 1.2187229   0.50014    -0.6212071 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-0.7346279   2.9705956  -0.45318994]\n [-0.947753    0.6556651   3.018978  ]\n [-1.5799906  -1.2278746   0.57451475]], shape=(3, 3), dtype=float32)\n \n\n\n\nprint(x4+y4)\nprint(\" \")\ntf.add(x4,y4)\n\ntf.Tensor(\n[[-0.14373273  6.5158863  -0.7975749 ]\n [-1.4715033   1.9456513   2.5121303 ]\n [-0.3612677  -0.7277346  -0.04669237]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.14373273,  6.5158863 , -0.7975749 ],\n       [-1.4715033 ,  1.9456513 ,  2.5121303 ],\n       [-0.3612677 , -0.7277346 , -0.04669237]], dtype=float32)&gt;\n\n\n\nprint(x4-y4)\nprint(\" \")\ntf.subtract(x4,y4)\n\ntf.Tensor(\n[[ 1.3255231   0.5746949   0.10880497]\n [ 0.4240027   0.63432103 -3.525826  ]\n [ 2.7987137   1.7280147  -1.1957219 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 1.3255231 ,  0.5746949 ,  0.10880497],\n       [ 0.4240027 ,  0.63432103, -3.525826  ],\n       [ 2.7987137 ,  1.7280147 , -1.1957219 ]], dtype=float32)&gt;\n\n\n\nprint(x4*y4)\nprint(\" \")\ntf.multiply(x4,y4)\n\ntf.Tensor(\n[[-0.43408808 10.531624    0.1560718 ]\n [ 0.49638593  0.8457989  -1.5301622 ]\n [-1.9255708  -0.6141092  -0.35689265]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.43408808, 10.531624  ,  0.1560718 ],\n       [ 0.49638593,  0.8457989 , -1.5301622 ],\n       [-1.9255708 , -0.6141092 , -0.35689265]], dtype=float32)&gt;\n\n\n\nprint(x4/y4)\nprint(\" \")\ntf.divide(x4,y4)\n\ntf.Tensor(\n[[-0.8043462   1.1934612   0.7599131 ]\n [ 0.5526232   1.9674467  -0.16788718]\n [-0.77134824 -0.40732172 -1.0812727 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.8043462 ,  1.1934612 ,  0.7599131 ],\n       [ 0.5526232 ,  1.9674467 , -0.16788718],\n       [-0.77134824, -0.40732172, -1.0812727 ]], dtype=float32)&gt;\n\n\n\n\nMatrix Multiplications\n\nx4_new = tf.random.normal((3,2))\ny4_new = tf.random.normal((2,3))\n\nprint(tf.matmul(x4_new,y4_new))\n\ntf.Tensor(\n[[ 0.5675167   0.29923582 -2.018334  ]\n [-0.5145724   1.9392778  -1.5560541 ]\n [ 1.0566927  -2.3777525   0.73752195]], shape=(3, 3), dtype=float32)\n\n\n\nx4_new @ y4_new\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.5675167 ,  0.29923582, -2.018334  ],\n       [-0.5145724 ,  1.9392778 , -1.5560541 ],\n       [ 1.0566927 , -2.3777525 ,  0.73752195]], dtype=float32)&gt;\n\n\nTwo ways of matrix of multiplication in tensors; There is also a way using dot product; which we will discuss later :)"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html",
    "href": "posts/shallow-linear-model/index.html",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "",
    "text": "This article primarly discusses on implementation of simple linear regression in both sklearn and tensorflow.\nimport tensorflow as tf\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nWe will consider a toy dataset of diabetes, described below, to perform both linear regression using OLS and shallow neural networks, and learn from both approaches.\ndata = load_diabetes()\nprint(data['DESCR'])\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nprint(\"Number of Independent features: \", data['data'].shape[1])\nprint(\"Number of Training Instances: \", data['target'].shape[0])\n\nNumber of Independent features:  10\nNumber of Training Instances:  442\nLet us divide the entire dataset into train and test set\nX= data['data']\ny = data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#linear-regression-sci-kit-learn",
    "href": "posts/shallow-linear-model/index.html#linear-regression-sci-kit-learn",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Linear Regression Sci-kit Learn",
    "text": "Linear Regression Sci-kit Learn\n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\nMean Squared Error: 2900.193628493482\n\n\n\nprint(\"Model Coefficients: \\n\", model.coef_)\n\nModel Coefficients: \n [  37.90402135 -241.96436231  542.42875852  347.70384391 -931.48884588\n  518.06227698  163.41998299  275.31790158  736.1988589    48.67065743]\n\n\n\nprint(\"Model Intercept: \\n\", model.intercept_)\n\nModel Intercept: \n 151.34560453985995"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#shallow-neural-network---linear-regression",
    "href": "posts/shallow-linear-model/index.html#shallow-neural-network---linear-regression",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Shallow Neural Network - Linear Regression",
    "text": "Shallow Neural Network - Linear Regression\nWe can view the shallow neural network for linear regression in below way\n\n\ny_reshaped = y.reshape(-1, 1)\n\n# Concatenate y as an additional column to X\nX_with_y = np.concatenate((X, y_reshaped), axis=1)\n\n# Compute the correlation matrix\ncorrelation_matrix = np.corrcoef(X_with_y, rowvar=False)\n\n\nprint(np.round(correlation_matrix,1))\n\n[[ 1.   0.2  0.2  0.3  0.3  0.2 -0.1  0.2  0.3  0.3  0.2]\n [ 0.2  1.   0.1  0.2  0.   0.1 -0.4  0.3  0.1  0.2  0. ]\n [ 0.2  0.1  1.   0.4  0.2  0.3 -0.4  0.4  0.4  0.4  0.6]\n [ 0.3  0.2  0.4  1.   0.2  0.2 -0.2  0.3  0.4  0.4  0.4]\n [ 0.3  0.   0.2  0.2  1.   0.9  0.1  0.5  0.5  0.3  0.2]\n [ 0.2  0.1  0.3  0.2  0.9  1.  -0.2  0.7  0.3  0.3  0.2]\n [-0.1 -0.4 -0.4 -0.2  0.1 -0.2  1.  -0.7 -0.4 -0.3 -0.4]\n [ 0.2  0.3  0.4  0.3  0.5  0.7 -0.7  1.   0.6  0.4  0.4]\n [ 0.3  0.1  0.4  0.4  0.5  0.3 -0.4  0.6  1.   0.5  0.6]\n [ 0.3  0.2  0.4  0.4  0.3  0.3 -0.3  0.4  0.5  1.   0.4]\n [ 0.2  0.   0.6  0.4  0.2  0.2 -0.4  0.4  0.6  0.4  1. ]]\n\n\nI’m using the above correlation matrix as weight initializer to check whether the weights will be in similar way to that of the regular OLS.\n\nno_of_features = X.shape[1]\n\ninitializer_weights = tf.keras.initializers.Constant(value= [0.2,0.2,0.3,0.3,0.2,-0.1,0.2,0.3,0.3,0.2])\n\ninitializer_bias = tf.keras.initializers.Constant(value = [1.0])\n\n\nmodel_tf = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1,\n                          activation=\"linear\",\n                          input_shape=(no_of_features,),\n                          kernel_initializer = initializer_weights,\n                          bias_initializer = initializer_bias)\n])\n\nmodel_tf.compile(optimizer='sgd', loss='mean_squared_error',metrics=['mse'])\n\nmodel_tf.fit(X_train, y_train, epochs=1500, verbose=0)\n\ny_pred_test = model_tf.predict(X_test)\n\n3/3 [==============================] - 0s 4ms/step\n\n\n\nmodel_tf.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_7 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 11 (44.00 Byte)\nTrainable params: 11 (44.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nAs we can see above there is just one layer connecting the paramters;\n11 Paramters are 10 weight paramters and 1 bias parameter that needs to be learned by the neural network\n\nmse_tf = mean_squared_error(y_test, y_pred_test.flatten())\nprint(\"Mean Squared Error:\", mse_tf)\n\nMean Squared Error: 2925.001255544474\n\n\n\nweights, bias = model_tf.layers[0].get_weights()\nprint(\"Weights:\")\nprint(weights)\nprint(\"Bias:\")\nprint(bias)\n\nWeights:\n[[ 5.9075676e+01]\n [-9.0772835e+01]\n [ 3.6696533e+02]\n [ 2.5461090e+02]\n [ 9.4142288e-02]\n [-3.7435982e+01]\n [-1.8325632e+02]\n [ 1.4889064e+02]\n [ 2.8909210e+02]\n [ 1.5162097e+02]]\nBias:\n[153.55777]"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#comparision-of-parameters",
    "href": "posts/shallow-linear-model/index.html#comparision-of-parameters",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Comparision of Parameters",
    "text": "Comparision of Parameters\n\nimport pandas as pd\n\n\npd.concat(\n    [pd.DataFrame(model.coef_, columns=[\"Model Coefficients of OLS\"]),\n    pd.DataFrame(weights,columns=[\"Model Coefficients of NN\"])],\n    axis = 1\n)\n\n\n\n  \n    \n\n\n\n\n\n\nModel Coefficients of OLS\nModel Coefficients of NN\n\n\n\n\n0\n37.904021\n59.075676\n\n\n1\n-241.964362\n-90.772835\n\n\n2\n542.428759\n366.965332\n\n\n3\n347.703844\n254.610901\n\n\n4\n-931.488846\n0.094142\n\n\n5\n518.062277\n-37.435982\n\n\n6\n163.419983\n-183.256317\n\n\n7\n275.317902\n148.890640\n\n\n8\n736.198859\n289.092102\n\n\n9\n48.670657\n151.620972\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\nAt this juncture, it’s evident that the weights in neural networks are derived through iterative optimization methods, notably gradient descent, optimizing model performance. Conversely, OLS regression employs statistical techniques to determine model coefficients. Consequently, the fundamental nature of these weights differs significantly.\nAt present, it’s feasible to manually interpret the coefficients derived from OLS regression, aiding in understanding the relationships between variables.\nHowever, this interpretability is not readily achievable with neural networks, underscoring their characterization as black box models primarily designed for prediction rather than inference.\nIt seems there is ongoing research seeks to enhance interpretability in neural networks which i’m not yet aware at this point of time."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This website is designed to record my learnings on Deep Learning using Tensor Flow Framework. Since, website will be very handy to review even on a mobile compared to Jupyter Notebook.\nThe coding is done in python majorly using the following packages as of now:\n\nTensor Flow\nnumpy\nPandas\nplotnine { Similar to that of ggplot of R }\ndfply { Similar to that of tidyverse of R }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShallow Neural Network - Linear Regression\n\n\n\n\n\n\nLinear Regression\n\n\nShallow Neural Network\n\n\n\nFirst Neural network model\n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics - 2\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics -1\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\nNo matching items"
  }
]
[
  {
    "objectID": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#what-is-tensor-flow",
    "href": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#what-is-tensor-flow",
    "title": "TensorFlow Basics",
    "section": "What is tensor Flow?",
    "text": "What is tensor Flow?\nTensor flow is deep learning framework developed by Google in 2011 and made it publicly available in the year 2015.\nIt is flexible, scalable solutio that enable us to build models with the existing frameworks.\nIt is like sci-kit learn library but more advanced and flexible as we can custom build our own neural network.\n\nprint(tf.__version__)\n\n2.15.0"
  },
  {
    "objectID": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#basics-unit-in-tensorflow-framework---tensor",
    "href": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#basics-unit-in-tensorflow-framework---tensor",
    "title": "TensorFlow Basics",
    "section": "Basics Unit in TensorFlow Framework - Tensor",
    "text": "Basics Unit in TensorFlow Framework - Tensor\nTensors are multi-dimensional arrays designed for numerical data representation; although they share some similarities with NumPy arrays, they possess certain unique features that give them an advantage in deep learning tasks. One of these key advantages is their ability to utilize hardware acceleration from GPUs and TPUs to significantly speed up computational operations, which is especially useful when working with input data such as images, text, and videos.\nIn simple words “ML” needs numbers; In case of large dimensions we need matrices. These matrices are called tensors. As these are specially designed to use hardware capabilities to accelerate learnings."
  },
  {
    "objectID": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#creating-tensors",
    "href": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#creating-tensors",
    "title": "TensorFlow Basics",
    "section": "Creating Tensors",
    "text": "Creating Tensors\n\ntf.constant(1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 1],\n       [1, 1]], dtype=int32)&gt;\n\n\nHere we have created a basic tensor with constant number 1 with a shape 2,2 i.e.; two rows and two columns.\nAnd its datatype is integer.\nIt is a numpy array.\n\n## Manually providing the shape\ny = tf.constant([[1, 2, 3], [4, 5, 6]])\nprint(y)\n\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nInstead of giving the shape here we have manually given the values\n\ntf.rank(y)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\nHere we are checking what is the rank of the tensor.\n\n\n\nimage.png\n\n\n\nprint(\"The rank of scalar is \" , tf.rank(tf.constant(1)))\nprint(\"The rank of vector is \" , tf.rank(tf.constant(1,shape=(5))))\nprint(\"The rank of matrix is \" , tf.rank(tf.constant(1,shape=(5,4))))\nprint(\"The rank of rank3tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3))))\n\nThe rank of scalar is  tf.Tensor(0, shape=(), dtype=int32)\nThe rank of vector is  tf.Tensor(1, shape=(), dtype=int32)\nThe rank of matrix is  tf.Tensor(2, shape=(), dtype=int32)\nThe rank of rank3tensor is  tf.Tensor(3, shape=(), dtype=int32)\n\n\nCan there me more than 3 dimensional; of course, but we cannot represent them pictographically.\n\nprint(\"The rank of rank5tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3))))\nprint(\"The rank of rank9tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3,1,1,3,3))))\n\nThe rank of rank5tensor is  tf.Tensor(5, shape=(), dtype=int32)\nThe rank of rank9tensor is  tf.Tensor(9, shape=(), dtype=int32)"
  },
  {
    "objectID": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#basic-tensor-operations",
    "href": "TensorFlow_Practice/Basics-1/TensorFlowBasics.html#basic-tensor-operations",
    "title": "TensorFlow Basics",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\ntf.constant(1.1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1.1, 1.1],\n       [1.1, 1.1]], dtype=float32)&gt;\n\n\n\nTypeCasting\nThe moment we kept 1.1 the data-type has changed to float; Lets check how to typecast integer to float and vice versa\n\nx_int = tf.constant(1, shape=(2,2))\nprint(x_int)\nx_float = tf.cast(x_int, dtype = tf.float32)\nprint(x_float)\nx_float_int = tf.cast(x_float, tf.int32)\nprint(x_float_int)\n\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\n\n\n\n\nIndexing\nSimilar to numpy array we can do indexing for the tensors\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\ny[0]\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\ny[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n\nExpanding a matrix\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(tf.expand_dims(y,axis=0)) ## Expanding at the beginning of the tensor\nprint(tf.expand_dims(y,axis=1)) ## Expanding at the Middle of the tensor { for this example}\nprint(tf.expand_dims(y,axis=-1)) ## Expanding at the End of the tensor\n\ntf.Tensor(\n[[[1 2 3]\n  [4 5 6]]], shape=(1, 2, 3), dtype=int32)\ntf.Tensor(\n[[[1 2 3]]\n\n [[4 5 6]]], shape=(2, 1, 3), dtype=int32)\ntf.Tensor(\n[[[1]\n  [2]\n  [3]]\n\n [[4]\n  [5]\n  [6]]], shape=(2, 3, 1), dtype=int32)\n\n\n\n\nTensor Aggregation\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(\"Smallest of the number is \",tf.reduce_min(y).numpy())\nprint(\"Largest of the number is \",tf.reduce_max(y).numpy())\n\nSmallest of the number is  1\nLargest of the number is  6\n\n\n\nprint(\"Sum of the numbers are \",tf.reduce_sum(y).numpy())\nprint(\"Average of the numbers are \",tf.reduce_mean(y).numpy())\n\nSum of the numbers are  21\nAverage of the numbers are  3\n\n\n\n\nMatrix with all ones,zeroes and identity\n\nz =  tf.ones([2,3])\nprint(z)\nprint(\" \")\n\nx  =  tf.constant(1,shape=(2,3),dtype=tf.float32)\nprint(x)\nprint(\" \")\n\nz =  tf.zeros([2,3])\nprint(z)\nprint(\" \")\n\nz = tf.eye(3)\nprint(z)\n\ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[0. 0. 0.]\n [0. 0. 0.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float32)\n\n\n\n\nReshaping and Transposing Tensors\n\nx_initial = tf.constant(1, shape=(4,3))\nprint(x_initial)\n\ntf.Tensor(\n[[1 1 1]\n [1 1 1]\n [1 1 1]\n [1 1 1]], shape=(4, 3), dtype=int32)\n\n\n\ntf.reshape(x_initial,shape=(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[1, 1, 1],\n        [1, 1, 1]],\n\n       [[1, 1, 1],\n        [1, 1, 1]]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(2,6))\n\n&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(12,1))\n\n&lt;tf.Tensor: shape=(12, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1]], dtype=int32)&gt;\n\n\nHere we can reshape to any other shape; However, the multiplication of shapes should remain the same.\n\ntf.reshape(x_initial,shape=-1) #Flatten the array\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)&gt;\n\n\n\ntf.transpose(x_initial)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[1, 1, 1, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 1]], dtype=int32)&gt;\n\n\nThe initial shape of (4,3) got changed to (3,4)\n\n\nDistributions\n\nx1  = tf.random.normal((3,3))\nprint(x1)\nprint(\" \")\n\nx1  = tf.random.normal((3,3),mean = 0, stddev =1 )\nprint(x1)\nprint(\" \")\n\nx2 =  tf.random.uniform((3,3))\nprint(x2)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.591363    0.12791212  0.38762185]\n [ 0.26025018  1.7209182  -0.7802837 ]\n [ 0.89150804 -0.9648455   0.64507854]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-1.0034778   0.05435322 -1.3141975 ]\n [-0.17819698 -1.9136705  -0.9396771 ]\n [ 2.2143493   0.33600262  0.8174351 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[0.04812372 0.79855204 0.8067709 ]\n [0.67069924 0.06617999 0.14941025]\n [0.31500185 0.17441607 0.7476181 ]], shape=(3, 3), dtype=float32)\n \n\n\n\n\nMathematical Operations\n\nAddition\nSubtraction\nMultiplication\nDivision\n\n\nx4 = tf.random.normal((3,3))\nprint(x4)\nprint(\" \")\ny4 = tf.random.normal((3,3))\nprint(y4)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.5908952   3.5452905  -0.34438497]\n [-0.5237503   1.2899861  -0.50684774]\n [ 1.2187229   0.50014    -0.6212071 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-0.7346279   2.9705956  -0.45318994]\n [-0.947753    0.6556651   3.018978  ]\n [-1.5799906  -1.2278746   0.57451475]], shape=(3, 3), dtype=float32)\n \n\n\n\nprint(x4+y4)\nprint(\" \")\ntf.add(x4,y4)\n\ntf.Tensor(\n[[-0.14373273  6.5158863  -0.7975749 ]\n [-1.4715033   1.9456513   2.5121303 ]\n [-0.3612677  -0.7277346  -0.04669237]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.14373273,  6.5158863 , -0.7975749 ],\n       [-1.4715033 ,  1.9456513 ,  2.5121303 ],\n       [-0.3612677 , -0.7277346 , -0.04669237]], dtype=float32)&gt;\n\n\n\nprint(x4-y4)\nprint(\" \")\ntf.subtract(x4,y4)\n\ntf.Tensor(\n[[ 1.3255231   0.5746949   0.10880497]\n [ 0.4240027   0.63432103 -3.525826  ]\n [ 2.7987137   1.7280147  -1.1957219 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 1.3255231 ,  0.5746949 ,  0.10880497],\n       [ 0.4240027 ,  0.63432103, -3.525826  ],\n       [ 2.7987137 ,  1.7280147 , -1.1957219 ]], dtype=float32)&gt;\n\n\n\nprint(x4*y4)\nprint(\" \")\ntf.multiply(x4,y4)\n\ntf.Tensor(\n[[-0.43408808 10.531624    0.1560718 ]\n [ 0.49638593  0.8457989  -1.5301622 ]\n [-1.9255708  -0.6141092  -0.35689265]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.43408808, 10.531624  ,  0.1560718 ],\n       [ 0.49638593,  0.8457989 , -1.5301622 ],\n       [-1.9255708 , -0.6141092 , -0.35689265]], dtype=float32)&gt;\n\n\n\nprint(x4/y4)\nprint(\" \")\ntf.divide(x4,y4)\n\ntf.Tensor(\n[[-0.8043462   1.1934612   0.7599131 ]\n [ 0.5526232   1.9674467  -0.16788718]\n [-0.77134824 -0.40732172 -1.0812727 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.8043462 ,  1.1934612 ,  0.7599131 ],\n       [ 0.5526232 ,  1.9674467 , -0.16788718],\n       [-0.77134824, -0.40732172, -1.0812727 ]], dtype=float32)&gt;\n\n\n\n\nMatrix Multiplications\n\nx4_new = tf.random.normal((3,2))\ny4_new = tf.random.normal((2,3))\n\nprint(tf.matmul(x4_new,y4_new))\n\ntf.Tensor(\n[[ 0.5675167   0.29923582 -2.018334  ]\n [-0.5145724   1.9392778  -1.5560541 ]\n [ 1.0566927  -2.3777525   0.73752195]], shape=(3, 3), dtype=float32)\n\n\n\nx4_new @ y4_new\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.5675167 ,  0.29923582, -2.018334  ],\n       [-0.5145724 ,  1.9392778 , -1.5560541 ],\n       [ 1.0566927 , -2.3777525 ,  0.73752195]], dtype=float32)&gt;\n\n\nTwo ways of matrix of multiplication in tensors; There is also a way using dot product; which we will discuss later :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This website is designed to record my learnings on Deep Learning using Tensor Flow Framework. Since, website will be very handy to review even on a mobile compared to Jupyter Notebook.\nThe coding is done in python majorly using the following packages as of now:\n\nTensor Flow\nnumpy\nPandas\nplotnine { Similar to that of ggplot of R }\ndfply { Similar to that of tidyverse of R }\n\n\n\n\nDate\nLink for my learning\nTopic\nComments\n\n\n\n\n20th Mar 2024\nTensor Basics\nTensor Basics\nMy initial learnings on tensors"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is dedicated for learnings on deep learning using tensor flow.\nMy other sites."
  }
]
[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is dedicated for learnings on deep learning using tensor flow.\nMy other sites."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html",
    "href": "posts/tensor-basics-part2/index.html",
    "title": "Tensor Flow Basics - 2",
    "section": "",
    "text": "This article primarily discussed on importance of broadcasting and its easy of implementation using tensors."
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "href": "posts/tensor-basics-part2/index.html#tensor-data-types",
    "title": "Tensor Flow Basics - 2",
    "section": "Tensor data types",
    "text": "Tensor data types\n\n\n\n\n\n\n\n\nData Type\nPython Type\nDescription\n\n\n\n\nDT_FLOAT\ntf.float32\n32 bits floating point.\n\n\nDT_DOUBLE\ntf.float64\n64 bits floating point.\n\n\nDT_INT8\ntf.int8\n8 bits signed integer.\n\n\nDT_INT16\ntf.int16\n16 bits signed integer.\n\n\nDT_INT32\ntf.int32\n32 bits signed integer.\n\n\nDT_INT64\ntf.int64\n64 bits signed integer.\n\n\nDT_UINT8\ntf.uint8\n8 bits unsigned integer.\n\n\nDT_STRING\ntf.string\nVariable length byte arrays. Each element of a tensor is a byte array.\n\n\nDT_BOOL\ntf.bool\nBoolean.\n\n\n\n\nimport tensorflow as tf\nprint(tf.__version__)\n\n2.15.0\n\n\n\nx_new = tf.constant(1,shape=(2,3),dtype=tf.int8)\n\n\ny_new = tf.constant(2,shape=(2,3),dtype=tf.int8)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "href": "posts/tensor-basics-part2/index.html#re-cap-operations",
    "title": "Tensor Flow Basics - 2",
    "section": "Re-cap Operations",
    "text": "Re-cap Operations\n\nElement Wise Operations\nMatrix Multiplicaitons\n\n\nprint(\"Addition:\\n\", tf.add(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nprint(\"Subtraction:\\n\", tf.subtract(x_new,y_new))\nprint(\" \")\nprint(\" \")\n\nAddition:\n tf.Tensor(\n[[3 3 3]\n [3 3 3]], shape=(2, 3), dtype=int8)\n \n \nSubtraction:\n tf.Tensor(\n[[-1 -1 -1]\n [-1 -1 -1]], shape=(2, 3), dtype=int8)\n \n \n\n\n\nprint(\"Multiplication:\\n\", tf.multiply(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(2,3))))\n\nMultiplication:\n tf.Tensor(\n[[-0.5718114  -0.18780218 -2.0768495 ]\n [ 0.29304612  0.08317164 -1.3320862 ]], shape=(2, 3), dtype=float32)\n\n\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,2))))\nprint(\" \")\nprint(\" \")\n\nprint(\"Matrix Multiplication:\\n\", tf.matmul(tf.random.normal(shape=(2,3)),tf.random.normal(shape=(3,1))))\n\nMatrix Multiplication:\n tf.Tensor(\n[[-1.6848389  -0.44041786]\n [-0.9548799  -0.22109865]], shape=(2, 2), dtype=float32)\n \n \nMatrix Multiplication:\n tf.Tensor(\n[[ 0.37789747]\n [-0.12570143]], shape=(2, 1), dtype=float32)"
  },
  {
    "objectID": "posts/tensor-basics-part2/index.html#broadcasting",
    "href": "posts/tensor-basics-part2/index.html#broadcasting",
    "title": "Tensor Flow Basics - 2",
    "section": "Broadcasting",
    "text": "Broadcasting\nBroadcasting is useful when the dimensions of matrices are different from each other. This is highly useful to add and subtract the matrices of different lengths.\nHere’s a brief summary of how broadcasting works for addition and subtraction:\nAddition: If the shapes of the two tensors are different, TensorFlow compares their shapes element-wise, starting from the trailing dimensions. If the dimensions are compatible or one of the dimensions is 1, broadcasting can occur. For example, you can add a scalar to a matrix, and the scalar value will be added to every element of the matrix.\nSubtraction: Similar to addition, broadcasting allows you to subtract a scalar or a vector from a matrix, or subtract matrices of different shapes.\n\nx1 = tf.random.normal(shape=(2,3))\n\nx2 = tf.random.normal(shape=(2,1))\n\n\nprint(\"x1:\\n\",x1)\nprint(\" \")\nprint(\" \")\nprint(\"x2:\\n\",x2)\n\nx1:\n tf.Tensor(\n[[ 0.39297998 -0.12811422 -0.60474324]\n [ 0.30773026  1.4076523  -0.57274765]], shape=(2, 3), dtype=float32)\n \n \nx2:\n tf.Tensor(\n[[0.10113019]\n [1.0277312 ]], shape=(2, 1), dtype=float32)\n\n\n\ntf.broadcast_to(x2, [2, 3])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[0.10113019, 0.10113019, 0.10113019],\n       [1.0277312 , 1.0277312 , 1.0277312 ]], dtype=float32)&gt;\n\n\n\ntf.add(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.49411017, -0.02698404, -0.50361305],\n       [ 1.3354614 ,  2.4353833 ,  0.45498353]], dtype=float32)&gt;\n\n\n\ntf.subtract(x1,x2)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 0.2918498 , -0.22924441, -0.7058734 ],\n       [-0.7200009 ,  0.37992108, -1.6004789 ]], dtype=float32)&gt;\n\n\nIn the above examples we have seen that broadcasting of same tensor dimension i.e.; of two dimensional.\nBelow, we will see the beauty of broadcasting of different dimensions.\n\nx3 = tf.random.normal(shape=(2, 2, 2)) #3d tensor\nx4 = tf.random.normal(shape=(2, 1)) #2d tensor\n\n\ntf.add(x3,x4)\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-1.0228443 ,  0.34588122],\n        [ 2.188323  , -0.7523911 ]],\n\n       [[-0.56017506, -1.5636952 ],\n        [ 1.572252  ,  1.6189265 ]]], dtype=float32)&gt;\n\n\n\ntf.broadcast_to(x4, [2,2,2])   ### Broadcasting internally\n\n&lt;tf.Tensor: shape=(2, 2, 2), dtype=float32, numpy=\narray([[[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]],\n\n       [[-0.66719866, -0.66719866],\n        [ 0.92839885,  0.92839885]]], dtype=float32)&gt;"
  },
  {
    "objectID": "posts/hugging_face_sentiment_classification/index.html",
    "href": "posts/hugging_face_sentiment_classification/index.html",
    "title": "Hugging Face - Sentiment Classification",
    "section": "",
    "text": "In this module we will explore basics of Hugging Face for Sentiment Classification. Subtopics that we will explore in this blog.\nPrimarly this is prepeared for my ready reference for future. Hope this helps others too."
  },
  {
    "objectID": "posts/hugging_face_sentiment_classification/index.html#hugging-face-pipeline",
    "href": "posts/hugging_face_sentiment_classification/index.html#hugging-face-pipeline",
    "title": "Hugging Face - Sentiment Classification",
    "section": "1. Hugging Face Pipeline",
    "text": "1. Hugging Face Pipeline\nThe pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. Pipeline Reference.\n\nfrom transformers import pipeline\n\nHere we need to import pipeline from transformers package. This function primarly has two important inputs.\n\nWhat Task we need to perform.\nWhat model we need to use to perform the task.\n\nAs shown in the below image we can choose the task that we need to use at hand.\n\nWe can select any relevant model from this link.\nTo start with I choose: 1. Task as Sentiment Analysis 2. Model as Distilbert-base\n\ntask = \"sentiment-analysis\"\n\ncheckpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n\nsentiment_classifer = pipeline(task,model = checkpoint,framework=\"tf\")\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\nAll PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n\nAll the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n\n\n\n\n\n\n\n\nIf we can observe all the related base files are loaded; that includes model configuration, model itself and vocab text\nNow predicting is simple like we use chatgpt\n\nsentiment_classifer(\"This is a cool blog!!!\")\n\n[{'label': 'POSITIVE', 'score': 0.9998422861099243}]\n\n\n\nsentiment_classifer(\"Games are boring!!!\")\n\n[{'label': 'NEGATIVE', 'score': 0.9996864795684814}]\n\n\nWe can even pass the inputs in a list\n\nraw_inputs = [\n    \"I've been waiting for a HuggingFace course my whole life.\",\n    \"I hate this so much!\",\n    \"My niece absolutely loves this. It’s a great way to get her outside but keep her distracted so she’s not running around the yard. It was very easy to assemble and super durable. Also super easy to clean.\"\n]\n\n\nsentiment_classifer(raw_inputs)\n\n[{'label': 'POSITIVE', 'score': 0.9598046541213989},\n {'label': 'NEGATIVE', 'score': 0.9994558691978455},\n {'label': 'POSITIVE', 'score': 0.9994500279426575}]\n\n\nWe can see how simple the pipeline module works. All we need to identify our task and model and create a pipeline.\nAfter creating a pipeline we need to pass our input data and wait for its prediction. The beauty of this prediction is that it provides the final labels too."
  },
  {
    "objectID": "posts/hugging_face_sentiment_classification/index.html#inside-pipeline",
    "href": "posts/hugging_face_sentiment_classification/index.html#inside-pipeline",
    "title": "Hugging Face - Sentiment Classification",
    "section": "2. Inside Pipeline",
    "text": "2. Inside Pipeline\nInside Pipeline we have three steps:\n\nTokenizing\nModel\nPost Processing\n\nTokenizing means - Tokenizing the sentence into numerical values. Model - Uses these token ids and predicts the logits Post Processing - We will use obtained logits to arrive at our required labels.\nFor reference see the below image.\n\n\n2.1 AutoTokenizer\nAutoTokenizer is a generic tokenizer that can be used for wide range of models offers by hugging face.\nWe need to create a tokenizer object\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntype(tokenizer)\n\n\n    transformers.models.distilbert.tokenization_distilbert_fast.DistilBertTokenizerFastdef __call__(text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]=None, text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]=None, text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]=None, text_pair_target: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]=None, add_special_tokens: bool=True, padding: Union[bool, str, PaddingStrategy]=False, truncation: Union[bool, str, TruncationStrategy]=None, max_length: Optional[int]=None, stride: int=0, is_split_into_words: bool=False, pad_to_multiple_of: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_token_type_ids: Optional[bool]=None, return_attention_mask: Optional[bool]=None, return_overflowing_tokens: bool=False, return_special_tokens_mask: bool=False, return_offsets_mapping: bool=False, return_length: bool=False, verbose: bool=True, **kwargs) -&gt; BatchEncoding/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/tokenization_distilbert_fast.pyConstruct a \"fast\" DistilBERT tokenizer (backed by HuggingFace's *tokenizers* library). Based on WordPiece.\n\nThis tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\nrefer to this superclass for more information regarding those methods.\n\nArgs:\n    vocab_file (`str`):\n        File containing the vocabulary.\n    do_lower_case (`bool`, *optional*, defaults to `True`):\n        Whether or not to lowercase the input when tokenizing.\n    unk_token (`str`, *optional*, defaults to `\"[UNK]\"`):\n        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n        token instead.\n    sep_token (`str`, *optional*, defaults to `\"[SEP]\"`):\n        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n        sequence classification or for a text and a question for question answering. It is also used as the last\n        token of a sequence built with special tokens.\n    pad_token (`str`, *optional*, defaults to `\"[PAD]\"`):\n        The token used for padding, for example when batching sequences of different lengths.\n    cls_token (`str`, *optional*, defaults to `\"[CLS]\"`):\n        The classifier token which is used when doing sequence classification (classification of the whole sequence\n        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n    mask_token (`str`, *optional*, defaults to `\"[MASK]\"`):\n        The token used for masking values. This is the token used when training this model with masked language\n        modeling. This is the token which the model will try to predict.\n    clean_text (`bool`, *optional*, defaults to `True`):\n        Whether or not to clean the text before tokenization by removing any control characters and replacing all\n        whitespaces by the classic one.\n    tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):\n        Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see [this\n        issue](https://github.com/huggingface/transformers/issues/328)).\n    strip_accents (`bool`, *optional*):\n        Whether or not to strip all accents. If this option is not specified, then it will be determined by the\n        value for `lowercase` (as in the original BERT).\n    wordpieces_prefix (`str`, *optional*, defaults to `\"##\"`):\n        The prefix for subwords.\n      \n      \n\n\nBased on the model it automatically loads the required type of tokenizer. Since we used our checkpoint as distilbert we got the same tokenizer.\nLets check the same with another model for demonstration sake.\n\ntype(AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment-latest\"))\n\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFastdef __call__(text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]=None, text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]=None, text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]=None, text_pair_target: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]=None, add_special_tokens: bool=True, padding: Union[bool, str, PaddingStrategy]=False, truncation: Union[bool, str, TruncationStrategy]=None, max_length: Optional[int]=None, stride: int=0, is_split_into_words: bool=False, pad_to_multiple_of: Optional[int]=None, return_tensors: Optional[Union[str, TensorType]]=None, return_token_type_ids: Optional[bool]=None, return_attention_mask: Optional[bool]=None, return_overflowing_tokens: bool=False, return_special_tokens_mask: bool=False, return_offsets_mapping: bool=False, return_length: bool=False, verbose: bool=True, **kwargs) -&gt; BatchEncoding/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/tokenization_roberta_fast.pyConstruct a \"fast\" RoBERTa tokenizer (backed by HuggingFace's *tokenizers* library), derived from the GPT-2\ntokenizer, using byte-level Byte-Pair-Encoding.\n\nThis tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\nbe encoded differently whether it is at the beginning of the sentence (without space) or not:\n\n```python\n&gt;&gt;&gt; from transformers import RobertaTokenizerFast\n\n&gt;&gt;&gt; tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n&gt;&gt;&gt; tokenizer(\"Hello world\")[\"input_ids\"]\n[0, 31414, 232, 2]\n\n&gt;&gt;&gt; tokenizer(\" Hello world\")[\"input_ids\"]\n[0, 20920, 232, 2]\n```\n\nYou can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\ncall it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n\n&lt;Tip&gt;\n\nWhen used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n\n&lt;/Tip&gt;\n\nThis tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\nrefer to this superclass for more information regarding those methods.\n\nArgs:\n    vocab_file (`str`):\n        Path to the vocabulary file.\n    merges_file (`str`):\n        Path to the merges file.\n    errors (`str`, *optional*, defaults to `\"replace\"`):\n        Paradigm to follow when decoding bytes to UTF-8. See\n        [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n    bos_token (`str`, *optional*, defaults to `\"&lt;s&gt;\"`):\n        The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n\n        &lt;Tip&gt;\n\n        When building a sequence using special tokens, this is not the token that is used for the beginning of\n        sequence. The token used is the `cls_token`.\n\n        &lt;/Tip&gt;\n\n    eos_token (`str`, *optional*, defaults to `\"&lt;/s&gt;\"`):\n        The end of sequence token.\n\n        &lt;Tip&gt;\n\n        When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n        The token used is the `sep_token`.\n\n        &lt;/Tip&gt;\n\n    sep_token (`str`, *optional*, defaults to `\"&lt;/s&gt;\"`):\n        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n        sequence classification or for a text and a question for question answering. It is also used as the last\n        token of a sequence built with special tokens.\n    cls_token (`str`, *optional*, defaults to `\"&lt;s&gt;\"`):\n        The classifier token which is used when doing sequence classification (classification of the whole sequence\n        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n    unk_token (`str`, *optional*, defaults to `\"&lt;unk&gt;\"`):\n        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n        token instead.\n    pad_token (`str`, *optional*, defaults to `\"&lt;pad&gt;\"`):\n        The token used for padding, for example when batching sequences of different lengths.\n    mask_token (`str`, *optional*, defaults to `\"&lt;mask&gt;\"`):\n        The token used for masking values. This is the token used when training this model with masked language\n        modeling. This is the token which the model will try to predict.\n    add_prefix_space (`bool`, *optional*, defaults to `False`):\n        Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n        other word. (RoBERTa tokenizer detect beginning of words by the preceding space).\n    trim_offsets (`bool`, *optional*, defaults to `True`):\n        Whether the post processing step should trim offsets to avoid including whitespaces.\n      \n      \n\n\nSince, we have used Roberta model. Autotokenizer automatically loads Roberta Tokenizer. The beauty of Autotokenizer wrapper is that it became model agnostic.\nAll we need is a simple command to convert words into token ids. Here we have added padding to create padding to ensure same structure of all sentence lengths. Truncation is then introducted to maintain the max token length of the model.\n\ntokenizer(raw_inputs,padding=True,truncation=True,return_tensors=\"tf\")\n\n{'input_ids': &lt;tf.Tensor: shape=(3, 48), dtype=int32, numpy=\narray([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n        12172,  2607,  2026,  2878,  2166,  1012,   102,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0],\n       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0],\n       [  101,  2026, 12286,  7078,  7459,  2023,  1012,  2009,  1521,\n         1055,  1037,  2307,  2126,  2000,  2131,  2014,  2648,  2021,\n         2562,  2014, 11116,  2061,  2016,  1521,  1055,  2025,  2770,\n         2105,  1996,  4220,  1012,  2009,  2001,  2200,  3733,  2000,\n        21365,  1998,  3565, 25634,  1012,  2036,  3565,  3733,  2000,\n         4550,  1012,   102]], dtype=int32)&gt;, 'attention_mask': &lt;tf.Tensor: shape=(3, 48), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1]], dtype=int32)&gt;}\n\n\nOutput generates input_ids these are numerical values that are specific to vocabulary. attention_mask to tell the model which are padded numbers and which aren’t. Now, we are all ready to use this generated tokenids for our modelling.\n\ntokenized_ids = tokenizer(raw_inputs,padding=True,truncation=True,return_tensors=\"tf\")\n\n\n\n2.2. Sequence Classification Model\nSimilar to AutoTokenizer for modelling we have AutoModel and since our task is Sequence Classification we will directly use AutoModelForSequenceClassification.\nNow we will create a model object\n\nfrom transformers import TFAutoModelForSequenceClassification\n\nbert_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n\ntype(bert_model)\n\nAll PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n\nAll the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n\n\n\n    transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassificationdef error_handler(*args, **kwargs)/usr/local/lib/python3.10/dist-packages/transformers/models/distilbert/modeling_tf_distilbert.pyDistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the\npooled output) e.g. for GLUE tasks.\n\n\nThis model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the\nlibrary implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\netc.)\n\nThis model is also a [keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it\nas a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and\nbehavior.\n\n&lt;Tip&gt;\n\nTensorFlow models and layers in `transformers` accept two formats as input:\n\n- having all inputs as keyword arguments (like PyTorch models), or\n- having all inputs as a list, tuple or dict in the first positional argument.\n\nThe reason the second format is supported is that Keras methods prefer this format when passing inputs to models\nand layers. Because of this support, when using methods like `model.fit()` things should \"just work\" for you - just\npass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second\nformat outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with\nthe Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first\npositional argument:\n\n- a single Tensor with `input_ids` only and nothing else: `model(input_ids)`\n- a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:\n`model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`\n- a dictionary with one or several input Tensors associated to the input names given in the docstring:\n`model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids})`\n\nNote that when creating models and layers with\n[subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry\nabout any of this, as you can just pass inputs like you would to any other Python function!\n\n&lt;/Tip&gt;\n\nParameters:\n    config ([`DistilBertConfig`]): Model configuration class with all the parameters of the model.\n        Initializing with a config file does not load the weights associated with the model, only the\n        configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n      \n      \n\n\nBased on the model it automatically loads the required type of TFModel. Since we used our checkpoint as distilbert we got the same model.\nHere TF stands for TensorFlow since we will using TensorFlow Framework isntead of pytorch.\nWe can see the model configuration by using the keywork .config. This model is trained on 30K vocab size.\n\nbert_model.config\n\nDistilBertConfig {\n  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForSequenceClassification\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"finetuning_task\": \"sst-2\",\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"NEGATIVE\",\n    \"1\": \"POSITIVE\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"NEGATIVE\": 0,\n    \"POSITIVE\": 1\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.41.0\",\n  \"vocab_size\": 30522\n}\n\n\nWe can explore the model architecture by simply viewing the summary as well. Which has a core distilbert model followed by a pre-classifier and then a classifer layer for predicting the output.\n\nbert_model.summary()\n\nModel: \"tf_distil_bert_for_sequence_classification_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n pre_classifier (Dense)      multiple                  590592    \n                                                                 \n classifier (Dense)          multiple                  1538      \n                                                                 \n dropout_39 (Dropout)        multiple                  0 (unused)\n                                                                 \n=================================================================\nTotal params: 66955010 (255.41 MB)\nTrainable params: 66955010 (255.41 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nWe can view the architecture of individual layers in the following way.\n\nfor layer in bert_model.layers:\n  print(layer.name)\n  print(layer.get_config())\n\ndistilbert\n{'name': 'distilbert', 'trainable': True, 'dtype': 'float32', 'config': {'vocab_size': 30522, 'max_position_embeddings': 512, 'sinusoidal_pos_embds': False, 'n_layers': 6, 'n_heads': 12, 'dim': 768, 'hidden_dim': 3072, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation': 'gelu', 'initializer_range': 0.02, 'qa_dropout': 0.1, 'seq_classif_dropout': 0.2, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': None, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': True, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['DistilBertForSequenceClassification'], 'finetuning_task': 'sst-2', 'id2label': {0: 'NEGATIVE', 1: 'POSITIVE'}, 'label2id': {'NEGATIVE': 0, 'POSITIVE': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 0, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'distilbert/distilbert-base-uncased-finetuned-sst-2-english', 'transformers_version': '4.41.0', 'model_type': 'distilbert', 'output_past': True, 'tie_weights_': True}}\npre_classifier\n{'name': 'pre_classifier', 'trainable': True, 'dtype': 'float32', 'units': 768, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\nclassifier\n{'name': 'classifier', 'trainable': True, 'dtype': 'float32', 'units': 2, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'module': 'keras.initializers', 'class_name': 'TruncatedNormal', 'config': {'mean': 0.0, 'stddev': 0.02, 'seed': None}, 'registered_name': None}, 'bias_initializer': {'module': 'keras.initializers', 'class_name': 'Zeros', 'config': {}, 'registered_name': None}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\ndropout_39\n{'name': 'dropout_39', 'trainable': True, 'dtype': 'float32', 'rate': 0.2, 'noise_shape': None, 'seed': None}\n\n\nNow let’s pass the tokenized_ids that we have saved in the previous section and pass to this model.\n\nbert_model(tokenized_ids)\n\nTFSequenceClassifierOutput(loss=None, logits=&lt;tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[-1.560698 ,  1.6122831],\n       [ 4.1692305, -3.3464472],\n       [-3.6664698,  3.8386683]], dtype=float32)&gt;, hidden_states=None, attentions=None)\n\n\nWhat did we get here??\nWe got logits which are nothing but the raw scores of the final layer. These are not probabilities. All we need to do is to add a softmax layer to get the probabilities of each label. Which can done by using tf.math.softmax\n\n\n2.3. Post Processing\n\nlogits = bert_model(tokenized_ids).logits\n\nimport tensorflow as tf\n\nprint(tf.math.softmax(logits).numpy())\n\n[[4.0195242e-02 9.5980471e-01]\n [9.9945587e-01 5.4418476e-04]\n [5.4994709e-04 9.9945003e-01]]\n\n\nNow we have extracted probabilities. Now, we can use argmax function to get the index of the maximum value of the probability.\n\ntf.argmax(tf.math.softmax(logits),axis=1).numpy()\n\narray([1, 0, 1])\n\n\n\n1 means positive\n0 means\n\nIn this way we have explored three parts of the pipeline individually."
  },
  {
    "objectID": "posts/hugging_face_sentiment_classification/index.html#whats-happening-inside-tokenizer",
    "href": "posts/hugging_face_sentiment_classification/index.html#whats-happening-inside-tokenizer",
    "title": "Hugging Face - Sentiment Classification",
    "section": "3. What’s happening inside Tokenizer",
    "text": "3. What’s happening inside Tokenizer\nBelow is we get when we use default tokenizer. Where we get the input_ids and attention_mask by default. Now, we will explore how this is formed.\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ntokenizer(\"I hate this so much!\")\n\n{'input_ids': [101, 1045, 5223, 2023, 2061, 2172, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\n3.1 Tokenization\nUsually there are three types of tokenization.\n\nWord based tokenization\nCharacter level tokenization\n\nBoth of the above are two extreme states of tokenization. The usual approach that is followed in large models are sub-word tokenization\nWe can tokenize the words by using tokenizer.tokenize function\n\ntokenizer.tokenize(\"I hate this so much!\")\n\n['i', 'hate', 'this', 'so', 'much', '!']\n\n\n\ntokenizer.tokenize(\"This beautilization is so lucky!\")  #Random Sentence to demonstrate automatic subword tokenization\n\n['this', 'beau', '##ti', '##lization', 'is', 'so', 'lucky', '!']\n\n\n\n\n3.2. Tokenized Words to IDs\nTo convert these tokens to words we need to map using the vocabulary which is trained through this model.\nEarlier using the config file we have explored that the vocabulary is of size 30K. Which can be further verified by using the len function and the we can see the first 10 vocabulary words along with ids.\n\nprint(len(tokenizer.vocab))\nlist(tokenizer.vocab.items())[:10]\n\n30522\n\n\n[('96', 5986),\n ('republished', 24476),\n ('worst', 5409),\n ('##bant', 29604),\n ('##ahu', 21463),\n ('fellow', 3507),\n ('explosives', 14792),\n ('infrared', 14611),\n ('##osaurus', 25767),\n ('tenant', 16713)]\n\n\n\ntokenizer.tokenize(\"I hate this so much!\")\n\n['i', 'hate', 'this', 'so', 'much', '!']\n\n\nWe can convert these into tokens by mapping the vocabulury key values\n\nfor i in tokenizer.tokenize(\"I hate this so much!\"):\n  print(tokenizer.vocab[i])\n\nprint(\"\")\n\ntokenizer(\"I hate this so much!\")[\"input_ids\"]\n\n1045\n5223\n2023\n2061\n2172\n999\n\n\n\n[101, 1045, 5223, 2023, 2061, 2172, 999, 102]\n\n\nNow, we can see that the we are able to map the tokens. However, there are some special tokens at start and at end which are labelled as 101 and 102. Lets explore what are they\n\nprint(dict(tokenizer.vocab)[\"[CLS]\"])\nprint(dict(tokenizer.vocab)[\"[SEP]\"])\n\n101\n102\n\n\nThese are [CLS] indicating the starting of the sequence and [SEP] indicates a seperator here it is end of sentence as it is Sentiment Classification task. We can convert them directly to ids using convert_token_to_ids function\n\ntokens = tokenizer.tokenize(\"I hate this so much!\")\n\ntokenizer.convert_tokens_to_ids(tokens)\n\n[1045, 5223, 2023, 2061, 2172, 999]\n\n\n\n\n\n3.3. Preparing tokens for downstream modelling\nWe need to prepare these tokens for downstream modelling by using function prepare_for_model which creates both input_ids and attention_mask\n\ntoken_to_ids = tokenizer.convert_tokens_to_ids(tokens)\n\ntokenizer.prepare_for_model(token_to_ids)\n\nYou're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n\n\n{'input_ids': [101, 1045, 5223, 2023, 2061, 2172, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
  },
  {
    "objectID": "posts/hugging_face_sentiment_classification/index.html#how-fine-tuning-works",
    "href": "posts/hugging_face_sentiment_classification/index.html#how-fine-tuning-works",
    "title": "Hugging Face - Sentiment Classification",
    "section": "4. How Fine Tuning Works!",
    "text": "4. How Fine Tuning Works!\nLet’s assume that we want to fine-tune the data to our dataset at hand.\nLets load the dataset of Amazon Review which we explored in our previous blog.\n\nfrom google.colab import drive\nimport os\n\nimport pandas as pd\n\ndrive.mount('/content/drive')\nos.chdir('/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/')\n\ntestdata = pd.read_csv('test_data_sample_complete.csv')\ntraindata = pd.read_csv('train_data_sample_complete.csv')\n\nMounted at /content/drive\n\n\nPreparing the dataset and taking a sample of 1000 rows of both training and test dataset.\n\ntrain_data = traindata.sample(n=1000, random_state=42)\ntest_data = testdata.sample(n=1000, random_state=42)\n\ntrain_data['class_index'] = train_data['class_index'].map({1:0, 2:1})\ntest_data['class_index'] = test_data['class_index'].map({1:0, 2:1})\n\ntrain_data['review_combined_lemma'] = train_data['review_combined_lemma'].fillna('')\ntest_data['review_combined_lemma'] = test_data['review_combined_lemma'].fillna('')\n\n\ntrain_data.head(3)\n\n\n  \n    \n\n\n\n\n\n\nclass_index\nreview_combined_lemma\n\n\n\n\n2079998\n0\nexpensive junk product consists piece thin fle...\n\n\n1443106\n0\ntoast dark even lowest setting toast dark liki...\n\n\n3463669\n1\nexcellent imagerydumbed story enjoyed disc vid...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\n4.1. Converting Pandas DataFrame to HuggingFace Dataset\nFor finetuning the hugging face model we need to convert the data frame into hugging face format which is pretty straight forward and easy\n\n!pip install datasets\n\nCollecting datasets\n  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 542.0/542.0 kB 7.3 MB/s eta 0:00:00\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\nRequirement already satisfied: pyarrow&gt;=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\nCollecting dill&lt;0.3.9,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 10.1 MB/s eta 0:00:00\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 9.6 MB/s eta 0:00:00\nCollecting multiprocess (from datasets)\n  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 14.0 MB/s eta 0:00:00\nRequirement already satisfied: fsspec[http]&lt;=2024.3.1,&gt;=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub&gt;=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (23.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.4.1)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (6.0.5)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (1.9.4)\nRequirement already satisfied: async-timeout&lt;5.0,&gt;=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp-&gt;datasets) (4.0.3)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub&gt;=0.21.2-&gt;datasets) (4.11.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2023.4)\nRequirement already satisfied: tzdata&gt;=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas-&gt;datasets) (2024.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.16.0)\nInstalling collected packages: xxhash, dill, multiprocess, datasets\nSuccessfully installed datasets-2.19.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n\n\n\nimport datasets\nfrom datasets import Dataset, DatasetDict\n\ntrain_data = Dataset.from_pandas(train_data)\ntest_data = Dataset.from_pandas(test_data)\n\nraw_data = DatasetDict()\n\nraw_data[\"train\"] =  train_data\nraw_data[\"test\"] = test_data\n\nprint(raw_data)\n\nDatasetDict({\n    train: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__'],\n        num_rows: 1000\n    })\n})\n\n\nTo these datasets we need to tokenize the input data using our tokenizer which is done in the following way\n\ndef tokenize_function(example):\n    return tokenizer(example[\"review_combined_lemma\"], truncation=True)\n\ntokenized_datasets = raw_data.map(tokenize_function, batched=True)\n\nprint(tokenized_datasets)\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n    test: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})\n\n\n\nDataset.to_pandas(tokenized_datasets[\"train\"]).head(3)\n\n\n  \n    \n\n\n\n\n\n\nclass_index\nreview_combined_lemma\n__index_level_0__\ninput_ids\nattention_mask\n\n\n\n\n0\n0\nexpensive junk product consists piece thin fle...\n2079998\n[101, 6450, 18015, 4031, 3774, 3538, 4857, 123...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n\n\n1\n0\ntoast dark even lowest setting toast dark liki...\n1443106\n[101, 15174, 2601, 2130, 7290, 4292, 15174, 26...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n\n\n2\n1\nexcellent imagerydumbed story enjoyed disc vid...\n3463669\n[101, 6581, 13425, 8566, 18552, 2094, 2466, 56...\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nFinally, we have prepared the dataset in the required format and tokenized the input data.\nDo note, we have not padding the input data. So, we lenghts of input_ids will be different based on the sentence length.\n\nprint(len(Dataset.to_pandas(tokenized_datasets[\"train\"])[\"attention_mask\"][0]))\nprint(len(Dataset.to_pandas(tokenized_datasets[\"train\"])[\"attention_mask\"][1]))\nprint(len(Dataset.to_pandas(tokenized_datasets[\"train\"])[\"review_combined_lemma\"][0]))\nprint(len(Dataset.to_pandas(tokenized_datasets[\"train\"])[\"review_combined_lemma\"][1]))\n\n81\n27\n422\n132\n\n\nWe need to use the DataCollatorWith Padding and finally convert this into individualized tensorflow datasets for faster processing.\n\nfrom transformers import DataCollatorWithPadding\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n\ntf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"class_index\"],\n    shuffle=True,\n    collate_fn=data_collator,\n    batch_size=8,\n)\n\ntf_validation_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n    columns=[\"attention_mask\", \"input_ids\"],\n    label_cols=[\"class_index\"],\n    shuffle=False,\n    collate_fn=data_collator,\n    batch_size=8,\n)\n\n/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:410: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\nOld behaviour: columns=['a'], labels=['labels'] -&gt; (tf.Tensor, tf.Tensor)  \n             : columns='a', labels='labels' -&gt; (tf.Tensor, tf.Tensor)  \nNew behaviour: columns=['a'],labels=['labels'] -&gt; ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n             : columns='a', labels='labels' -&gt; (tf.Tensor, tf.Tensor) \n  warnings.warn(\n\n\n\n\n4.2. Fine Tuning the Model\nWe have already loaded the bert_model in our previous steps which we will use for training.\n\nbert_model.summary()\n\nModel: \"tf_distil_bert_for_sequence_classification_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n pre_classifier (Dense)      multiple                  590592    \n                                                                 \n classifier (Dense)          multiple                  1538      \n                                                                 \n dropout_39 (Dropout)        multiple                  0         \n                                                                 \n=================================================================\nTotal params: 66955010 (255.41 MB)\nTrainable params: 66955010 (255.41 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nOnce we have this model we need to change the loss to logits as the model output logits which needs to be further converted into probabilities.\ncompile and fit is simlar to that of regular tensorflow operations\n\nbert_model.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\nbert_model.fit(\n    tf_train_dataset,\n    validation_data=tf_validation_dataset,\n    verbose=1,\n    epochs=10\n)\n\nEpoch 1/10\n\n\nWARNING:tensorflow:AutoGraph could not transform &lt;function infer_framework at 0x7fde78705750&gt; and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n\n\nWARNING: AutoGraph could not transform &lt;function infer_framework at 0x7fde78705750&gt; and will run it as-is.\nCause: for/else statement not yet supported\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n125/125 [==============================] - 92s 297ms/step - loss: 0.7160 - accuracy: 0.5250 - val_loss: 0.6932 - val_accuracy: 0.5040\nEpoch 2/10\n125/125 [==============================] - 21s 167ms/step - loss: 0.7053 - accuracy: 0.4940 - val_loss: 0.6956 - val_accuracy: 0.5040\nEpoch 3/10\n125/125 [==============================] - 18s 144ms/step - loss: 1.0466 - accuracy: 0.4770 - val_loss: 0.6949 - val_accuracy: 0.4960\nEpoch 4/10\n125/125 [==============================] - 18s 148ms/step - loss: 0.6946 - accuracy: 0.5030 - val_loss: 0.6934 - val_accuracy: 0.4960\nEpoch 5/10\n125/125 [==============================] - 18s 147ms/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6933 - val_accuracy: 0.4960\nEpoch 6/10\n125/125 [==============================] - 16s 129ms/step - loss: 0.6934 - accuracy: 0.4910 - val_loss: 0.6932 - val_accuracy: 0.4960\nEpoch 7/10\n125/125 [==============================] - 16s 127ms/step - loss: 0.6933 - accuracy: 0.4990 - val_loss: 0.6932 - val_accuracy: 0.4960\nEpoch 8/10\n125/125 [==============================] - 17s 138ms/step - loss: 0.6933 - accuracy: 0.4830 - val_loss: 0.6932 - val_accuracy: 0.4960\nEpoch 9/10\n125/125 [==============================] - 16s 132ms/step - loss: 0.6933 - accuracy: 0.4720 - val_loss: 0.6931 - val_accuracy: 0.4960\nEpoch 10/10\n125/125 [==============================] - 20s 162ms/step - loss: 0.6933 - accuracy: 0.4770 - val_loss: 0.6931 - val_accuracy: 0.5040\n\n\n&lt;tf_keras.src.callbacks.History at 0x7fdd8571d810&gt;\n\n\nCurrently the accuracy is 50% because we have use small data and as well smaller epochs. It would definetly increase the accuracy if we can on full data which we will explore in next blog. However, this comes with a huge cost of training as fine-tuning the full model will take time as it needs to retain lot of paramters.\nSince, majority of the paramters lie in distilbert layer; Let’s not train this layer. Which can be adjusted by making that layer trainable as False\n\nbert_new_model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n## Reloading the original model such that the original paramters are restored.\n\nbert_new_model.layers[0].trainable = False\n\nbert_new_model.summary()\n\nAll PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n\nAll the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n\n\nModel: \"tf_distil_bert_for_sequence_classification_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n pre_classifier (Dense)      multiple                  590592    \n                                                                 \n classifier (Dense)          multiple                  1538      \n                                                                 \n dropout_59 (Dropout)        multiple                  0 (unused)\n                                                                 \n=================================================================\nTotal params: 66955010 (255.41 MB)\nTrainable params: 592130 (2.26 MB)\nNon-trainable params: 66362880 (253.15 MB)\n_________________________________________________________________\n\n\nNow, we can see that the Trainable Paramters are now 5.9 Lacs\n\nbert_new_model.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\nbert_new_model.fit(\n    tf_train_dataset,\n    validation_data=tf_validation_dataset,\n    verbose=1,\n    epochs=10\n)\n\nEpoch 1/10\n125/125 [==============================] - 34s 164ms/step - loss: 0.4001 - accuracy: 0.8290 - val_loss: 0.3727 - val_accuracy: 0.8600\nEpoch 2/10\n125/125 [==============================] - 11s 84ms/step - loss: 0.3654 - accuracy: 0.8440 - val_loss: 0.3842 - val_accuracy: 0.8390\nEpoch 3/10\n125/125 [==============================] - 11s 91ms/step - loss: 0.3495 - accuracy: 0.8390 - val_loss: 0.3832 - val_accuracy: 0.8580\nEpoch 4/10\n125/125 [==============================] - 9s 76ms/step - loss: 0.3416 - accuracy: 0.8490 - val_loss: 0.3533 - val_accuracy: 0.8590\nEpoch 5/10\n125/125 [==============================] - 9s 75ms/step - loss: 0.3180 - accuracy: 0.8570 - val_loss: 0.3428 - val_accuracy: 0.8530\nEpoch 6/10\n125/125 [==============================] - 8s 66ms/step - loss: 0.3136 - accuracy: 0.8660 - val_loss: 0.3652 - val_accuracy: 0.8530\nEpoch 7/10\n125/125 [==============================] - 9s 70ms/step - loss: 0.3091 - accuracy: 0.8660 - val_loss: 0.4172 - val_accuracy: 0.8580\nEpoch 8/10\n125/125 [==============================] - 9s 76ms/step - loss: 0.3076 - accuracy: 0.8580 - val_loss: 0.3496 - val_accuracy: 0.8520\nEpoch 9/10\n125/125 [==============================] - 9s 76ms/step - loss: 0.3013 - accuracy: 0.8690 - val_loss: 0.3473 - val_accuracy: 0.8580\nEpoch 10/10\n125/125 [==============================] - 9s 68ms/step - loss: 0.2840 - accuracy: 0.8910 - val_loss: 0.3472 - val_accuracy: 0.8530\n\n\n&lt;tf_keras.src.callbacks.History at 0x7fdd7e938190&gt;\n\n\nSufficiently large data and with large epocs should increase validation accuracy which we will explore in next blog"
  },
  {
    "objectID": "posts/hugging_face_sentiment_classification/index.html#adding-custom-layers-on-top-of-hugging-face-model",
    "href": "posts/hugging_face_sentiment_classification/index.html#adding-custom-layers-on-top-of-hugging-face-model",
    "title": "Hugging Face - Sentiment Classification",
    "section": "5. Adding Custom Layers on Top of Hugging Face Model",
    "text": "5. Adding Custom Layers on Top of Hugging Face Model\nIn the earlier model since there is already a classifier built-in it is not wise to add custom layers on top of it. Hugging face provides a default AutoModel without a classifier for such scenarios for us.\n\nfrom transformers import TFAutoModel\n\nbert_for_custom_model = TFAutoModel.from_pretrained(checkpoint)\n\nbert_for_custom_model.summary()\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n\n\nModel: \"tf_distil_bert_model_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n=================================================================\nTotal params: 66362880 (253.15 MB)\nTrainable params: 66362880 (253.15 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nNow, we can see that we have got the same model without the classifer layers which we can add on top of this.\n\n5.1. Sample Exploration of the layer\n\ninputs = tokenizer(\"This is a sample input\", return_tensors=\"tf\")\nbert_for_custom_model(inputs)\n\nTFBaseModelOutput(last_hidden_state=&lt;tf.Tensor: shape=(1, 7, 768), dtype=float32, numpy=\narray([[[ 0.05297906,  0.10692392,  0.49419168, ...,  0.12507138,\n          0.23212025,  0.17222881],\n        [-0.01472363, -0.11656911,  0.28193325, ...,  0.12857884,\n          0.30744654,  0.17272332],\n        [ 0.01157056, -0.07407673,  0.54636   , ...,  0.03764485,\n          0.2051253 ,  0.49154142],\n        ...,\n        [ 0.32394847,  0.08095304,  0.4270281 , ...,  0.08437095,\n          0.18978027, -0.0091958 ],\n        [-0.15465298,  0.17761149,  0.5083473 , ...,  0.19376224,\n          0.36129084, -0.14435732],\n        [ 1.0194045 ,  0.17841692,  0.48938137, ...,  0.79112005,\n         -0.12476905, -0.09671681]]], dtype=float32)&gt;, hidden_states=None, attentions=None)\n\n\nHere we can see that the output is named as last_hidden_state which has a shape of (1,7,768)\n7 represents the number of input_ids.\n768 is the dimension of the model which can be seen in the config part.\n\nbert_for_custom_model.config\n\nDistilBertConfig {\n  \"_name_or_path\": \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n  \"activation\": \"gelu\",\n  \"architectures\": [\n    \"DistilBertForSequenceClassification\"\n  ],\n  \"attention_dropout\": 0.1,\n  \"dim\": 768,\n  \"dropout\": 0.1,\n  \"finetuning_task\": \"sst-2\",\n  \"hidden_dim\": 3072,\n  \"id2label\": {\n    \"0\": \"NEGATIVE\",\n    \"1\": \"POSITIVE\"\n  },\n  \"initializer_range\": 0.02,\n  \"label2id\": {\n    \"NEGATIVE\": 0,\n    \"POSITIVE\": 1\n  },\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"distilbert\",\n  \"n_heads\": 12,\n  \"n_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"qa_dropout\": 0.1,\n  \"seq_classif_dropout\": 0.2,\n  \"sinusoidal_pos_embds\": false,\n  \"tie_weights_\": true,\n  \"transformers_version\": \"4.41.0\",\n  \"vocab_size\": 30522\n}\n\n\n\n\n5.2. Adding Custom Layers\n\nfrom tensorflow.keras.layers import Layer\n\nclass HuggingFaceModelLayer(Layer):\n    def __init__(self, model, **kwargs):\n        super(HuggingFaceModelLayer, self).__init__(**kwargs)\n        self.model = model\n\n    def call(self, inputs):\n        input_ids, attention_mask = inputs\n        return self.model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n\ninput_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name='attention_mask')\n\nlast_hidden_state = HuggingFaceModelLayer(bert_for_custom_model)([input_ids, attention_mask])\n\n# Taking the first CLS token which usually captures the overall information\ncls_token_state = last_hidden_state[:, 0, :]\n\n# Add custom layers\nx = tf.keras.layers.Dense(256, activation='relu')(cls_token_state)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\nx = tf.keras.layers.Dense(64, activation='relu')(x)\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # For binary classification, use 'sigmoid'\n\n# Create the final model\ncustom_model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n\ncustom_model.summary()\n\nModel: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, None)]               0         []                            \n                                                                                                  \n attention_mask (InputLayer  [(None, None)]               0         []                            \n )                                                                                                \n                                                                                                  \n hugging_face_model_layer_5  (None, None, 768)            6636288   ['input_ids[0][0]',           \n  (HuggingFaceModelLayer)                                 0          'attention_mask[0][0]']      \n                                                                                                  \n tf.__operators__.getitem_1  (None, 768)                  0         ['hugging_face_model_layer_5[0\n 2 (SlicingOpLambda)                                                ][0]']                        \n                                                                                                  \n dense_16 (Dense)            (None, 256)                  196864    ['tf.__operators__.getitem_12[\n                                                                    0][0]']                       \n                                                                                                  \n dense_17 (Dense)            (None, 128)                  32896     ['dense_16[0][0]']            \n                                                                                                  \n dense_18 (Dense)            (None, 64)                   8256      ['dense_17[0][0]']            \n                                                                                                  \n dense_19 (Dense)            (None, 1)                    65        ['dense_18[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 66600961 (254.06 MB)\nTrainable params: 66600961 (254.06 MB)\nNon-trainable params: 0 (0.00 Byte)\n__________________________________________________________________________________________________\n\n\nHere we have successfuly added layers on top of the Hugging Face Model.\n\n\n5.3. Fine Tuning the Model\nFor this blog sake. We will keep the Distilbert layer as non-trainable.\n\n\ncustom_model.layers[2].trainable = False\n\ncustom_model.summary()\n\nModel: \"model_4\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n input_ids (InputLayer)      [(None, None)]               0         []                            \n                                                                                                  \n attention_mask (InputLayer  [(None, None)]               0         []                            \n )                                                                                                \n                                                                                                  \n hugging_face_model_layer_5  (None, None, 768)            6636288   ['input_ids[0][0]',           \n  (HuggingFaceModelLayer)                                 0          'attention_mask[0][0]']      \n                                                                                                  \n tf.__operators__.getitem_1  (None, 768)                  0         ['hugging_face_model_layer_5[0\n 2 (SlicingOpLambda)                                                ][0]']                        \n                                                                                                  \n dense_16 (Dense)            (None, 256)                  196864    ['tf.__operators__.getitem_12[\n                                                                    0][0]']                       \n                                                                                                  \n dense_17 (Dense)            (None, 128)                  32896     ['dense_16[0][0]']            \n                                                                                                  \n dense_18 (Dense)            (None, 64)                   8256      ['dense_17[0][0]']            \n                                                                                                  \n dense_19 (Dense)            (None, 1)                    65        ['dense_18[0][0]']            \n                                                                                                  \n==================================================================================================\nTotal params: 66600961 (254.06 MB)\nTrainable params: 238081 (930.00 KB)\nNon-trainable params: 66362880 (253.15 MB)\n__________________________________________________________________________________________________\n\n\nWhile using earlier we used entropy loss with logits; Now, we can use binary_cross entropy directly. Since we have added our own classifer custom layers.\n\ncustom_model.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"],\n)\ncustom_model.fit(\n    tf_train_dataset,\n    validation_data=tf_validation_dataset,\n    verbose=1,\n    epochs=10\n)\n\nEpoch 1/10\n125/125 [==============================] - 84s 176ms/step - loss: 0.4009 - accuracy: 0.8310 - val_loss: 0.3699 - val_accuracy: 0.8440\nEpoch 2/10\n125/125 [==============================] - 11s 85ms/step - loss: 0.3430 - accuracy: 0.8480 - val_loss: 0.3606 - val_accuracy: 0.8410\nEpoch 3/10\n125/125 [==============================] - 11s 86ms/step - loss: 0.3390 - accuracy: 0.8430 - val_loss: 0.3617 - val_accuracy: 0.8520\nEpoch 4/10\n125/125 [==============================] - 11s 87ms/step - loss: 0.3144 - accuracy: 0.8670 - val_loss: 0.3468 - val_accuracy: 0.8570\nEpoch 5/10\n125/125 [==============================] - 9s 70ms/step - loss: 0.3013 - accuracy: 0.8650 - val_loss: 0.3452 - val_accuracy: 0.8420\nEpoch 6/10\n125/125 [==============================] - 8s 66ms/step - loss: 0.2875 - accuracy: 0.8790 - val_loss: 0.3706 - val_accuracy: 0.8410\nEpoch 7/10\n125/125 [==============================] - 9s 70ms/step - loss: 0.2686 - accuracy: 0.8910 - val_loss: 0.4193 - val_accuracy: 0.8460\nEpoch 8/10\n125/125 [==============================] - 12s 98ms/step - loss: 0.2655 - accuracy: 0.8900 - val_loss: 0.4044 - val_accuracy: 0.8500\nEpoch 9/10\n125/125 [==============================] - 10s 80ms/step - loss: 0.2356 - accuracy: 0.9020 - val_loss: 0.3540 - val_accuracy: 0.8540\nEpoch 10/10\n125/125 [==============================] - 8s 66ms/step - loss: 0.2549 - accuracy: 0.9080 - val_loss: 0.4120 - val_accuracy: 0.8530\n\n\n&lt;keras.src.callbacks.History at 0x7fdd0b8c0cd0&gt;\n\n\nFinally we successfully trained the custom model. Since we are training on baby dataset the validation accuracy hasn’t improved. Will explore to train on larger datasets in next blog and will share the results.\nOverall, this is a good learning on the fundementals of the Hugging Face module usage. Please do comment on further explanation where i could not provide in the above."
  },
  {
    "objectID": "posts/tensor_math_operations/index.html",
    "href": "posts/tensor_math_operations/index.html",
    "title": "Basic Tensor Math Operations",
    "section": "",
    "text": "This blog, I just a basic commparison of math operations between numpy and tensor variables.\nThis is a good learning for me as almost all commands are similar to that of numpy."
  },
  {
    "objectID": "posts/tensor_math_operations/index.html#loading-libraries",
    "href": "posts/tensor_math_operations/index.html#loading-libraries",
    "title": "Basic Tensor Math Operations",
    "section": "0. Loading Libraries",
    "text": "0. Loading Libraries\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.__version__\n\n'2.15.0'\n\n\n\ntf.config.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
  },
  {
    "objectID": "posts/tensor_math_operations/index.html#defining-variables",
    "href": "posts/tensor_math_operations/index.html#defining-variables",
    "title": "Basic Tensor Math Operations",
    "section": "1. Defining variables",
    "text": "1. Defining variables\n\nvar = tf.constant([[30,40,50],[45,65,70]])\n\nvar1 = np.array([[30,40,50],[45,65,70]])\n\nprint(var)\nprint(\" \")\nprint(var1)\n\ntf.Tensor(\n[[30 40 50]\n [45 65 70]], shape=(2, 3), dtype=int32)\n \n[[30 40 50]\n [45 65 70]]"
  },
  {
    "objectID": "posts/tensor_math_operations/index.html#math-operations",
    "href": "posts/tensor_math_operations/index.html#math-operations",
    "title": "Basic Tensor Math Operations",
    "section": "2. Math Operations",
    "text": "2. Math Operations\n\n2.1. Absolute\n\nprint(var * -1 )\nprint(\" \")\nprint(var1 * -1 )\n\ntf.Tensor(\n[[-30 -40 -50]\n [-45 -65 -70]], shape=(2, 3), dtype=int32)\n \n[[-30 -40 -50]\n [-45 -65 -70]]\n\n\n\nprint(tf.math.abs(var * -1))\nprint(\"\")\nprint(np.abs(var1 * -1))\n\ntf.Tensor(\n[[30 40 50]\n [45 65 70]], shape=(2, 3), dtype=int32)\n\n[[30 40 50]\n [45 65 70]]\n\n\nHere we can see both the outputs and commands are very similar to that of numpy\n\n\n2.2. Argmax/Argmin\n\nprint(tf.math.argmax(var))\nprint(\"\")\nprint(np.argmax(var1))\n\ntf.Tensor([1 1 1], shape=(3,), dtype=int64)\n\n5\n\n\nThis is the only difference we can see where tensorflow by default considers as axis=0 while numpy takes all numbers\n\nprint(tf.math.argmax(var,axis=0))\nprint(\"\")\nprint(np.argmax(var1,axis=0))\n\ntf.Tensor([1 1 1], shape=(3,), dtype=int64)\n\n[1 1 1]\n\n\n\nprint(tf.math.argmax(var,axis=1))\nprint(\"\")\nprint(np.argmax(var1,axis=1))\n\ntf.Tensor([2 2], shape=(2,), dtype=int64)\n\n[2 2]\n\n\n\n\n2.3. Maximum or Minumum\n\nprint(tf.math.reduce_max(var))\nprint(\"\")\nprint(np.max(var1))\n\ntf.Tensor(70, shape=(), dtype=int32)\n\n70\n\n\n\nprint(tf.math.reduce_max(var,axis = 0))\nprint(\"\")\nprint(np.max(var1,axis = 0))\n\ntf.Tensor([45 65 70], shape=(3,), dtype=int32)\n\n[45 65 70]\n\n\n\nprint(tf.math.reduce_max(var,axis = 1))\nprint(\"\")\nprint(np.max(var1,axis = 1))\n\ntf.Tensor([50 70], shape=(2,), dtype=int32)\n\n[50 70]\n\n\n\n\n2.4. Sum\n\nprint(tf.math.reduce_sum(var))\nprint(\"\")\nprint(np.sum(var1))\n\ntf.Tensor(300, shape=(), dtype=int32)\n\n300\n\n\n\nprint(tf.math.reduce_sum(var,axis=0))\nprint(\"\")\nprint(np.sum(var1,axis=0))\n\ntf.Tensor([ 75 105 120], shape=(3,), dtype=int32)\n\n[ 75 105 120]\n\n\n\n\n2.5. Power\n\nprint(tf.math.pow(var,2))\nprint(\"\")\nprint(np.power(var1,2))\nprint(\"\")\nprint(var ** 2)\nprint(\"\")\nprint(var1 ** 2)\n\ntf.Tensor(\n[[ 900 1600 2500]\n [2025 4225 4900]], shape=(2, 3), dtype=int32)\n\n[[ 900 1600 2500]\n [2025 4225 4900]]\n\ntf.Tensor(\n[[ 900 1600 2500]\n [2025 4225 4900]], shape=(2, 3), dtype=int32)\n\n[[ 900 1600 2500]\n [2025 4225 4900]]\n\n\n\n\n2.6. Log\n\nvar = tf.cast(var, tf.float32)\n\nprint(tf.math.log(var))\nprint(\"\")\nprint(np.log(var1))\n\ntf.Tensor(\n[[3.4011974 3.6888795 3.912023 ]\n [3.8066626 4.1743875 4.248495 ]], shape=(2, 3), dtype=float32)\n\n[[3.40119738 3.68887945 3.91202301]\n [3.80666249 4.17438727 4.24849524]]\n\n\n\n\n2.7. Exponential\n\nvar = tf.cast(var, tf.float32)\n\nprint(tf.math.exp(var))\nprint(\"\")\nprint(np.exp(var1))\n\ntf.Tensor(\n[[1.0686474e+13 2.3538525e+17 5.1847055e+21]\n [3.4934271e+19 1.6948892e+28 2.5154387e+30]], shape=(2, 3), dtype=float32)\n\n[[1.06864746e+13 2.35385267e+17 5.18470553e+21]\n [3.49342711e+19 1.69488924e+28 2.51543867e+30]]\n\n\n\n\n2.8. Squareroot\n\nprint(tf.math.sqrt(var))\nprint(\"\")\nprint(np.sqrt(var1))\n\ntf.Tensor(\n[[5.4772253 6.3245554 7.071068 ]\n [6.708204  8.062258  8.3666   ]], shape=(2, 3), dtype=float32)\n\n[[5.47722558 6.32455532 7.07106781]\n [6.70820393 8.06225775 8.36660027]]"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html",
    "href": "posts/deep-learning-project-msis/index.html",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "",
    "text": "This article explores the use of deep learning models, such as feed forward neural networks (FFNN) and recurrent neural networks (RNN), such Bidirectional LSTM (BiLSTM) for sentiment analysis. This is one of our first project of deep learning where-in we have took this opportunity to build our basics strongly.\nFor the purpose of analyzing sentiment trends over an extended period, we utilize a substantial dataset consisting of 3 Million Amazon product reviews. This data, sourced from the Stanford Network Analysis Project (SNAP), spans 18 years, providing a rich longitudinal view of consumer sentiments. However, for our modeling we could use only 0.1 Million of the data considering our system constraints.\n\n\n\n\n\nEach review includes a numeric score representing the sentiment polarity. Negative review is represented as class 1 while positive review is represented with class 2. This serves as a foundational metric for sentiment analysis"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#introduction",
    "href": "posts/deep-learning-project-msis/index.html#introduction",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "",
    "text": "This article explores the use of deep learning models, such as feed forward neural networks (FFNN) and recurrent neural networks (RNN), such Bidirectional LSTM (BiLSTM) for sentiment analysis. This is one of our first project of deep learning where-in we have took this opportunity to build our basics strongly.\nFor the purpose of analyzing sentiment trends over an extended period, we utilize a substantial dataset consisting of 3 Million Amazon product reviews. This data, sourced from the Stanford Network Analysis Project (SNAP), spans 18 years, providing a rich longitudinal view of consumer sentiments. However, for our modeling we could use only 0.1 Million of the data considering our system constraints.\n\n\n\n\n\nEach review includes a numeric score representing the sentiment polarity. Negative review is represented as class 1 while positive review is represented with class 2. This serves as a foundational metric for sentiment analysis"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#data-preprocessing",
    "href": "posts/deep-learning-project-msis/index.html#data-preprocessing",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nIn the preprocessing pipeline for sentiment analysis, the following steps were performed using nltk library : Punctuation Removal , Tokenization, Stopword Elimination and Lemmatization. Notably, the stopword list has been specifically curated to retain negations such as “not,” “no,” and other negatory contractions.\n\nstop_words = set(stopwords.words('english')) - { 'not', 'no', 'couldn', \"couldn't\", \"wouldn't\", \"shouldn't\", \"isn't\",\n                                                \"aren't\", \"wasn't\", \"weren't\", \"don't\", \"doesn't\", \"hadn't\", \"hasn't\",\n                                                 \"won't\", \"can't\", \"mightn't\",\"needn't\",\"nor\",\"shouldn\",\"should've\",\"should\",\n                                                 \"weren\",\"wouldn\",\"mustn't\",\"mustn\",\"didn't\",\"didn\",\"doesn\",\"did\",\"does\",\"hadn\",\n                                                 \"hasn\",\"haven't\",\"haven\",\"needn\",\"shan't\"}\n\ndef preprocess(text):\n    # Convert text to lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    # Tokenize text into words\n    words = word_tokenize(text)\n    # Remove stopwords\n    words = [word for word in words if word not in stop_words]\n    # Lemmatize words\n    lemmatizer = WordNetLemmatizer()\n    words = [lemmatizer.lemmatize(word) for word in words]\n    # Join the words back into a single string\n    text = ' '.join(words)\n    return text"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#data-visualization",
    "href": "posts/deep-learning-project-msis/index.html#data-visualization",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Data Visualization",
    "text": "Data Visualization\n \nThis word cloud is characterized by a significant presence of highly positive terms such as “love,” “great,” “best,” “perfect,” and “excellent.” These words indicate strong satisfaction and enjoyment, commonly found in reviews that endorse a product. The words “highly recommend,” “amazing,” and “favorite” suggest that positive reviews often include recommendations and personal favoritism towards the products. The presence of words like “beautiful” and “enjoy” also emphasizes an emotional connection with the product.\nThe negative word cloud features words such as “disappoint,” “waste,” “poor,” “bad,” and “problem.” These strongly negative terms are indicative of dissatisfaction and issues with the products. Terms like “return” and “refund” suggest actions taken by dissatisfied customers. Words like “boring,” “dull,” and “worst” reflect critical opinions about the product’s quality or entertainment value."
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#traditional-machine-learning",
    "href": "posts/deep-learning-project-msis/index.html#traditional-machine-learning",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Traditional Machine Learning",
    "text": "Traditional Machine Learning\nAfter conducting a thorough evaluation, we concluded that the Random Forest model outperformed SVM in multiple metrics, making it the preferred baseline model. This initial selection lays the foundation for further exploration and refinement of sentiment analysis techniques.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nHyper Parameters\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\nRandom Forest with Count Vectorizer\nEstimators : 200, Max Depth:20, Min Samples Split : 2\n82%\n79%\n79%\n\n\nRandom Forest with TF-IDF\nEstimators : 200, Max Depth:20, Min Samples Split : 5\n86%\n83%\n84%"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#neural-networks",
    "href": "posts/deep-learning-project-msis/index.html#neural-networks",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Neural Networks",
    "text": "Neural Networks\nIn our pursuit of advancing practical expertise in deep learning applications, we executed the following steps in a phased manner within the Neural Networks framework:\n\nPreprocessing in neural networks\n\n# Initialize a tokenizer with an out-of-vocabulary token\ntokenizer = Tokenizer(oov_token=\"&lt;UNK&gt;\")\n\n# Fit the tokenizer on the training data to build the vocabulary\ntokenizer.fit_on_texts(X_train)\n\n# Add a special padding token to the word index with index 0\ntokenizer.word_index['&lt;PAD&gt;'] = 0\n\n# Convert the text data into sequences of token indices using the trained tokenizer\nX_sequences_train = tokenizer.texts_to_sequences(X_train)\nX_sequences = tokenizer.texts_to_sequences(X)\n\n# Pad the sequences to ensure uniform length\n# maxlen is set to 100, meaning sequences longer than 100 tokens will be truncated, and shorter sequences will be padded\nX_train = pad_sequences(X_sequences_train, maxlen=100)\nX = pad_sequences(X_sequences, maxlen=100)\n\n\n\nFeed Forward Network\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFeed Forward Network\n10,000 to 3.6 million\n51%\n51%\n51%\nPoor model\n\n\n\nThe reason for its poor performance is likely due to the fact that the input words are represented as numerical numbers. In traditional machine learning, the representation of TF-IDF has shown better results. Therefore, the lesson learned is that we need to convert the input words into a better representation, such as one-hot encoding.\n\n\nOne Hot Encoding\nNow, we have successfully converted the input data into one-hot vectors and we see that the number of parameters to be learned is also huge.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFeed Forward Network with one hot encoding\n10,000\nSystem Crashed\nSystem Crashed\nSystem Crashed\nSytem Crashed\n\n\n\nImmediately after this the system got crashed because of the size of vector and its computation even the computer size of 50 GB RAM could. not sustain.\n\n\n\n\n\ntherefore, the lesson learned is that we need to convert the input words into a better representation, like an embedding layer and then we performed on different architectures to explore the better fit for the model.\n\n\nNeural Networks with Embedding Layer\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFeed Forward Network with Embedding Layer\n10,000\n100%\n85%\n85%\nOverfitting\n\n\nGRU with Embedding Layer\n10,000\n100%\n80%\n80%\nOverfitting\n\n\nLSTM with Embedding Layer\n10,000\n100%\n80%\n80%\nOverfitting\n\n\nBi-LSTM with Embedding Layer\n10,000\n100%\n81%\n81%\nOverfitting\n\n\nBi-LSTM with Embedding Layer\n100,000\n100%\n86%\n86%\nOverfitting\n\n\n\nThe reason for overfitting is likely because, given the size of the data, the embedding layer is attempting to learn model parameters within the vocabulary of the input data. However, during validation and testing, there may be many out-of-vocabulary words, leading to underperformance. However, when we increased the dataset to 100K, the accuracy improved. The lesson learned is that if we can input the data with pre-trained embeddings learned on a larger corpus, we can achieve a better and more balanced model.\n\n\nNeural Networks with Pre-Trained Embedding Layer\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nBi-LSTM with Pretrained twitter Embeddings of 50D\n10,000\n90%\n85%\n84%\nDecent Model\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D\n100,000\n94%\n87%\n85%\nDecent Model\n\n\n\nGiven the success of pre-trained embeddings with larger dimensions, we aim to retain this learning and proceed to incorporate a more advanced architecture. The lesson we learned is that larger-dimensional embeddings capture richer attributes of words, which is beneficial. As part of our efforts to enhance the model’s learning, we decided to add an attention layer. This layer allows the model to focus on specific words, further improving its performance.\n\n\nBi-LSTMs with Attention Layer\nWe explored with three variants of attention. Couple of them are custom created and other is with Self Attention layer.\n\n## Custom Made Simple Attention Layer\n\nclass Attention(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        # Initialize the attention mechanism's parameters\n        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")  # Dense layer to compute attention scores\n        self.V = tf.keras.layers.Dense(1)  # Dense layer for the attention mechanism's weight calculation\n\n    def call(self, features):\n        # Compute attention scores\n        score = self.W1(features)\n\n        # Apply softmax activation to obtain attention weights\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n        # Compute context vector as the weighted sum of features\n        context_vector = attention_weights * features\n\n        return context_vector\n\n\n## Custom Made Slightly Complicated Attention Layer\n\nclass Attention_Update(tf.keras.Model):\n    def __init__(self, units):\n        super(Attention_Update, self).__init__()\n        # Initialize parameters for the attention mechanism\n        self.W1 = tf.keras.layers.Dense(units, activation=\"tanh\")  # Dense layer to compute attention scores\n        self.V = tf.keras.layers.Dense(1)  # Dense layer for attention weight calculation\n\n    def build(self, input_shape):\n        # Initialize trainable weights for attention mechanism\n        self.Wa = self.add_weight(name=\"att_weight_1\", shape=(input_shape[-1], 8),\n                                  initializer=\"normal\")  # Weight matrix for context vector computation\n        self.Wb = self.add_weight(name=\"att_weight_2\", shape=(input_shape[-1], 8),\n                                  initializer=\"normal\")  # Weight matrix for input features\n        self.b = self.add_weight(name=\"att_bias_2\", shape=(input_shape[1], 8),\n                                 initializer=\"zeros\")  # Bias term for context vector computation\n\n        super(Attention_Update, self).build(input_shape)\n\n    def call(self, features):\n        # Compute attention scores\n        score = self.W1(features)\n\n        # Apply softmax activation to obtain attention weights\n        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n\n        # Compute context vector as the weighted sum of features\n        context_vector = attention_weights * features\n\n        # Update the hidden state using attention mechanism\n        new_hidden_state = tf.tanh(tf.matmul(context_vector, self.Wa) + tf.matmul(features, self.Wb) + self.b)\n\n        return new_hidden_state\n\n\n# Define input layer with shape (100,)\ninputs = Input(shape=(100,))\n\n# Create an embedding layer with pre-trained weights\n# vocab_size: size of the vocabulary\n# output_dim: dimension of the embedding space\n# input_length: length of input sequences\n# weights: pre-trained embedding matrix\n# trainable: set to False to keep the pre-trained weights fixed during training\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n\n# Apply bidirectional LSTM to capture contextual information\nbilstm = Bidirectional(LSTM(4, activation='tanh', return_sequences=True))(embedding_layer)\n\n# Apply self-attention mechanism to focus on important features\ncontext_vector = SeqSelfAttention(attention_activation='sigmoid')(bilstm)\n\n# Apply SimpleRNN to capture sequential patterns\nsimplernn = SimpleRNN(4, activation=\"tanh\")(context_vector)\n\n# Output layer with sigmoid activation for binary classification\noutput = Dense(1, activation=\"sigmoid\")(simplernn)\n\n# Define the model\nmodel_lstm_bi_embed_selfattention = Model(inputs=inputs, outputs=output)\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D - Simple Attention\n100,000\n94%\n90%\n90%\nGood Model\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D - Slightly Complicated Attention\n100,000\n94%\n89%\n90%\nGood Model\n\n\nBi-LSTM with Pretrained twitter Embeddings of 200D - Keras Self Attention Layer\n100,000\n94%\n90%\n90%\nGood Model\n\n\n\nAll these models performed equally well; however, our intention is to create an even better model. Therefore, we proceeded to develop a custom model consisting of two bi-LSTMs with a simple attention layer, followed by an RNN, two feedforward networks, and finally, a softmax layer.\n\n\nCustom Made Neural Network Block\n\n\n\n\n\nThis Block we feel it can enhance sentence comprehension by learning relevant words and their dependencies within the sentence. It consists of several components that work together to achieve this goal:\n\nCombining Two Bi-LSTMs: This increases its complexity and enables it to learn sentence context in both forward and backward directions. This helps to capture a more comprehensive understanding of the text.\nAttention Layer: It focuses on relevant words within the sentence, allowing the model to concentrate on key information while disregarding irrelevant details. This mechanism helps to improve the overall accuracy of the model.\nSimple RNN: It helps to learn and capture relevant parameters based on context. This facilitates the understanding of word dependencies within the sentence and enables the model to achieve more accurate sentiment analysis.\n\nNotably, this block operates without a dense layer. Instead, it focuses on leveraging Bi-LSTMs, attention mechanisms, and simple RNNs to achieve effective sentence comprehension and sentiment analysis. However, dense layers can be introduced to the model to introduce more complexity and enable interactions between words and their attributes, thus improving overall comprehension and analysis.\n\n# Define hyperparameters\nlstm_units = 64\nattention_units = 96\nrnn_units = 64\ndense_units = 128\nlearning_rate = 0.001\n\n# Define input layer with shape (100,)\ninputs = Input(shape=(100,))\n\n# Create an embedding layer with pre-trained weights\nembedding_layer = Embedding(input_dim=vocab_size, output_dim=200, input_length=100, weights=[embedding_matrix_twitter_200d], trainable=False)(inputs)\n\n# Apply bidirectional LSTM layers with regularization\nbilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(embedding_layer)\nbilstm = Bidirectional(LSTM(lstm_units, activation='tanh', return_sequences=True, kernel_regularizer=l2(0.0001)))(bilstm)\n\n# Apply attention mechanism\ncontext_vector = Attention(attention_units)(bilstm)\n\n# Apply SimpleRNN layer with regularization\nsimplernn = SimpleRNN(rnn_units, activation=\"tanh\", return_sequences=True, kernel_regularizer=l2(0.0001))(context_vector)\n\n# Flatten the output for feedforward layers\nflatten = Flatten()(simplernn)\n\n# Apply two feedforward layers with regularization\nffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001))(flatten)\nffn = Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001))(ffn)\n\n# Output layer with sigmoid activation for binary classification\noutput = Dense(1, activation=\"sigmoid\")(ffn)\n\n# Define the model\nmodel_lstm_bi_embed_attention_complex_regularized_tuned = Model(inputs=inputs, outputs=output)\n\n# Compile the model\noptimizer = keras.optimizers.Adam(learning_rate)\nmodel_lstm_bi_embed_attention_complex_regularized_tuned.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Print model summary\nmodel_lstm_bi_embed_attention_complex_regularized_tuned.summary()\n\n\n\n\n\n\n\n\n\n\n\n\nModels\nData Size\nTrain Accuracy\nValidation Accuracy\nTest Accuracy\nComments\n\n\n\n\nFinal Custom block model with hyper tuned parameters\n100K\n91%\n91%\n91%\nBalanced Model"
  },
  {
    "objectID": "posts/deep-learning-project-msis/index.html#conclusion",
    "href": "posts/deep-learning-project-msis/index.html#conclusion",
    "title": "Amazon Reviews - Sentiment Classification",
    "section": "Conclusion",
    "text": "Conclusion\nNeural networks, when fine-tuned, regularized, and expanded, possess a great capacity to arrive at a better model. While traditional machine learning approach has given us 85% accuracy, the inherent flexibility of neural networks enables us to create of more sophisticated/complicated models. These models we feel are better to capture intricate patterns within the data, ultimately leading to superior performance.\nWe found great fulfillment in undertaking this project, prioritizing our learning journey beyond the confines of grading rubrics. It provided us with invaluable insights and a deeper understanding of the intricacies involved.\nEntire code can be downloaded from this link."
  },
  {
    "objectID": "posts/tensorflow_important_functions/index.html",
    "href": "posts/tensorflow_important_functions/index.html",
    "title": "Tensor Flow - Important Functions",
    "section": "",
    "text": "Previously, we were exploring basics of tensorflow mathematical and arthimetic operations. In this article we will explore few important functions that would be helpful in our modelling journey.\nimport tensorflow as tf\nimport numpy as np"
  },
  {
    "objectID": "posts/tensorflow_important_functions/index.html#stack-concat",
    "href": "posts/tensorflow_important_functions/index.html#stack-concat",
    "title": "Tensor Flow - Important Functions",
    "section": "1. Stack & Concat",
    "text": "1. Stack & Concat\nWe will explore through examples when we can use stack and concat.\nInitially we will explore the syntax of stack and concat and then understand its operations. Based on the output of these operations we will deduce how to use them.\n\n1.1 Syntax Exploration\n\nlist1 = [1,3,4]\nlist2 = [2,4,5]\nlist3 = [3,5,6]\nlist4 = [4,6,7]\n\n[list1, list2, list3, list4]\n\n[[1, 3, 4], [2, 4, 5], [3, 5, 6], [4, 6, 7]]\n\n\n\ntf.stack([list1, list2, list3, list4])\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[1, 3, 4],\n       [2, 4, 5],\n       [3, 5, 6],\n       [4, 6, 7]], dtype=int32)&gt;\n\n\n\ntf.concat([list1, list2, list3, list4], axis=0)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 3, 4, 2, 4, 5, 3, 5, 6, 4, 6, 7], dtype=int32)&gt;\n\n\nstack command is stacking the list where as concat has unrolled the list and concatened the elements.\n\n\n1.2 Exploring the Outputs\n\nlist1 = tf.constant([1,3,4])\nlist2 = tf.constant([2,4,5])\nlist3 = tf.constant([3,5,6])\nlist4 = tf.constant([4,6,7])\n\nprint(list1.shape)\n\nprint(tf.stack([list1, list2, list3, list4]).shape)\n\nprint(tf.concat([list1, list2, list3, list4], axis=0).shape)\n\n(3,)\n(4, 3)\n(12,)\n\n\nFrom this output we can deduce that when we are converting the same list into EagerTensor the output remained the same. Since, the dimensions of the input remanined the same.\n\nlist1 = tf.constant([1,3,4],shape=(1,3))\nlist2 = tf.constant([2,4,5],shape=(1,3))\nlist3 = tf.constant([3,5,6],shape=(1,3))\nlist4 = tf.constant([4,6,7],shape=(1,3))\n\nprint(list1.shape)\n\nprint(tf.stack([list1, list2, list3, list4]).shape)\n\nprint(tf.concat([list1, list2, list3, list4], axis=0).shape)\n\n(1, 3)\n(4, 1, 3)\n(4, 3)\n\n\n\nlist1 = tf.constant([1,3,4])\nprint(list1)\nlist1 = tf.constant([1,3,4],shape=(1,3))\nprint(list1)\n\ntf.Tensor([1 3 4], shape=(3,), dtype=int32)\ntf.Tensor([[1 3 4]], shape=(1, 3), dtype=int32)\n\n\nFrom this output we can deduce that when we convert from tensor 1D to 2D the difference in operations is conspicious.\nstack stacks the tensors one above the other, while concat combines the tensor on user-defined axis.\nOn this learning, we can use these commands in below scenario as per my comprenhension.\nIf we have multiple images and we want to stack them into a single variable we use stack. On the other side concat can we used to combined already pre-available stacked images in a variable.\n\nlist1 = tf.random.normal((24,24,3))\nlist2 = tf.random.normal((24,24,3))\nlist3 = tf.random.normal((24,24,3))\nlist4 = tf.random.normal((24,24,3))\ntf.stack([list1, list2, list3, list4]).shape\n\nTensorShape([4, 24, 24, 3])\n\n\nIn the above toy example we are stacking four images of 24X24X3 dimensions using tf.stack\n\nlist1 = tf.random.normal((4,24,24,3))\nlist2 = tf.random.normal((4,24,24,3))\nlist3 = tf.random.normal((4,24,24,3))\nlist4 = tf.random.normal((4,24,24,3))\ntf.concat([list1, list2, list3, list4], axis=0).shape\n\nTensorShape([16, 24, 24, 3])\n\n\nFurther, we are combining the stacked images into a larger stack of 16 images."
  },
  {
    "objectID": "posts/tensorflow_important_functions/index.html#unstack",
    "href": "posts/tensorflow_important_functions/index.html#unstack",
    "title": "Tensor Flow - Important Functions",
    "section": "2. UnStack",
    "text": "2. UnStack\nUnstack is straightforward and simple\n\nlist1 = tf.random.normal((24,24,3))\nlist2 = tf.random.normal((24,24,3))\nlist3 = tf.random.normal((24,24,3))\nlist4 = tf.random.normal((24,24,3))\ntf.stack([list1, list2, list3, list4]).shape\n\nTensorShape([4, 24, 24, 3])\n\n\n\nprint(len(tf.unstack(tf.stack([list1, list2, list3, list4]), axis=0)))\n\nprint(type(tf.unstack(tf.stack([list1, list2, list3, list4]), axis=0)))\n\n4\n&lt;class 'list'&gt;\n\n\nUnstacking splits the stacked image in a list"
  },
  {
    "objectID": "posts/tensorflow_important_functions/index.html#expand-dimensions-gather",
    "href": "posts/tensorflow_important_functions/index.html#expand-dimensions-gather",
    "title": "Tensor Flow - Important Functions",
    "section": "3. Expand Dimensions & Gather",
    "text": "3. Expand Dimensions & Gather\nWe will explore through examples when we can use expand_dims and gather.\nInitially we will explore the syntax of expand_dims and gather and then understand its operations. Based on the output of these operations we will deduce how to use them.\n\nx = tf.random.normal((3,))\nprint(x)\nprint(\"\")\nprint(tf.expand_dims(x, axis = 0))\nprint(\"\")\nprint(tf.expand_dims(x, axis = 1))\n\ntf.Tensor([-0.67980325  0.951565   -0.67902976], shape=(3,), dtype=float32)\n\ntf.Tensor([[-0.67980325  0.951565   -0.67902976]], shape=(1, 3), dtype=float32)\n\ntf.Tensor(\n[[-0.67980325]\n [ 0.951565  ]\n [-0.67902976]], shape=(3, 1), dtype=float32)\n\n\nexpand_dims looks like adding one more dimensions by converting from 1D to 2D as defined by user.\n\nprint(tf.gather(x, [0]))\nprint(\"\")\nprint(tf.gather(x, [0,1]))\nprint(\"\")\nprint(tf.gather(x, [1,2,0]))\n\ntf.Tensor([-0.67980325], shape=(1,), dtype=float32)\n\ntf.Tensor([-0.67980325  0.951565  ], shape=(2,), dtype=float32)\n\ntf.Tensor([ 0.951565   -0.67902976 -0.67980325], shape=(3,), dtype=float32)\n\n\ngather is gathering the information based in the indices shared by the user.\n\n3.1 Exploding Expand_Dims\nBased on the outputs I can think of using expand_dims while performing any operations which would not have been possible without expanding dimensions.\n\n\ntensor1d = tf.constant([1, 2, 3])\ntensor2d = tf.constant([[4, 5, 6], [7, 8, 9]])\n\ntensor_1d + tensor_2d\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-78-3733c7bbb351&gt; in &lt;cell line: 4&gt;()\n      2 tensor2d = tf.constant([[4, 5, 6], [7, 8, 9]])\n      3 \n----&gt; 4 tensor_1d + tensor_2d\n\nNameError: name 'tensor_2d' is not defined\n\n\n\n\n\ntensor1d_expanded = tf.expand_dims(tensor1d, axis=0)  # Expand along axis 0\n\n# Now you can add the two tensors\nresult = tensor1d_expanded + tensor2d\nprint(result)\n\ntf.Tensor(\n[[ 5  7  9]\n [ 8 10 12]], shape=(2, 3), dtype=int32)\n\n\n\n\n3.2. Gather\nBased on the outputs I can think of using gather while extracting relevant dimension of the tensor for further operations in the modelling\n\nimage_tensor = tf.random.normal((224, 224, 3))  # Example image tensor\n\n# Extract the red and blue channels (indices 0 and 2)\nextracted_channels = tf.gather(image_tensor, [0, 2], axis=-1)\nprint(extracted_channels.shape)  # Output: (224, 224, 2)\n\n(224, 224, 2)\n\n\nIn the above example we have extracted only two channels instead of three channels."
  },
  {
    "objectID": "posts/kaggle_vegetables_project/index.html",
    "href": "posts/kaggle_vegetables_project/index.html",
    "title": "Vegetable Classification and Recognition",
    "section": "",
    "text": "This article explores the use of convolution neural networks (CNN). Also, since it is my second deep learning project, I took every opportuinity to build my basics strongly.\nDataset consists of 15 vegetable ( Bean, Bitter_Gourd, Bottle_Gourd, Brinjal, Broccoli, Cabbage, Capsicum, Carrot, Cauliflower, Cucumber, Papaya, Potato, Pumpkin, Radish, Tomato) images resized to 224X224 pixel and arranged in three different folders of train, test and dev.\n\n\n\n\n\nEach class has equal proportion of images making it a balanced dataset."
  },
  {
    "objectID": "posts/kaggle_vegetables_project/index.html#introduction",
    "href": "posts/kaggle_vegetables_project/index.html#introduction",
    "title": "Vegetable Classification and Recognition",
    "section": "",
    "text": "This article explores the use of convolution neural networks (CNN). Also, since it is my second deep learning project, I took every opportuinity to build my basics strongly.\nDataset consists of 15 vegetable ( Bean, Bitter_Gourd, Bottle_Gourd, Brinjal, Broccoli, Cabbage, Capsicum, Carrot, Cauliflower, Cucumber, Papaya, Potato, Pumpkin, Radish, Tomato) images resized to 224X224 pixel and arranged in three different folders of train, test and dev.\n\n\n\n\n\nEach class has equal proportion of images making it a balanced dataset."
  },
  {
    "objectID": "posts/kaggle_vegetables_project/index.html#traditional-machine-learning",
    "href": "posts/kaggle_vegetables_project/index.html#traditional-machine-learning",
    "title": "Vegetable Classification and Recognition",
    "section": "Traditional Machine Learning",
    "text": "Traditional Machine Learning\nFirst of all, I have used the de-facto image processing package of python - PIL ( Python Imaging Library ). Further, created an iterative function load_images to load images from the respective directory.\n\ndef load_images(imagepath):\n  image_data = []\n  image_data_flatten = []\n  labels = []\n  files_or_folders = os.listdir(imagepath)\n  for i in files_or_folders:\n    if(os.path.isdir(imagepath+i)):\n      imagefiles = os.listdir(imagepath+i)\n      for j in imagefiles:\n        if (np.asarray(Image.open(imagepath+i+\"/\"+j)).shape == (224,224,3)):\n          data = np.asarray(Image.open(imagepath+i+\"/\"+j))\n          image_data.append(data)\n          data = data.reshape(-1,)\n          image_data_flatten.append(data)\n          labels.append(i)\n  return image_data, image_data_flatten, labels\n\nLater, have modeled both Logistic Regression & Random Forest. Random Forest gave a good lift on test accuracy, but it is over-fitting on training data.\n\n\n\nModel\nTrain Accuracy\nTest/Validation Accuracy\n\n\n\n\nLogistic Regression\n77%\n62%\n\n\nRandom Forest\n100%\n79%\n\n\n\nBelow is detailed classification matrix for both these models. ( Carrot identification better by both models )"
  },
  {
    "objectID": "posts/kaggle_vegetables_project/index.html#deep-learning-neural-networks",
    "href": "posts/kaggle_vegetables_project/index.html#deep-learning-neural-networks",
    "title": "Vegetable Classification and Recognition",
    "section": "Deep Learning/ Neural Networks",
    "text": "Deep Learning/ Neural Networks\nIf I had performed my experimentation on same notebook and in one sitting, I would have used the loaded images output which was used earlier. However, as the learning continued for multiple days, I came across image_dataset_from_directory which is far more easier to load the images in keras.\nBelow is the simple command where we can load the images directly.\n\ntrain_ds = tf.keras.utils.image_dataset_from_directory(\n  train_dir,\n  seed=123,\n  image_size=(224, 224),\n  batch_size=32)\n\n\ntest_ds = tf.keras.utils.image_dataset_from_directory(\n  test_dir,\n  seed=123,\n  image_size=(224, 224),\n  batch_size=32)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n  val_dir,\n  seed=123,\n  image_size=(224, 224),\n  batch_size=32)\n\n\nOne & Two Layered CONV2D Model\nTo start with I have explored Single and Two layered CONV2D Models.\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(15, activation='softmax')\n])\n\n\nmodel_layered = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (5,5), activation='relu', input_shape=(224, 224, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1024, activation='relu'),\n    tf.keras.layers.Dense(15, activation='softmax')\n])\n\n\n\n\nModel\nEpochs\nTrain Accuracy\nTest/Validation Accuracy\n\n\n\n\nOne-Layered\n10\n97%\n63%\n\n\nTwo-Layered\n10\n90%\n52%\n\n\n\nBoth these models, definetly over fits on the training data suggesting us to explore better models.\n\n\nRescaling\nRescaling helps to create a better model.\n\nmodel_rescale = tf.keras.Sequential([\n    tfl.Rescaling(1./255, input_shape=(224, 224, 3)),\n    tfl.Conv2D(32,7,padding='same',activation='relu'),\n    tfl.MaxPooling2D(),\n    tfl.Conv2D(64,5,padding='same',activation='relu'),\n    tfl.MaxPooling2D(),\n    tfl.Conv2D(128,3,padding='same',activation='relu'),\n    tfl.MaxPooling2D(),\n    tfl.Flatten(),\n    tfl.Dense(1024,activation='relu'),\n    tfl.Dense(128,activation='relu'),\n    tfl.Dense(15,activation='softmax')\n])\n\n\n\n\nModel\nEpochs\nTrain Accuracy\nTest/Validation Accuracy\n\n\n\n\nRe-scaling\n10\n98%\n93%\n\n\n\nWe can see that by reshaping the data and adding a layered CONV2D structure we got a way better model compared to traditional ML methods. However, still it over fits on the training data.\n\n\nData-Augmentation\nThe most common way to avoid overfitting is to perform either flipping, rotating or zoomning the image. The commands are fairly simple if we use keras.\n\ninput = tf.keras.Input(shape=(224, 224, 3))\nx = tfl.RandomFlip(\"horizontal\")(input)\nx = tfl.RandomRotation(0.2)(x)\nx = tfl.RandomZoom(0.2)(x)\nx = tfl.Rescaling(1./255)(x)\nx = tfl.Conv2D(32,7,padding='same',activation='relu')(x)\nx = tfl.MaxPooling2D()(x)\nx = tfl.Conv2D(64,7,padding='same',activation='relu')(x)\nx = tfl.MaxPooling2D()(x)\nx = tfl.Conv2D(128,7,padding='same',activation='relu')(x)\nx = tfl.MaxPooling2D()(x)\nx = tfl.Flatten()(x)\nx = tfl.Dense(1024,activation='relu')(x)\nx = tfl.Dense(512,activation='relu')(x)\noutput = tfl.Dense(15,activation = \"softmax\")(x)\n\nmodel_dataaug = tf.keras.Model(input,output)\nmodel_dataaug.summary()\n\n\n\n\n\n\n\n\n\n\nModel\nEpochs\nTrain Accuracy\nTest/Validation Accuracy\n\n\n\n\nData Augmentation\n10\n95%\n95%\n\n\n\nWow!!! we got a very decent and balanced model with 95% accuracy with data augmentation. Now, Let’s explore transfer learning.\n\n\nTransfer Learning with Custom Layers\nEarlier, transfer learning of MobileNetV2 without custom layer has got an accuracy of 97% on test and dev sets. So, I have added two custom layers of Dense with 1024 and 512 neurons each.\nHere is the network structure of the transfer learning model.\n\n\n\n\n\n\ninputs = tf.keras.Input(shape=(224, 224, 3))\nx = tfl.Rescaling(1./255)(inputs)\nx = tfl.RandomFlip(\"horizontal\")(x)\nx = tfl.RandomRotation(0.2)(x)\nx = tfl.RandomZoom(0.2)(x)\nx = mobilenet_model(x, training=False)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tfl.Flatten()(x)\nx = tfl.Dense(1024, activation='relu')(x)\nx = tfl.Dropout(0.2)(x)\nx = tfl.Dense(512, activation='relu')(x)\nx = tfl.Dropout(0.2)(x)\noutputs = tf.keras.layers.Dense(15)(x)\nmodel = tf.keras.Model(inputs, outputs)\n\nAnd WOW!!! This model gave 100 % Accuracy. Wohooo!!!!\n\n\n\n\n\n\n\n\n\nModel\nEpochs\nTrain Accuracy\nTest/Validation Accuracy\n\n\n\n\nTransfer_Learning\n15\n100%\n100%\n\n\n\nBelow is detailed classification matrix for this model on test data."
  },
  {
    "objectID": "posts/kaggle_vegetables_project/index.html#conclusion",
    "href": "posts/kaggle_vegetables_project/index.html#conclusion",
    "title": "Vegetable Classification and Recognition",
    "section": "Conclusion",
    "text": "Conclusion\nIt is wonderful to learn that CNN models are far better for images. Adding to it, transfer learning with custom layers is always better to achieve higher accuracy in quick time. I am so happy to get 100% accuracy on my first exploration on image dataset."
  },
  {
    "objectID": "posts/Word2Vec/index.html",
    "href": "posts/Word2Vec/index.html",
    "title": "Word2Vec Similarities and Product Recommendations",
    "section": "",
    "text": "The prime purpose of this article is to demonstrate the creation of embedding vectors using gensism Word2Vec."
  },
  {
    "objectID": "posts/Word2Vec/index.html#reconstruction-of-famous-king---man-woman-queen",
    "href": "posts/Word2Vec/index.html#reconstruction-of-famous-king---man-woman-queen",
    "title": "Word2Vec Similarities and Product Recommendations",
    "section": "Reconstruction of famous king - man + woman = queen",
    "text": "Reconstruction of famous king - man + woman = queen\n\nfrom gensim.models import Word2Vec\n\nsentences = [\n    [\"the\", \"king\", \"rules\", \"the\", \"kingdom\", \"with\", \"wisdom\"],\n    [\"the\", \"queen\", \"leads\", \"the\", \"kingdom\", \"with\", \"grace\"],\n    [\"a\", \"prince\", \"inherits\", \"the\", \"throne\", \"from\", \"his\", \"father\"],\n    [\"a\", \"princess\", \"inherits\", \"the\", \"throne\", \"from\", \"her\", \"mother\"],\n    [\"the\", \"king\", \"and\", \"the\", \"queen\", \"govern\", \"together\"],\n    [\"the\", \"prince\", \"respects\", \"his\", \"father\", \"the\", \"king\"],\n    [\"the\", \"princess\", \"admires\", \"her\", \"mother\", \"the\", \"queen\"],\n    [\"the\", \"actor\", \"performs\", \"in\", \"a\", \"grand\", \"theater\"],\n    [\"the\", \"actress\", \"won\", \"an\", \"award\", \"for\", \"her\", \"role\"],\n    [\"a\", \"man\", \"is\", \"known\", \"for\", \"his\", \"strength\", \"and\", \"wisdom\"],\n    [\"a\", \"woman\", \"is\", \"admired\", \"for\", \"her\", \"compassion\", \"and\", \"intelligence\"],\n    [\"the\", \"future\", \"king\", \"is\", \"the\", \"prince\"],\n    [\"daughter\", \"is\", \"the\", \"princess\"],\n    [\"son\", \"is\", \"the\", \"prince\"],\n    [\"only\", \"a\", \"man\", \"can\", \"be\", \"a\", \"king\"],\n    [\"only\", \"a\", \"woman\", \"can\", \"be\", \"a\", \"queen\"],\n    [\"the\", \"princess\", \"will\", \"be\", \"a\", \"queen\"],\n    [\"queen\", \"and\", \"king\", \"rule\", \"the\", \"realm\"],\n    [\"the\", \"prince\", \"is\", \"a\", \"strong\", \"man\"],\n    [\"the\", \"princess\", \"is\", \"a\", \"beautiful\", \"woman\"],\n    [\"the\", \"royal\", \"family\", \"is\", \"the\", \"king\", \"and\", \"queen\", \"and\", \"their\", \"children\"],\n    [\"prince\", \"is\", \"only\", \"a\", \"boy\", \"now\"],\n    [\"a\", \"boy\", \"will\", \"be\", \"a\", \"man\"]\n]\n\nmodel = Word2Vec(sentences, vector_size=2, window=5, min_count=1, workers=4, sg=1, epochs=100, negative=10)\n\nHere we have created few random sentences and ran Word2Vector model\nvector_size=2 is initialized visualize the embedding vectors on 2d space to start with\n\nword_vectors_dict = {word: model.wv[word].tolist() for word in model.wv.index_to_key}\n\nHere we have converted the embedding into dictionary for easy plotting\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor word in list(word_vectors_dict.keys()):\n    coord = word_vectors_dict.get(word)\n    if coord is not None:\n        plt.scatter(coord[0], coord[1])\n        plt.annotate(word, (coord[0], coord[1]))\n\nplt.title(\"2D Visualization of Word Embeddings\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on this result we can see that the few words are grouped togther like admires , rules and respect award, performs and theater\nHowever, most of the other word groups doesn’t make any sense at this point of time. The prime reason behind it is due to the embedding vector size. If we increase the embedding vector size the more nuanced relationships will be formed.\nHowever, this demonstrates that the word2vec model tries to gather similar words together.\nNow let us try the famous example of kind and queen\n\nresult = model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\nprint(result)\n\n[('be', 0.9999998211860657)]\n\n\nOOOPs! it got failed.\nNo worries!!! lets increase the dimensions of the vector :)\n\nmodel_highdimensional = Word2Vec(sentences, vector_size=16, window=5, min_count=1, workers=4, sg=1, epochs=1000, negative=10)\n\n\nresult = model_highdimensional.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)\nprint(result)\n\n[('queen', 0.8557833433151245)]\n\n\nHurray we got it !!!! :)"
  },
  {
    "objectID": "posts/Word2Vec/index.html#exploring-similarities",
    "href": "posts/Word2Vec/index.html#exploring-similarities",
    "title": "Word2Vec Similarities and Product Recommendations",
    "section": "Exploring Similarities",
    "text": "Exploring Similarities\n\n model_highdimensional.wv.most_similar(\"mother\", topn=5)\n\n[('admires', 0.927798867225647),\n ('inherits', 0.8661039471626282),\n ('princess', 0.8615163564682007),\n ('from', 0.8585997819900513),\n ('throne', 0.8579676151275635)]\n\n\n\n model_highdimensional.wv.most_similar(\"father\", topn=5)\n\n[('respects', 0.9215425848960876),\n ('throne', 0.8824634552001953),\n ('prince', 0.861110270023346),\n ('from', 0.8586324453353882),\n ('inherits', 0.8541713356971741)]\n\n\nHere, we can see that the most similar word to mother is admire, which makes perfect sense. Similarly, for father, the most similar word is respect.\nThis also helps us explore the idea of finding the most similar words, akin to finding the most similar products in a recommendation system. Imagine that instead of sentences, we have a transaction table. We need to find the most similar products to recommend together.\nRather than using traditional Apriori for association rule mining, we can apply Word2Vec on a large dataset to derive a set of recommendations. This can be used in conjunction with traditional association algorithms to improve the quality of recommendations."
  },
  {
    "objectID": "posts/Word2Vec/index.html#extending-similarties-vis-a-vis-recommendations",
    "href": "posts/Word2Vec/index.html#extending-similarties-vis-a-vis-recommendations",
    "title": "Word2Vec Similarities and Product Recommendations",
    "section": "Extending Similarties vis-a-vis Recommendations",
    "text": "Extending Similarties vis-a-vis Recommendations\n\nimport random\n\ntransactions = [\n    [\"laptop\", \"mouse\", \"keyboard\", \"usb_c_hub\"],\n    [\"smartphone\", \"wireless_charger\", \"earbuds\", \"phone_case\"],\n    [\"gaming_console\", \"gaming_controller\", \"headset\", \"gaming_mouse\"],\n    [\"tv\", \"soundbar\", \"streaming_device\"],\n    [\"tablet\", \"stylus\", \"tablet_case\"],\n    [\"laptop\", \"external_hard_drive\", \"usb_c_hub\"],\n    [\"smartphone\", \"screen_protector\", \"phone_case\"],\n    [\"gaming_console\", \"gaming_headset\", \"gaming_keyboard\"],\n    [\"tv\", \"bluetooth_speaker\", \"universal_remote\"],\n    [\"tablet\", \"portable_charger\", \"tablet_stand\"],\n    [\"camera\", \"tripod\", \"memory_card\", \"camera_bag\"],\n    [\"drone\", \"drone_batteries\", \"camera\", \"action_cam\"],\n    [\"smartwatch\", \"fitness_band\", \"wireless_earbuds\"],\n    [\"gaming_console\", \"gaming_mouse\", \"gaming_monitor\"],\n    [\"smartphone\", \"portable_charger\", \"wireless_earbuds\"],\n]\n\nrandom.shuffle(transactions)\n\n\nfrom gensim.models import Word2Vec\n\nmodel = Word2Vec(sentences=transactions, vector_size=100, window=4, min_count=1, workers=4, sg=1, epochs=1000, negative=5)\n\n\nproduct = \"laptop\"\nrecommendations = model.wv.most_similar(product, topn=5)\n\nprint(f\"Top recommendations for '{product}':\")\nfor item, score in recommendations:\n    print(f\"{item}: {score:.4f}\")\n\nTop recommendations for 'laptop':\nmouse: 0.9697\nusb_c_hub: 0.9696\ncamera: 0.9673\nkeyboard: 0.9666\ntripod: 0.9665\n\n\nHurray!! We have constructed a mini and simple recommendation model using Word2Vec.\nWow! this isn’t beautiful"
  },
  {
    "objectID": "posts/RepresentationModels_TextClassification/index.html",
    "href": "posts/RepresentationModels_TextClassification/index.html",
    "title": "Hugging Face - Representation Models",
    "section": "",
    "text": "In this module, we will explore the basics of two approaches to text classification using Encoder Transformers: - Using BERT - Using Label Encodings (Sentence Transformers)\nEncourage to explore this article to understand the background and intuition behind these two models.\nIn this article, we will also delve into sentiment classification through the following methods: - Without training - Using BERT LLM and Logistic Regression - Using Sentence Transformers LLM and Logistic Regression - Creating labels when they are not available\n\n\n\nInstalling & Loading Libraries\n\n!pip install datasets\n\nCollecting datasets\n  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow&gt;=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\nCollecting dill&lt;0.3.9,&gt;=0.3.0 (from datasets)\n  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests&gt;=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm&gt;=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nCollecting xxhash (from datasets)\n  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nCollecting multiprocess&lt;0.70.17 (from datasets)\n  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: fsspec&lt;=2024.12.0,&gt;=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]&lt;=2024.12.0,&gt;=2023.1.0-&gt;datasets) (2024.10.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub&gt;=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml&gt;=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (2.4.6)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.3.2)\nRequirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (25.1.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.5.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (6.1.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (0.2.1)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-&gt;datasets) (1.18.3)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub&gt;=0.24.0-&gt;datasets) (4.12.2)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.1)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2.3.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests&gt;=2.32.2-&gt;datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2.8.2)\nRequirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-&gt;datasets) (2025.1)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)\nDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 485.4/485.4 kB 15.2 MB/s eta 0:00:00\nDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.3/116.3 kB 15.1 MB/s eta 0:00:00\nDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.5/143.5 kB 16.8 MB/s eta 0:00:00\nDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.8/194.8 kB 22.9 MB/s eta 0:00:00\nInstalling collected packages: xxhash, dill, multiprocess, datasets\nSuccessfully installed datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n\n\n\nfrom google.colab import drive\nimport os\n\nimport pandas as pd\n\nfrom transformers import TFAutoModelForSequenceClassification, AutoTokenizer\nimport tensorflow as tf\nimport numpy as np\n\nimport datasets\nfrom datasets import Dataset, DatasetDict\n\n\n\nBERT Model - Sentiment Prediction w/o Training\n\ncheckpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n\nAll the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n\n\nIf we can observe all the related base files are loaded; that includes model configuration, model itself and vocab text\nNow predicting is simple like we use chatgpt\n\nmodel.summary()\n\nModel: \"tf_distil_bert_for_sequence_classification\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n pre_classifier (Dense)      multiple                  590592    \n                                                                 \n classifier (Dense)          multiple                  1538      \n                                                                 \n dropout_19 (Dropout)        multiple                  0 (unused)\n                                                                 \n=================================================================\nTotal params: 66955010 (255.41 MB)\nTrainable params: 66955010 (255.41 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nSince we have used TFAutoModelForSequenceClassification the model has a default classifier which predicts the output etiher as positive or negative.\n\nos.chdir('/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/')\n\ntest_data = pd.read_csv('test_data_sample_complete.csv')\ntrain_data = pd.read_csv('train_data_sample_complete.csv')\n\ntest_data = test_data.sample(n=1500, random_state=42)\ntrain_data = train_data.sample(n=1500, random_state=42)\n\ntest_data['class_index'] = test_data['class_index'].map({1:0, 2:1})\ntrain_data['class_index'] = train_data['class_index'].map({1:0, 2:1})\n\ntest_data['review_combined_lemma'] = test_data['review_combined_lemma'].fillna('')\ntrain_data['review_combined_lemma'] = train_data['review_combined_lemma'].fillna('')\n\ntest_data = Dataset.from_pandas(test_data)\ntrain_data = Dataset.from_pandas(train_data)\nraw_data = DatasetDict()\nraw_data[\"test\"] = test_data\nraw_data[\"train\"] = train_data\n\nprint(raw_data)\n\nDatasetDict({\n    test: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__'],\n        num_rows: 1500\n    })\n    train: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__'],\n        num_rows: 1500\n    })\n})\n\n\nThis dataset contains Amazon reviews, downloaded from Kaggle, pre-processed, and stored for a project assignment I completed about a year ago.\nUsing Datasets package, we have converted the dataset into the required format of huggingface transformers processing\n\nDataset.to_pandas(raw_data['test'])\n\n\n  \n    \n\n\n\n\n\n\nclass_index\nreview_combined_lemma\n__index_level_0__\n\n\n\n\n0\n1\ngreat book must preface saying not religious l...\n23218\n\n\n1\n0\nhuge disappointment big time long term trevani...\n20731\n\n\n2\n1\nwayne tight cant hang turk album hot want howe...\n39555\n\n\n3\n1\nexcellent read book elementary school probably...\n147506\n\n\n4\n0\nnot anusara although book touted several anusa...\n314215\n\n\n...\n...\n...\n...\n\n\n1495\n0\nindifferent hears big dog little dog yap away ...\n316639\n\n\n1496\n1\nmovie watch grandchild good movie little gore ...\n91834\n\n\n1497\n1\npatriot did win superbowl great piece memorabi...\n176737\n\n\n1498\n0\n11 stinker never fan series cd really bizarre ...\n298198\n\n\n1499\n0\nreason sampler no soul intensity orchestra pla...\n277986\n\n\n\n\n1500 rows × 3 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\n\ntokenized_ids = tokenizer(raw_data['test']['review_combined_lemma'], truncation=True,padding=True,return_tensors=\"tf\", max_length=128)\nmodel_output = model(tokenized_ids)\n\nHere we converted the raw data into numerical format using tokenizer, which tokenizes the text into numbers using the downloaded vocab dictionary.\nThese tokens are passed into the model and output is captured.\nSince, we are not training the model again we are tokenizing only the test data set.\n\nfrom sklearn.metrics import classification_report\n\ntf.keras.backend.clear_session()\n\nprint(classification_report(raw_data['test']['class_index'], tf.argmax(model_output.logits,axis=1)))\n\n              precision    recall  f1-score   support\n\n           0       0.72      0.95      0.82       722\n           1       0.93      0.67      0.78       778\n\n    accuracy                           0.80      1500\n   macro avg       0.83      0.81      0.80      1500\nweighted avg       0.83      0.80      0.80      1500\n\n\n\nHere, we can see that the default foundationmodel of BERT is giving us 80% accuracy. Which is very good :).\n\n\nBert with Logistic Regression\n\nfrom transformers import TFAutoModel\n\nbert_model = TFAutoModel.from_pretrained(checkpoint)\nbert_model.summary()\n\nSome weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\n- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFDistilBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n\n\nModel: \"tf_distil_bert_model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n distilbert (TFDistilBertMa  multiple                  66362880  \n inLayer)                                                        \n                                                                 \n=================================================================\nTotal params: 66362880 (253.15 MB)\nTrainable params: 66362880 (253.15 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nSince, we will be training the classifer layer we are loading the model without classifer layer using the command TFAutoModel. You can see the difference in outputs in both models\n\ntokenized_ids = tokenizer(raw_data['train']['review_combined_lemma'], truncation=True,padding=True,return_tensors=\"tf\", max_length=128)\nbert_output = bert_model(tokenized_ids)\n\nWe are tokenizing the training dataset.\n\nbert_output.last_hidden_state.numpy().mean(axis=1).shape\nreshaped_output = bert_output.last_hidden_state.numpy().mean(axis=1)\n\nExtracting the last layer output\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(reshaped_output, raw_data['train']['class_index'])\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\nFeeding the BERT last layer output to the Logistic Regression and trained the Logistic Regression.\n\nfrom sklearn.metrics import classification_report\ntokenized_ids = tokenizer(raw_data['test']['review_combined_lemma'], truncation=True,padding=True,return_tensors=\"tf\", max_length=128)\nbert_output = bert_model(tokenized_ids)\nreshaped_output = bert_output.last_hidden_state.numpy().mean(axis=1)\ny_pred = lr.predict(reshaped_output)\nprint(classification_report(raw_data['test']['class_index'], y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.84      0.86      0.85       722\n           1       0.87      0.84      0.85       778\n\n    accuracy                           0.85      1500\n   macro avg       0.85      0.85      0.85      1500\nweighted avg       0.85      0.85      0.85      1500\n\n\n\nOn Test Dataset we can see that the accuracy has jumped from 80% to 85% with a mere Logistic Classifier at the end. Isn’t it beautiful. However, only drawback of this is that is consumes lot of GPU memory.\n\n\nSentence Transformers with Logistic Regression\n\nos.chdir('/content/drive/My Drive/MSIS/IntroductiontoDeepLearning/Project/')\n\ntest_data = pd.read_csv('test_data_sample_complete.csv')\ntrain_data = pd.read_csv('train_data_sample_complete.csv')\n\ntest_data = test_data.sample(n=10000, random_state=42)\ntrain_data = train_data.sample(n=10000, random_state=42)\n\ntest_data['class_index'] = test_data['class_index'].map({1:0, 2:1})\ntrain_data['class_index'] = train_data['class_index'].map({1:0, 2:1})\n\ntest_data['review_combined_lemma'] = test_data['review_combined_lemma'].fillna('')\ntrain_data['review_combined_lemma'] = train_data['review_combined_lemma'].fillna('')\n\ntest_data = Dataset.from_pandas(test_data)\ntrain_data = Dataset.from_pandas(train_data)\nraw_data = DatasetDict()\nraw_data[\"test\"] = test_data\nraw_data[\"train\"] = train_data\n\nprint(raw_data)\n\nDatasetDict({\n    test: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__'],\n        num_rows: 10000\n    })\n    train: Dataset({\n        features: ['class_index', 'review_combined_lemma', '__index_level_0__'],\n        num_rows: 100000\n    })\n})\n\n\nI have reloaded the dataset to demonstrate that sentence transformers can handle larger datasets more efficiently compared to the BERT model shown earlier. Sentence transformers effortlessly convert text into embeddings, reducing memory usage for tokenization and subsequent model processing.\nAlthough both models are based on BERT, sentence transformers offer better memory efficiency.\n\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n\ntrain_embeddings = model.encode(raw_data['train']['review_combined_lemma'], show_progress_bar=True)\ntest_embeddings = model.encode(raw_data['test']['review_combined_lemma'], show_progress_bar=True)\n\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLoaded the model and converted both train data and test data into embeddings.\n\ntrain_embeddings.shape\n\n(100000, 768)\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(max_iter=1000)\nlr.fit(train_embeddings, raw_data['train']['class_index'])\n\nLogisticRegression(max_iter=1000)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression(max_iter=1000) \n\n\nFurthermore, we trained a lightweight logistic regression model using those embeddings.\n\nfrom sklearn.metrics import classification_report\n\ny_pred = lr.predict(test_embeddings)\nprint(classification_report(raw_data['test']['class_index'], y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.88      0.86      0.87      4972\n           1       0.86      0.88      0.87      5028\n\n    accuracy                           0.87     10000\n   macro avg       0.87      0.87      0.87     10000\nweighted avg       0.87      0.87      0.87     10000\n\n\n\nHere, we can see that our accuracy increased from 85% to 87%. However, we cannot directly attribute this improvement to the use of sentence transformers alone, as both BERT and sentence transformers capture the context of the information. That said, based on my understanding, sentence transformers are faster, more scalable, and reliable.\n\n\nCreating Labels Using Sentence Transformers\nLet’s assume that instead of predicting positive or negative sentiment, we want to classify sentiment on a 5-point Likert scale. Sentence transformers come in handy here, as they allow us to explore the similarity between the labels and the input text, helping us tag the input accordingly.\n\nlabel_embeddings = model.encode( [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"], show_progress_bar=True)\n\n\n\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncosine_similarity(test_embeddings, label_embeddings)\n\narray([[0.15364526, 0.17884818, 0.12452998, 0.1333864 , 0.08642562],\n       [0.27978075, 0.19118355, 0.12162416, 0.17023209, 0.17311683],\n       [0.07127699, 0.14324695, 0.07260972, 0.08962228, 0.07847168],\n       ...,\n       [0.15041098, 0.13494283, 0.01669509, 0.1404528 , 0.17394802],\n       [0.00270087, 0.05694368, 0.01807276, 0.0432991 , 0.03236848],\n       [0.13147888, 0.17518383, 0.14696477, 0.15878314, 0.17004938]],\n      dtype=float32)\n\n\nIts simple, we have arrived at cosine similarly of both input text and output labels that we have defined above.\n\nsim_matrix = cosine_similarity(test_embeddings, label_embeddings)\ny_pred = np.argmax(sim_matrix, axis=1)\ny_pred\n\narray([1, 0, 1, ..., 4, 1, 1])\n\n\n\n\nlabels = [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]\ny_pred_labels = [labels[i] for i in y_pred]\n\ntest_df = Dataset.to_pandas(raw_data['test'])\ny_pred_df = pd.DataFrame(y_pred_labels, columns=['Predicted_Labels'])\n\ncombined_df = pd.concat([test_df.reset_index(drop=True), y_pred_df.reset_index(drop=True)], axis=1)\ncombined_df\n\n\n  \n    \n\n\n\n\n\n\nclass_index\nreview_combined_lemma\n__index_level_0__\nPredicted_Labels\n\n\n\n\n0\n1\ngreat book must preface saying not religious l...\n23218\nNegative\n\n\n1\n0\nhuge disappointment big time long term trevani...\n20731\nVery Negative\n\n\n2\n1\nwayne tight cant hang turk album hot want howe...\n39555\nNegative\n\n\n3\n1\nexcellent read book elementary school probably...\n147506\nPositive\n\n\n4\n0\nnot anusara although book touted several anusa...\n314215\nNegative\n\n\n...\n...\n...\n...\n...\n\n\n9995\n0\nleft many question read book recently diagnose...\n105263\nPositive\n\n\n9996\n1\nliked wontrom reading rest great book no doubt...\n334968\nNegative\n\n\n9997\n1\nrecorder product durable bought fourth grader ...\n355111\nVery Positive\n\n\n9998\n1\nlike book elizabeth von arnim enjoy gardening ...\n95143\nNegative\n\n\n9999\n0\ndisappointed copy book offered sale catalog wa...\n158471\nNegative\n\n\n\n\n10000 rows × 4 columns\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n  \n    \n    \n\n  \n    \n  \n    \n    \n  \n\n    \n  \n\n\nWohooo!!! We have custom created our own Predicted Labels using sentence tranfomers. Although they might not be completely accurate but it helps us to arrive at a quick conclusion when we have no information about the input text.\nThis programming article enhanced my understanding of how to use representation models in practice, providing new insights and uncovering exciting possibilities for leveraging embedding models. More to come—stay tuned!"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html",
    "href": "posts/gradient_tape_learning/index.html",
    "title": "Basic Tensor Gradient Tape",
    "section": "",
    "text": "This blog has been created out of curiosity to develop gradient descent from scratch rather than using the gradient descent algorithm directly.\nThis has been a good learning experience for me, and I have created it as a blog post for both my future reference and for sharing what I’ve learned"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#loading-libraries",
    "href": "posts/gradient_tape_learning/index.html#loading-libraries",
    "title": "Basic Tensor Gradient Tape",
    "section": "0. Loading Libraries",
    "text": "0. Loading Libraries\n\nimport tensorflow as tf\nimport numpy as np"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#first-derivative-at-one-point",
    "href": "posts/gradient_tape_learning/index.html#first-derivative-at-one-point",
    "title": "Basic Tensor Gradient Tape",
    "section": "1. First Derivative (At one point)",
    "text": "1. First Derivative (At one point)\n\n1.1. First Derivate for single variable\n\nx = tf.constant(100.0)\nb = tf.constant(10.0)\n\nwith tf.GradientTape() as tape:\n  tape.watch(x)\n  y = x ** 2 + b\n  dy_dx = tape.gradient(y, x)\n\ndel tape\n\n\nprint(dy_dx)\n\ntf.Tensor(200.0, shape=(), dtype=float32)\n\n\nFor equation x**2 +b the first derivate at point where x=100 is 200\n\n\n1.2. First Derivate with two variables\nWhen calculating two derivative it is mandatory to define as persistent=True\n\nx = tf.constant(20.0)\nb = tf.constant(10.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n  tape.watch(x)\n  tape.watch(b)\n  y = x ** 2 + b ** 2\n  dy_dx = tape.gradient(y, x)\n  dy_db = tape.gradient(y, b)\n\ndel tape\n\nWARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\nWARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n\n\n\nprint(dy_dx)\nprint(dy_db)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\ntf.Tensor(20.0, shape=(), dtype=float32)\n\n\nFor equation x**2 + b**2 the first derivate at point where x=20 is 40 and where b=10 is 20\n\n\n1.3. First Derivate with two variables - Simpler Code\n\n1.3.1. Using tf.constant - No output\nWe when remove tape.watch(x) it is important for us to define as tf.Variable as gradient needs to be calculated iteratively at that point\n\nx = tf.constant(20.0)\nb = tf.constant(10.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    y = x ** 2 + b ** 2\ndy_dx, dy_db = tape.gradient(y, [x, b])\n\n\nprint(dy_dx)\nprint(dy_db)\n\nNone\nNone\n\n\n\n\n1.3.2. Using tf.Variable - Output\nAlso, using simpler code we can see we can pass variables in a list\n\nx = tf.Variable(20.0)\nb = tf.Variable(10.0)\n\nwith tf.GradientTape(persistent=True) as tape:\n    y = x ** 2 + b ** 2\ndy_dx, dy_db = tape.gradient(y, [x, b])\n\n\nprint(dy_dx)\nprint(dy_db)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\ntf.Tensor(20.0, shape=(), dtype=float32)"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#second-derivate-using-one-variable",
    "href": "posts/gradient_tape_learning/index.html#second-derivate-using-one-variable",
    "title": "Basic Tensor Gradient Tape",
    "section": "2. Second Derivate using one variable",
    "text": "2. Second Derivate using one variable\n\n2.1. Wrong indentation of code\nThe issue with the below is code is about code indentation when we need to calculate second derivative.\n\nx = tf.Variable(20.0)\nb = tf.Variable(10.0)\n\nwith tf.GradientTape(persistent=True) as tape2:\n  with tf.GradientTape(persistent=True) as tape1:\n    y = x ** 2 + b ** 2\ndy_dx = tape1.gradient(y, x)\ndy_dx_1 = tape2.gradient(dy_dx, x)\n\n\nprint(dy_dx)\nprint(dy_dx_1)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\nNone\n\n\n\n\n2.2. With right indentation of code\n\nx = tf.Variable(20.0)\nb = tf.Variable(10.0)\n\nwith tf.GradientTape(persistent=True) as tape2:\n  with tf.GradientTape(persistent=True) as tape1:\n    y = x ** 2 + b ** 2\n  dy_dx = tape1.gradient(y, x)\ndy_dx_1 = tape2.gradient(dy_dx, x)\n\n\nprint(dy_dx)\nprint(dy_dx_1)\n\ntf.Tensor(40.0, shape=(), dtype=float32)\ntf.Tensor(2.0, shape=(), dtype=float32)\n\n\nFor equation x**2 + b**2 the first derivate at point where x=20 is 40 and where b=10 is 20\n\n\n2.3. Second Order Derivate for array of numbers\n\nx = tf.Variable([-3,-2,-1,0,1,2,3],dtype=tf.float32)\n\nwith tf.GradientTape(persistent=True) as tape2:\n  with tf.GradientTape(persistent=True) as tape1:\n    y = tf.math.square(x)\n  dy_dx = tape1.gradient(y, x)\ndy_dx_1 = tape2.gradient(dy_dx, x)\n\n\nprint(dy_dx)\nprint(dy_dx_1)\n\ntf.Tensor([-6. -4. -2.  0.  2.  4.  6.], shape=(7,), dtype=float32)\ntf.Tensor([2. 2. 2. 2. 2. 2. 2.], shape=(7,), dtype=float32)"
  },
  {
    "objectID": "posts/gradient_tape_learning/index.html#gradient-descent-function",
    "href": "posts/gradient_tape_learning/index.html#gradient-descent-function",
    "title": "Basic Tensor Gradient Tape",
    "section": "3.0 Gradient Descent Function",
    "text": "3.0 Gradient Descent Function\n\n\n\n\n\nHere we will try to create a gradient descent function which will iterative to calculate the derivate and update the weights as per the learning rate.\n\ndef gradientdescent(learning_rate, w0):\n  with tf.GradientTape() as tape:\n    y = tf.math.square(w0)\n\n  dy_dw0 = tape.gradient(y, w0)\n  w0 = w0 - learning_rate * dy_dw0\n  return w0\n\n\nw0 = tf.Variable(1.0,dtype=tf.float32)\n\nBelow we are running for 10k epochs to arrive at the minimal value given the function y = x^2 which is nothing but a parabola.\n\nfor i in range(10000):\n  w0 = tf.Variable(gradientdescent(0.01,w0).numpy(),dtype=tf.float32)\n\n\nw0.numpy()\n\n5.803526e-37\n\n\nAfter running for 10K epochs we can clearly observe how we have arrived at almost 0 value.\n\nw0 = tf.Variable(1.0,dtype=tf.float32)\n\nweights = []\nfor i in range(10000):\n  weights.append(w0.numpy())\n  w0 = tf.Variable(gradientdescent(0.01,w0).numpy(),dtype=tf.float32)\n  \nimport pandas as pd\nfrom plotnine import *\n\n# Create a pandas DataFrame\ndf = pd.DataFrame({'epoch': range(10000), 'w0': weights})\n\n# Plot the data using ggplot\n(ggplot(df, aes(x='epoch', y='w0'))\n + geom_line()\n + labs(title='w0 over epochs', x='Epoch', y='w0')\n + theme_minimal())\n\n\nAs we can see clearly how we have successfully performed gradient descent for a toy example"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html",
    "href": "posts/tensor-basics-part1/index.html",
    "title": "Tensor Flow Basics -1",
    "section": "",
    "text": "These are initial learnings on tensors hope it is helpful for everyone\nimport tensorflow as tf"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "href": "posts/tensor-basics-part1/index.html#what-is-tensor-flow",
    "title": "Tensor Flow Basics -1",
    "section": "What is tensor Flow?",
    "text": "What is tensor Flow?\nTensor flow is deep learning framework developed by Google in 2011 and made it publicly available in the year 2015.\nIt is flexible, scalable solutio that enable us to build models with the existing frameworks.\nIt is like sci-kit learn library but more advanced and flexible as we can custom build our own neural network.\n\nprint(tf.__version__)\n\n2.15.0"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "href": "posts/tensor-basics-part1/index.html#basics-unit-in-tensorflow-framework---tensor",
    "title": "Tensor Flow Basics -1",
    "section": "Basics Unit in TensorFlow Framework - Tensor",
    "text": "Basics Unit in TensorFlow Framework - Tensor\nTensors are multi-dimensional arrays designed for numerical data representation; although they share some similarities with NumPy arrays, they possess certain unique features that give them an advantage in deep learning tasks. One of these key advantages is their ability to utilize hardware acceleration from GPUs and TPUs to significantly speed up computational operations, which is especially useful when working with input data such as images, text, and videos.\nIn simple words “ML” needs numbers; In case of large dimensions we need matrices. These matrices are called tensors. As these are specially designed to use hardware capabilities to accelerate learnings."
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#creating-tensors",
    "href": "posts/tensor-basics-part1/index.html#creating-tensors",
    "title": "Tensor Flow Basics -1",
    "section": "Creating Tensors",
    "text": "Creating Tensors\n\ntf.constant(1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 1],\n       [1, 1]], dtype=int32)&gt;\n\n\nHere we have created a basic tensor with constant number 1 with a shape 2,2 i.e.; two rows and two columns.\nAnd its datatype is integer.\nIt is a numpy array.\n\n## Manually providing the shape\ny = tf.constant([[1, 2, 3], [4, 5, 6]])\nprint(y)\n\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nInstead of giving the shape here we have manually given the values\n\ntf.rank(y)\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=2&gt;\n\n\nHere we are checking what is the rank of the tensor.\n\n\nprint(\"The rank of scalar is \" , tf.rank(tf.constant(1)))\nprint(\"The rank of vector is \" , tf.rank(tf.constant(1,shape=(5))))\nprint(\"The rank of matrix is \" , tf.rank(tf.constant(1,shape=(5,4))))\nprint(\"The rank of rank3tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3))))\n\nThe rank of scalar is  tf.Tensor(0, shape=(), dtype=int32)\nThe rank of vector is  tf.Tensor(1, shape=(), dtype=int32)\nThe rank of matrix is  tf.Tensor(2, shape=(), dtype=int32)\nThe rank of rank3tensor is  tf.Tensor(3, shape=(), dtype=int32)\n\n\nCan there me more than 3 dimensional; of course, but we cannot represent them pictographically.\n\nprint(\"The rank of rank5tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3))))\nprint(\"The rank of rank9tensor is \" , tf.rank(tf.constant(1,shape=(4,2,3,3,3,1,1,3,3))))\n\nThe rank of rank5tensor is  tf.Tensor(5, shape=(), dtype=int32)\nThe rank of rank9tensor is  tf.Tensor(9, shape=(), dtype=int32)"
  },
  {
    "objectID": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "href": "posts/tensor-basics-part1/index.html#basic-tensor-operations",
    "title": "Tensor Flow Basics -1",
    "section": "Basic Tensor Operations",
    "text": "Basic Tensor Operations\n\ntf.constant(1.1, shape=(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1.1, 1.1],\n       [1.1, 1.1]], dtype=float32)&gt;\n\n\n\nTypeCasting\nThe moment we kept 1.1 the data-type has changed to float; Lets check how to typecast integer to float and vice versa\n\nx_int = tf.constant(1, shape=(2,2))\nprint(x_int)\nx_float = tf.cast(x_int, dtype = tf.float32)\nprint(x_float)\nx_float_int = tf.cast(x_float, tf.int32)\nprint(x_float_int)\n\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\ntf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\ntf.Tensor(\n[[1 1]\n [1 1]], shape=(2, 2), dtype=int32)\n\n\n\n\nIndexing\nSimilar to numpy array we can do indexing for the tensors\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\ny[0]\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\ny[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n\nExpanding a matrix\n\ny =  tf.constant([[1,2,3],[4,5,6]])\n\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(tf.expand_dims(y,axis=0)) ## Expanding at the beginning of the tensor\nprint(tf.expand_dims(y,axis=1)) ## Expanding at the Middle of the tensor { for this example}\nprint(tf.expand_dims(y,axis=-1)) ## Expanding at the End of the tensor\n\ntf.Tensor(\n[[[1 2 3]\n  [4 5 6]]], shape=(1, 2, 3), dtype=int32)\ntf.Tensor(\n[[[1 2 3]]\n\n [[4 5 6]]], shape=(2, 1, 3), dtype=int32)\ntf.Tensor(\n[[[1]\n  [2]\n  [3]]\n\n [[4]\n  [5]\n  [6]]], shape=(2, 3, 1), dtype=int32)\n\n\n\n\nTensor Aggregation\n\ny\n\n&lt;tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[1, 2, 3],\n       [4, 5, 6]], dtype=int32)&gt;\n\n\n\nprint(\"Smallest of the number is \",tf.reduce_min(y).numpy())\nprint(\"Largest of the number is \",tf.reduce_max(y).numpy())\n\nSmallest of the number is  1\nLargest of the number is  6\n\n\n\nprint(\"Sum of the numbers are \",tf.reduce_sum(y).numpy())\nprint(\"Average of the numbers are \",tf.reduce_mean(y).numpy())\n\nSum of the numbers are  21\nAverage of the numbers are  3\n\n\n\n\nMatrix with all ones,zeroes and identity\n\nz =  tf.ones([2,3])\nprint(z)\nprint(\" \")\n\nx  =  tf.constant(1,shape=(2,3),dtype=tf.float32)\nprint(x)\nprint(\" \")\n\nz =  tf.zeros([2,3])\nprint(z)\nprint(\" \")\n\nz = tf.eye(3)\nprint(z)\n\ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 1. 1.]\n [1. 1. 1.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[0. 0. 0.]\n [0. 0. 0.]], shape=(2, 3), dtype=float32)\n \ntf.Tensor(\n[[1. 0. 0.]\n [0. 1. 0.]\n [0. 0. 1.]], shape=(3, 3), dtype=float32)\n\n\n\n\nReshaping and Transposing Tensors\n\nx_initial = tf.constant(1, shape=(4,3))\nprint(x_initial)\n\ntf.Tensor(\n[[1 1 1]\n [1 1 1]\n [1 1 1]\n [1 1 1]], shape=(4, 3), dtype=int32)\n\n\n\ntf.reshape(x_initial,shape=(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[1, 1, 1],\n        [1, 1, 1]],\n\n       [[1, 1, 1],\n        [1, 1, 1]]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(2,6))\n\n&lt;tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1]], dtype=int32)&gt;\n\n\n\ntf.reshape(x_initial,shape=(12,1))\n\n&lt;tf.Tensor: shape=(12, 1), dtype=int32, numpy=\narray([[1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1],\n       [1]], dtype=int32)&gt;\n\n\nHere we can reshape to any other shape; However, the multiplication of shapes should remain the same.\n\ntf.reshape(x_initial,shape=-1) #Flatten the array\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)&gt;\n\n\n\ntf.transpose(x_initial)\n\n&lt;tf.Tensor: shape=(3, 4), dtype=int32, numpy=\narray([[1, 1, 1, 1],\n       [1, 1, 1, 1],\n       [1, 1, 1, 1]], dtype=int32)&gt;\n\n\nThe initial shape of (4,3) got changed to (3,4)\n\n\nDistributions\n\nx1  = tf.random.normal((3,3))\nprint(x1)\nprint(\" \")\n\nx1  = tf.random.normal((3,3),mean = 0, stddev =1 )\nprint(x1)\nprint(\" \")\n\nx2 =  tf.random.uniform((3,3))\nprint(x2)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.591363    0.12791212  0.38762185]\n [ 0.26025018  1.7209182  -0.7802837 ]\n [ 0.89150804 -0.9648455   0.64507854]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-1.0034778   0.05435322 -1.3141975 ]\n [-0.17819698 -1.9136705  -0.9396771 ]\n [ 2.2143493   0.33600262  0.8174351 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[0.04812372 0.79855204 0.8067709 ]\n [0.67069924 0.06617999 0.14941025]\n [0.31500185 0.17441607 0.7476181 ]], shape=(3, 3), dtype=float32)\n \n\n\n\n\nMathematical Operations\n\nAddition\nSubtraction\nMultiplication\nDivision\n\n\nx4 = tf.random.normal((3,3))\nprint(x4)\nprint(\" \")\ny4 = tf.random.normal((3,3))\nprint(y4)\nprint(\" \")\n\ntf.Tensor(\n[[ 0.5908952   3.5452905  -0.34438497]\n [-0.5237503   1.2899861  -0.50684774]\n [ 1.2187229   0.50014    -0.6212071 ]], shape=(3, 3), dtype=float32)\n \ntf.Tensor(\n[[-0.7346279   2.9705956  -0.45318994]\n [-0.947753    0.6556651   3.018978  ]\n [-1.5799906  -1.2278746   0.57451475]], shape=(3, 3), dtype=float32)\n \n\n\n\nprint(x4+y4)\nprint(\" \")\ntf.add(x4,y4)\n\ntf.Tensor(\n[[-0.14373273  6.5158863  -0.7975749 ]\n [-1.4715033   1.9456513   2.5121303 ]\n [-0.3612677  -0.7277346  -0.04669237]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.14373273,  6.5158863 , -0.7975749 ],\n       [-1.4715033 ,  1.9456513 ,  2.5121303 ],\n       [-0.3612677 , -0.7277346 , -0.04669237]], dtype=float32)&gt;\n\n\n\nprint(x4-y4)\nprint(\" \")\ntf.subtract(x4,y4)\n\ntf.Tensor(\n[[ 1.3255231   0.5746949   0.10880497]\n [ 0.4240027   0.63432103 -3.525826  ]\n [ 2.7987137   1.7280147  -1.1957219 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 1.3255231 ,  0.5746949 ,  0.10880497],\n       [ 0.4240027 ,  0.63432103, -3.525826  ],\n       [ 2.7987137 ,  1.7280147 , -1.1957219 ]], dtype=float32)&gt;\n\n\n\nprint(x4*y4)\nprint(\" \")\ntf.multiply(x4,y4)\n\ntf.Tensor(\n[[-0.43408808 10.531624    0.1560718 ]\n [ 0.49638593  0.8457989  -1.5301622 ]\n [-1.9255708  -0.6141092  -0.35689265]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.43408808, 10.531624  ,  0.1560718 ],\n       [ 0.49638593,  0.8457989 , -1.5301622 ],\n       [-1.9255708 , -0.6141092 , -0.35689265]], dtype=float32)&gt;\n\n\n\nprint(x4/y4)\nprint(\" \")\ntf.divide(x4,y4)\n\ntf.Tensor(\n[[-0.8043462   1.1934612   0.7599131 ]\n [ 0.5526232   1.9674467  -0.16788718]\n [-0.77134824 -0.40732172 -1.0812727 ]], shape=(3, 3), dtype=float32)\n \n\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[-0.8043462 ,  1.1934612 ,  0.7599131 ],\n       [ 0.5526232 ,  1.9674467 , -0.16788718],\n       [-0.77134824, -0.40732172, -1.0812727 ]], dtype=float32)&gt;\n\n\n\n\nMatrix Multiplications\n\nx4_new = tf.random.normal((3,2))\ny4_new = tf.random.normal((2,3))\n\nprint(tf.matmul(x4_new,y4_new))\n\ntf.Tensor(\n[[ 0.5675167   0.29923582 -2.018334  ]\n [-0.5145724   1.9392778  -1.5560541 ]\n [ 1.0566927  -2.3777525   0.73752195]], shape=(3, 3), dtype=float32)\n\n\n\nx4_new @ y4_new\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[ 0.5675167 ,  0.29923582, -2.018334  ],\n       [-0.5145724 ,  1.9392778 , -1.5560541 ],\n       [ 1.0566927 , -2.3777525 ,  0.73752195]], dtype=float32)&gt;\n\n\nTwo ways of matrix of multiplication in tensors; There is also a way using dot product; which we will discuss later :)"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html",
    "href": "posts/shallow-linear-model/index.html",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "",
    "text": "This article primarly discusses on implementation of simple linear regression in both sklearn and tensorflow.\nimport tensorflow as tf\n\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nWe will consider a toy dataset of diabetes, described below, to perform both linear regression using OLS and shallow neural networks, and learn from both approaches.\ndata = load_diabetes()\nprint(data['DESCR'])\n\n.. _diabetes_dataset:\n\nDiabetes dataset\n----------------\n\nTen baseline variables, age, sex, body mass index, average blood\npressure, and six blood serum measurements were obtained for each of n =\n442 diabetes patients, as well as the response of interest, a\nquantitative measure of disease progression one year after baseline.\n\n**Data Set Characteristics:**\n\n  :Number of Instances: 442\n\n  :Number of Attributes: First 10 columns are numeric predictive values\n\n  :Target: Column 11 is a quantitative measure of disease progression one year after baseline\n\n  :Attribute Information:\n      - age     age in years\n      - sex\n      - bmi     body mass index\n      - bp      average blood pressure\n      - s1      tc, total serum cholesterol\n      - s2      ldl, low-density lipoproteins\n      - s3      hdl, high-density lipoproteins\n      - s4      tch, total cholesterol / HDL\n      - s5      ltg, possibly log of serum triglycerides level\n      - s6      glu, blood sugar level\n\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\n\nSource URL:\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\n\nFor more information see:\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\nprint(\"Number of Independent features: \", data['data'].shape[1])\nprint(\"Number of Training Instances: \", data['target'].shape[0])\n\nNumber of Independent features:  10\nNumber of Training Instances:  442\nLet us divide the entire dataset into train and test set\nX= data['data']\ny = data['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#linear-regression-sci-kit-learn",
    "href": "posts/shallow-linear-model/index.html#linear-regression-sci-kit-learn",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Linear Regression Sci-kit Learn",
    "text": "Linear Regression Sci-kit Learn\n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)\n\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\nMean Squared Error: 2900.193628493482\n\n\n\nprint(\"Model Coefficients: \\n\", model.coef_)\n\nModel Coefficients: \n [  37.90402135 -241.96436231  542.42875852  347.70384391 -931.48884588\n  518.06227698  163.41998299  275.31790158  736.1988589    48.67065743]\n\n\n\nprint(\"Model Intercept: \\n\", model.intercept_)\n\nModel Intercept: \n 151.34560453985995"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#shallow-neural-network---linear-regression",
    "href": "posts/shallow-linear-model/index.html#shallow-neural-network---linear-regression",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Shallow Neural Network - Linear Regression",
    "text": "Shallow Neural Network - Linear Regression\nWe can view the shallow neural network for linear regression in below way\n\n\ny_reshaped = y.reshape(-1, 1)\n\n# Concatenate y as an additional column to X\nX_with_y = np.concatenate((X, y_reshaped), axis=1)\n\n# Compute the correlation matrix\ncorrelation_matrix = np.corrcoef(X_with_y, rowvar=False)\n\n\nprint(np.round(correlation_matrix,1))\n\n[[ 1.   0.2  0.2  0.3  0.3  0.2 -0.1  0.2  0.3  0.3  0.2]\n [ 0.2  1.   0.1  0.2  0.   0.1 -0.4  0.3  0.1  0.2  0. ]\n [ 0.2  0.1  1.   0.4  0.2  0.3 -0.4  0.4  0.4  0.4  0.6]\n [ 0.3  0.2  0.4  1.   0.2  0.2 -0.2  0.3  0.4  0.4  0.4]\n [ 0.3  0.   0.2  0.2  1.   0.9  0.1  0.5  0.5  0.3  0.2]\n [ 0.2  0.1  0.3  0.2  0.9  1.  -0.2  0.7  0.3  0.3  0.2]\n [-0.1 -0.4 -0.4 -0.2  0.1 -0.2  1.  -0.7 -0.4 -0.3 -0.4]\n [ 0.2  0.3  0.4  0.3  0.5  0.7 -0.7  1.   0.6  0.4  0.4]\n [ 0.3  0.1  0.4  0.4  0.5  0.3 -0.4  0.6  1.   0.5  0.6]\n [ 0.3  0.2  0.4  0.4  0.3  0.3 -0.3  0.4  0.5  1.   0.4]\n [ 0.2  0.   0.6  0.4  0.2  0.2 -0.4  0.4  0.6  0.4  1. ]]\n\n\nI’m using the above correlation matrix as weight initializer to check whether the weights will be in similar way to that of the regular OLS.\n\nno_of_features = X.shape[1]\n\ninitializer_weights = tf.keras.initializers.Constant(value= [0.2,0.2,0.3,0.3,0.2,-0.1,0.2,0.3,0.3,0.2])\n\ninitializer_bias = tf.keras.initializers.Constant(value = [1.0])\n\n\nmodel_tf = tf.keras.Sequential([\n    tf.keras.layers.Dense(units=1,\n                          activation=\"linear\",\n                          input_shape=(no_of_features,),\n                          kernel_initializer = initializer_weights,\n                          bias_initializer = initializer_bias)\n])\n\nmodel_tf.compile(optimizer='sgd', loss='mean_squared_error',metrics=['mse'])\n\nmodel_tf.fit(X_train, y_train, epochs=1500, verbose=0)\n\ny_pred_test = model_tf.predict(X_test)\n\n3/3 [==============================] - 0s 4ms/step\n\n\n\nmodel_tf.summary()\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_7 (Dense)             (None, 1)                 11        \n                                                                 \n=================================================================\nTotal params: 11 (44.00 Byte)\nTrainable params: 11 (44.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nAs we can see above there is just one layer connecting the paramters;\n11 Paramters are 10 weight paramters and 1 bias parameter that needs to be learned by the neural network\n\nmse_tf = mean_squared_error(y_test, y_pred_test.flatten())\nprint(\"Mean Squared Error:\", mse_tf)\n\nMean Squared Error: 2925.001255544474\n\n\n\nweights, bias = model_tf.layers[0].get_weights()\nprint(\"Weights:\")\nprint(weights)\nprint(\"Bias:\")\nprint(bias)\n\nWeights:\n[[ 5.9075676e+01]\n [-9.0772835e+01]\n [ 3.6696533e+02]\n [ 2.5461090e+02]\n [ 9.4142288e-02]\n [-3.7435982e+01]\n [-1.8325632e+02]\n [ 1.4889064e+02]\n [ 2.8909210e+02]\n [ 1.5162097e+02]]\nBias:\n[153.55777]"
  },
  {
    "objectID": "posts/shallow-linear-model/index.html#comparision-of-parameters",
    "href": "posts/shallow-linear-model/index.html#comparision-of-parameters",
    "title": "Shallow Neural Network - Linear Regression",
    "section": "Comparision of Parameters",
    "text": "Comparision of Parameters\n\nimport pandas as pd\n\n\npd.concat(\n    [pd.DataFrame(model.coef_, columns=[\"Model Coefficients of OLS\"]),\n    pd.DataFrame(weights,columns=[\"Model Coefficients of NN\"])],\n    axis = 1\n)\n\n\n  \n    \n\n\n\n\n\n\nModel Coefficients of OLS\nModel Coefficients of NN\n\n\n\n\n0\n37.904021\n59.075676\n\n\n1\n-241.964362\n-90.772835\n\n\n2\n542.428759\n366.965332\n\n\n3\n347.703844\n254.610901\n\n\n4\n-931.488846\n0.094142\n\n\n5\n518.062277\n-37.435982\n\n\n6\n163.419983\n-183.256317\n\n\n7\n275.317902\n148.890640\n\n\n8\n736.198859\n289.092102\n\n\n9\n48.670657\n151.620972\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n\n    \n  \n\n\nAt this juncture, it’s evident that the weights in neural networks are derived through iterative optimization methods, notably gradient descent, optimizing model performance. Conversely, OLS regression employs statistical techniques to determine model coefficients. Consequently, the fundamental nature of these weights differs significantly.\nAt present, it’s feasible to manually interpret the coefficients derived from OLS regression, aiding in understanding the relationships between variables.\nHowever, this interpretability is not readily achievable with neural networks, underscoring their characterization as black box models primarily designed for prediction rather than inference.\nIt seems there is ongoing research seeks to enhance interpretability in neural networks which i’m not yet aware at this point of time."
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html",
    "href": "posts/shallow_to_multi_linear/index.html",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "",
    "text": "In this blog, I will share my insights gained from analyzing a simulated dataset comprising 10,000 rows. Specifically, I’ll delve into a comparative analysis between simple models and complex models.\nAs a foundational understanding, it is widely acknowledged in the field that for simple handcrafted datasets of smaller to modest sizes simpler models outperform neural network models.\nLet us validate the same through practical experimentation and exploration."
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#loading-libraries",
    "href": "posts/shallow_to_multi_linear/index.html#loading-libraries",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "0. Loading Libraries",
    "text": "0. Loading Libraries\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error"
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#generating-simulated-dataset",
    "href": "posts/shallow_to_multi_linear/index.html#generating-simulated-dataset",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "1. Generating Simulated dataset",
    "text": "1. Generating Simulated dataset\n\nx1 = np.random.normal(size=10000)\nx2 = np.random.normal(size=10000) * 2\nx3 = np.power(x1,2)\n\ny = x1 + x2 + x3\n\ny1 = (x1 * np.exp(x2) + np.exp(-x3) * x2 ) * x1 + y\n\nfinaldf = pd.DataFrame({\n    \"X1\" : x1,\n    \"X2\" : x2,\n    \"X3\" : x3,\n    \"Y\"  : y\n})\n\nfinaldf1 = pd.DataFrame({\n    \"X1\" : x1,\n    \"X2\" : x2,\n    \"X3\" : x3,\n    \"Y\"  : y1\n})\n\nprint(finaldf.head())\n\nprint(finaldf1.head())\n\n         X1        X2        X3         Y\n0  1.524132 -0.934670  2.322977  2.912439\n1 -0.813685  3.410324  0.662084  3.258723\n2  0.183997 -3.398534  0.033855 -3.180682\n3  0.161710 -4.845459  0.026150 -4.657599\n4 -0.567619  3.020272  0.322191  2.774844\n         X1        X2        X3          Y\n0  1.524132 -0.934670  2.322977   3.685127\n1 -0.813685  3.410324  0.662084  21.872114\n2  0.183997 -3.398534  0.033855  -3.784055\n3  0.161710 -4.845459  0.026150  -5.420728\n4 -0.567619  3.020272  0.322191   8.136591\n\n\nHere we have created two dataframes;\n\nOne with simple linear relationship\nOther with complex relationship with exponential functions"
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#linear-regression-sci-kit-learn",
    "href": "posts/shallow_to_multi_linear/index.html#linear-regression-sci-kit-learn",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "2. Linear Regression Sci-Kit Learn",
    "text": "2. Linear Regression Sci-Kit Learn\n\n2.1 Linear Relationship Model\n\nX = finaldf[[\"X1\",\"X2\",\"X3\"]]\nY = finaldf[[\"Y\"]]\n\n\nlmmodel = LinearRegression()\n\nlmmodel.fit(X,Y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlmmodel.coef_\n\narray([[1., 1., 1.]])\n\n\n\ny_pred = lmmodel.predict(X)\n\nmean_squared_error(Y, y_pred)\n\n2.3287394852055727e-30\n\n\nBecause of the linear relationship between the dependent and indepdent variables; the mse is very low.\n\n\n2.2 Complex Relationship Linear Model\n\nX1 = finaldf1[[\"X1\",\"X2\",\"X3\"]]\nY1 = finaldf1[[\"Y\"]]\n\n\nlmmodel1 = LinearRegression()\n\nlmmodel1.fit(X1,Y1)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\nlmmodel1.coef_\n\narray([[2.69112711, 8.39388391, 9.43535975]])\n\n\n\ny_pred = lmmodel.predict(X1)\n\nmean_squared_error(Y1, y_pred)\n\n4385.825525517447\n\n\nBecause of the complex relationship between the dependent and independent variables; the mse is very high compared to earlier model.\n\n\n\nModel Type\nDatset\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\n\nNow let us explore Neural Network Models."
  },
  {
    "objectID": "posts/shallow_to_multi_linear/index.html#tensor-flow-nn-linear-models.",
    "href": "posts/shallow_to_multi_linear/index.html#tensor-flow-nn-linear-models.",
    "title": "Shallow to Multi-Layer Neural Network - Linear Regression",
    "section": "3. Tensor Flow NN Linear Models.",
    "text": "3. Tensor Flow NN Linear Models.\n\n3.1 Shallow NN - Linear Relationship\n\nmodetf = tf.keras.Sequential([\n    tfl.Dense(1,\n              input_shape = (3,),\n              activation = \"linear\"\n              )\n])\n\n\nmodetf.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 4 (16.00 Byte)\nTrainable params: 4 (16.00 Byte)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\nmodetf.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf.fit(X,Y,epochs=100, verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec5a55b0f40&gt;\n\n\n\nmodetf.layers[0].get_weights()\n\n[array([[1.0000002 ],\n        [0.99999994],\n        [1.        ]], dtype=float32),\n array([1.8922645e-08], dtype=float32)]\n\n\nAs we have seen in the previous blog the weights are not same and accurate compared to the OLS model.\n\nmean_squared_error(Y, modetf.predict(X).flatten())\n\n313/313 [==============================] - 1s 1ms/step\n\n\n8.450062478432384e-14\n\n\nmse is very low.\n\n\n3.2 Shallow NN - Complex Relationship\n\nmodetf.fit(X1,Y1,epochs=100,verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec59cd10760&gt;\n\n\n\nmodetf.layers[0].get_weights()\n\n[array([[ 7.9459767],\n        [10.005791 ],\n        [19.434795 ]], dtype=float32),\n array([-2.91453], dtype=float32)]\n\n\n\nmean_squared_error(Y1, modetf.predict(X1).flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n4279.216985484737\n\n\nmse is high compared to earlier one.\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\n\nNow let us explore Multi-Layer Neural Network Models.\n\n\n3.3 Multi-Layer NN - Linear Relationship\n\nmodetf1 = tf.keras.Sequential([\n    tfl.Dense(100,\n              input_shape = (3,),\n              activation = \"linear\"\n              ),\n    tfl.Dense(1,\n              activation = \"linear\"\n              )\n])\n\n\nmodetf1.summary()\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_1 (Dense)             (None, 100)               400       \n                                                                 \n dense_2 (Dense)             (None, 1)                 101       \n                                                                 \n=================================================================\nTotal params: 501 (1.96 KB)\nTrainable params: 501 (1.96 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\nTheory it is still a shallow linear model. As activations used are of identity function. So, we will not see any change in mse\n\nmodetf1.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf1.fit(X,Y,epochs=100, verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec5902b9360&gt;\n\n\n\nmean_squared_error(Y, modetf1.predict(X).flatten())\n\n313/313 [==============================] - 1s 2ms/step\n\n\n3.834542424936751e-14\n\n\nmse almost remained the same for this model as to that of shallow NN model.\n\n\n3.4. Multi-Layer NN - Complex Relationship\n\nmodetf1.fit(X1,Y1,epochs=100, verbose=0)\n#modetf2_pred =\n#mean_squared_error(Y, modetf2_pred.flatten())\nmean_squared_error(Y1, np.nan_to_num(modetf1.predict(X1)).flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n4506.472209255081\n\n\nmse is also same for this model.\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\nMulti-Layer NN\nLinear Relationship\n3.834542424936751e-14\n\n\nMulti-Layer NN\nComplex Relationship\n4506.472209255081\n\n\n\n\n\n3.5 Mulit-Layer NN with Sigmoid - Simpler Relationship\n\nmodetf2 = tf.keras.Sequential([\n    tfl.Dense(4,\n              input_shape = (3,),\n              activation = \"linear\"\n              ),\n    tfl.Dense(3,\n              input_shape = (3,),\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(1,\n              activation = \"linear\"\n              )\n])\n\n\nmodetf2.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf2.fit(X,Y,epochs=100, verbose=0)\n\n&lt;keras.src.callbacks.History at 0x7ec5901c7b80&gt;\n\n\n\nmodetf2_pred = np.nan_to_num(modetf2.predict(X))\nmean_squared_error(Y, modetf2_pred.flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n0.013774647608438807\n\n\nUpon introducing a model with a complex activation function to a simpler dataset, the mean squared error (MSE) surged significantly. This sharp increase suggests that such a model may not be suitable for this dataset.\n\n\n3.6 Mulit-Layer NN with Sigmoid - Complex Relationship\n\nmodetf2.fit(X1,Y1,epochs=100, verbose=0)\nmean_squared_error(Y1, np.nan_to_num(modetf2.predict(X1)).flatten())\n\n313/313 [==============================] - 0s 1ms/step\n\n\n4385.579539532295\n\n\nSince the dataset has intricate complex nature, the mse for this model remained the same.\n\n\n\n\n\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\nMulti-Layer NN\nLinear Relationship\n3.834542424936751e-14\n\n\nMulti-Layer NN\nComplex Relationship\n4506.472209255081\n\n\nMulti-Layer with Sigmoid NN\nLinear Relationship\n0.013774647608438807.\n\n\nMulti-Layer with Sigmoid NN\nComplex Relationship\n4385.579539532295\n\n\n\n\n\n3.7. Multi-Layer NN with Sigmoid & Relu - Complex Relationship\n\nmodetf3 = tf.keras.Sequential([\n    tfl.Dense(3,\n              input_shape = (3,),\n              activation = \"relu\"\n              ),\n    tfl.Dense(6,\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(12,\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(6,\n              activation = \"sigmoid\"\n              ),\n    tfl.Dense(1,\n              activation = \"linear\"\n              )\n])\n\nmodetf3.compile(optimizer = \"sgd\",\n               loss = \"mean_squared_error\",\n               metrics = [\"mse\"])\n\nmodetf3.fit(X1,Y1,epochs=1000, verbose=0)\n\nmean_squared_error(Y1, modetf3.predict(X1).flatten())\n\n313/313 [==============================] - 1s 2ms/step\n\n\n4433.14019525845\n\n\nSince the dataset has intricate complex nature, the mse for this model remained the same.\n\n\n\n\n\n\n\n\nModel Type\nDataset Type\nMSE\n\n\n\n\nSci-Kit LM\nLinear Relationship\n2.3287394852055727e-30.\n\n\nSci-Kit LM\nComplex Relationship\n4385.825525517447.\n\n\nShallow NN\nLinear Relationship\n8.450062478432384e-14.\n\n\nShallow NN\nComplex Relationship\n4279.216985484737.\n\n\nMulti-Layer NN\nLinear Relationship\n3.834542424936751e-14\n\n\nMulti-Layer NN\nComplex Relationship\n4506.472209255081\n\n\nMulti-Layer with Sigmoid NN\nLinear Relationship\n0.013774647608438807.\n\n\nMulti-Layer with Sigmoid NN\nComplex Relationship\n4385.579539532295.\n\n\nMulti-Layer with Sigmoid & Relu NN\nComplex Relationship\n4433.14019525845\n\n\n\nThis blog serves as a straightforward demonstration illustrating that when a dataset lacks intricate complexities and comprises handcrafted features where each row remains relatively consistent within a random normal distribution, simpler models outperform their more complex counterparts.\nIt’s important to note that while simpler models excel in such scenarios, neural networks incur significantly longer computing times. Thus, every level of complexity comes with its associated costs. Therefore, it’s crucial for us to clearly define the ultimate objective of our modeling endeavors before venturing into additional complexities."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "This website is designed to record my learnings on Deep Learning using Tensor Flow Framework. Since, website will be very handy to review even on a mobile compared to Jupyter Notebook.\nThe coding is done in python majorly using the following packages as of now:\n\nTensor Flow\nnumpy\nPandas\nplotnine { Similar to that of ggplot of R }\ndfply { Similar to that of tidyverse of R }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face - Representation Models\n\n\n\n\n\n\nRepresentation Models\n\n\nHugging Face\n\n\nSentiment Classification\n\n\n\nExploration of Represenation Models & Text Classification\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWord2Vec Similarities and Product Recommendations\n\n\n\n\n\n\nWord2Vec\n\n\n\nExending Similarities to Recommendations\n\n\n\n\n\nFeb 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nVegetable Classification and Recognition\n\n\n100% Accuracy on Dev and Test Dataset\n\n\n\ntensorflow\n\n\nclassification\n\n\ncnn\n\n\ntransferlearning\n\n\n\n\n\n\n\n\n\nJun 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow - Important Functions\n\n\n\n\n\n\nTensor Basics\n\n\n\n\n\n\n\n\n\nJun 22, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHugging Face - Sentiment Classification\n\n\n\n\n\n\nPre Trained Models\n\n\nHugging Face\n\n\nSentiment Classification\n\n\n\nExploration of Hugging Face Basics with Sentiment Classification\n\n\n\n\n\nMay 26, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nAmazon Reviews - Sentiment Classification\n\n\n\n\n\n\ntensorflow\n\n\nclassification\n\n\nsequencemodels\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Tensor Gradient Tape\n\n\n\n\n\n\nTensor Basics\n\n\nGradient Tape\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nBasic Tensor Math Operations\n\n\n\n\n\n\nTensor Basics\n\n\nTensor Math\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShallow to Multi-Layer Neural Network - Linear Regression\n\n\n\n\n\n\nLinear Regression\n\n\nShallow Neural Network\n\n\nMulti-Layer Neural Network\n\n\n\nSecond Neural network model\n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nShallow Neural Network - Linear Regression\n\n\n\n\n\n\nLinear Regression\n\n\nShallow Neural Network\n\n\n\nFirst Neural network model\n\n\n\n\n\nMar 28, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics - 2\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTensor Flow Basics -1\n\n\n\n\n\n\nTensor Basics\n\n\n\nMy initial learnings on tensors\n\n\n\n\n\nMar 20, 2024\n\n\n\n\n\n\nNo matching items"
  }
]